<?php
     $which_page = "plenary";
     include('common_header.php');
?>


<table>
<tr><td colspan="3" align="center"><h2>CVPR 2014 Plenary Speakers</br></h2></td></tr>
<tr><td colspan="3">
<table cellspacing="0" cellpadding="5" width="100%">
<!--<tr><td width="15%" align="center"><u><b>Date</b></u></td><td width="*"><u><b>Tutorial Title / Organizers</b></u></td></tr>-->

<tr><td rowspan="3" valign="top"><img src="images/Tsao_200.png" width="200" border="0"></td>
        <td><span class="cvprsectionheader">Neural mechanisms for face processing</span>
<br/><a target="_blank" href="http://tsaolab.caltech.edu/">Professor Doris Tsao</a>, California Institute of Technology (Caltech)<br/></td></tr>
<tr>    <td>How the brain distills a representation of meaningful objects from retinal input is one of the central challenges of systems neuroscience.  Functional imaging experiments in the macaque reveal that one ecologically important class of objects, faces, is represented by a system of six discrete, strongly interconnected regions. Electrophysiological recordings show that these 'face patches' have unique functional profiles. By studying the distinct visual representations maintained in these six face patches, the sequence of information flow between them, and the role each plays in face perception, we are gaining new insights into hierarchical information processing in the brain.</td></tr>
<tr><td><br/><br/><br/></td></tr>



<tr><td rowspan="3" valign="top"><img src="images/StephaneMallat_200.png" width="200" border="0"></td>
        <td><span class="cvprsectionheader">Are Deep Networks a Solution to Curse of Dimensionality?</span>
<br/><a target="_blank" href="http://www.cmap.polytechnique.fr/~mallat/mallat.html">Professor St&eacute;phane Mallat</a>, &Eacute;cole Normale Sup&eacute;rieure, Paris<br/></td></tr>
<tr>    <td>Learning gave a considerable and surprising boost to computer vision, and deep neural networks appear to be
the new winners of the fierce race on classification errors. Algorithm refinements are now going well beyond
our understanding of the problem, and seem to make  irrelevant any study of computer vision models.
<br/><br/>
Yet, learning from high-dimensional data such as images, suffers from a curse of dimensionality which predicts a combinatorial
explosion. Why are these neural architectures avoiding this curse ? Is this rooted in properties of images and
visual tasks ? Can these properties be related to high-dimensional problems in other fields ?
We shall explore the mathematical roots of these questions, and tell a story where invariants, contractions, sparsity,
dimension reduction and multiscale analysis play important roles. Images and examples will give a colorful background to the talk.</td></tr>
<tr><td>&nbsp;</td></tr>




</table>

</td></tr>
</table>





<?php
    include('common_footer.php');    
?>


