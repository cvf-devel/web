<?xml version="1.0"?>
<Workbook xmlns="urn:schemas-microsoft-com:office:spreadsheet"
 xmlns:o="urn:schemas-microsoft-com:office:office"
 xmlns:x="urn:schemas-microsoft-com:office:excel"
 xmlns:ss="urn:schemas-microsoft-com:office:spreadsheet"
 xmlns:html="http://www.w3.org/TR/REC-html40">
 <DocumentProperties xmlns="urn:schemas-microsoft-com:office:office">
  <Version>14.0</Version>
 </DocumentProperties>
 <OfficeDocumentSettings xmlns="urn:schemas-microsoft-com:office:office">
  <AllowPNG/>
 </OfficeDocumentSettings>
 <ExcelWorkbook xmlns="urn:schemas-microsoft-com:office:excel">
  <WindowHeight>6240</WindowHeight>
  <WindowWidth>10000</WindowWidth>
  <WindowTopX>120</WindowTopX>
  <WindowTopY>140</WindowTopY>
  <ProtectStructure>False</ProtectStructure>
  <ProtectWindows>False</ProtectWindows>
 </ExcelWorkbook>
 <Styles>
  <Style ss:ID="Default" ss:Name="Normal">
   <Alignment ss:Vertical="Bottom"/>
   <Borders/>
   <Font ss:FontName="Calibri" x:Family="Swiss" ss:Size="12" ss:Color="#000000"/>
   <Interior/>
   <NumberFormat/>
   <Protection/>
  </Style>
  <Style ss:ID="s62">
   <Alignment ss:Vertical="Bottom"/>
   <Borders/>
   <Font x:Family="Swiss" ss:Bold="1"/>
   <Interior/>
   <NumberFormat/>
   <Protection/>
  </Style>
 </Styles>
 <Worksheet ss:Name="ICCV2015">
  <Table ss:ExpandedColumnCount="4" ss:ExpandedRowCount="473" x:FullColumns="1"
   x:FullRows="1" ss:DefaultColumnWidth="65" ss:DefaultRowHeight="15">
   <Row>
    <Cell><Data ss:Type="String">ICCV2015</Data></Cell>
   </Row>
   <Row ss:Index="3">
    <Cell ss:StyleID="s62"><Data ss:Type="String">Paper ID</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Paper Title</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Abstract</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Author Names</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">8</Data></Cell>
    <Cell><Data ss:Type="String">Attributed Grammars for Joint Estimation of Human Attributes, Part and Pose</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we are interested in developing compositional models to explicit representing  pose, parts and attributes and tackling the tasks of attribute recognition, pose estimation and part localization jointly. This is different from the recent trend of using CNN-based approaches for training and testing on these tasks separately with a large amount of data. Conventional attribute models typically use a large number of region-based attribute classifiers on parts of pre-trained pose estimator without explicitly detecting the object or its parts, or considering the correlations between attributes. In contrast, our approach jointly represents both the object parts and their semantic attributes within a unified compositional hierarchy. We apply our attributed grammar model to the task of human parsing by simultaneously performing part localization and attribute recognition. We show our modeling helps performance improvements on pose-estimation task and also outperforms on other existing methods on attribute prediction task.  </Data></Cell>
    <Cell><Data ss:Type="String">Seyoung Park*, UCLA; Song-Chun Zhu, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">9</Data></Cell>
    <Cell><Data ss:Type="String">Weakly Supervised Learning of Part Detectors for Visual Recognition</Data></Cell>
    <Cell><Data ss:Type="String">In this work, we propose a novel Weakly Supervised  Learning (WSL) framework dedicated to learn discriminative part detectors from images annotated with a global label.   Our WSL detection scheme encompasses three main contributions.   Firstly, we introduce a structured output latent variable scheme, Minimum mAximum lateNt sTRucturAl  SVM (MANTRA), where two sets of latent variables  are incorporated into the prediction function.   The model is trained with a cutting plane algorithm based on the one-slack formulation, making the learning efficient.     Secondly, we instantiate MANTRA for two different WSL detection tasks: multi-class classification and ranking.  For ranking, we propose efficient solutions to exactly solve the inference and the loss-augmented problems.  Finally, extensive experiments highlight the relevance of the proposed method: MANTRA outperforms state-of-the art results on five different datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Thibaut Durand*, LIP6; Nicolas Thome, ; Matthieu Cord, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">12</Data></Cell>
    <Cell><Data ss:Type="String">Face Flow</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a method for the robust and efficient computation of   multi-frame optical flow in an expressive sequence of facial images.  We formulate a novel energy minimisation problem and we  establish dense correspondences between a  neutral template and every frame of the sequence.  We propose to exploit the highly correlated nature of human  expression by representing the dense facial motion using a deformation basis.  Furthermore, we exploit the even higher correlation between face deformations in   a specific input sequence by imposing a low-rank prior on the coefficients of the   deformation basis, yielding temporally consistent optical flow.  Our proposed model-based problem formulation, in conjunction with the inverse   compositional strategy and low-rank matrix optimisation that we adopt, leads to   a highly efficient algorithm for calculating optical flow across a facial sequence.   For experimental evaluation, we show quantitative experiments on a very challenging   novel benchmark of face sequences with dense ground truth optical flow based on motion   capture data. We also provide qualitative results on a real sequence displaying  fast motion and natural occlusions.  Extensive quantitative and qualitative comparisons demonstrate that the proposed method   outperforms state-of-the-art optical flow and dense non-rigid registration techniques   whilst running an order of magnitude faster.</Data></Cell>
    <Cell><Data ss:Type="String">Patrick Snape*, Imperial College London; Anastasios Roussos, Imperial College London; Yannis Panagakis, ; Stefanos Zafeiriou, Imperial College London</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">26</Data></Cell>
    <Cell><Data ss:Type="String">Love Thy Neighbors: Image Annotation by Exploiting Social Metadata</Data></Cell>
    <Cell><Data ss:Type="String">Some images that are difficult to recognize on their own may become more clear in the context of a social neighborhood of related images with similar social metadata. We build on this intuition to improve multilabel image annotation. Our model uses social metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities, then uses a deep neural network to blend visual information from the image and its neighbors. Prior work typically models image metadata parametrically; in contrast, our nonparametric treatment allows our model to perform well even when the vocabulary of social metadata changes between training and testing. We perform comprehensive experiments on the NUS-WIDE dataset, where we show that our model outperforms state-of-the-art methods for multilabel image annotation even in extreme cases where our model is forced to generalize to unseen vocabularies or new types of social metadata.</Data></Cell>
    <Cell><Data ss:Type="String">Lamberto Ballan*, Stanford University; Justin Johnson, Stanford University; Fei-Fei Li, Stanford University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">37</Data></Cell>
    <Cell><Data ss:Type="String">Fill and Transfer: A Simple Physics-based Approach for Containability Reasoning</Data></Cell>
    <Cell><Data ss:Type="String">The visual perception of object affordances has emerged as a useful ingredient for building powerful computer vision and robotic applications. In this paper we introduce a novel approach to reason about liquid containability - the affordance of containing liquid. Our approach analyzes container objects based on two simple physical processes: the Fill and Transfer of liquid. First, it reasons about whether a given 3D object is a liquid container and its best filling direction. Second, it proposes directions to transfer its contained liquid to the outside while avoiding spillage. We compare our simplified model with a common fluid dynamics simulation and demonstrate that our algorithm makes human-like choices about the best directions to fill containers and transfer liquid from them. We apply our approach to reason about the containability of several real-world objects acquired using a consumer-grade depth camera.  </Data></Cell>
    <Cell><Data ss:Type="String">Lap-Fai Yu*, University of Massachusetts; Noah Duncan, UCLA; Sai-Kit Yeung, SUTD</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">40</Data></Cell>
    <Cell><Data ss:Type="String">Discriminative Low-Rank Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Good tracking performance is in general attributed to accurate representation over previously obtained targets or reliable discrimination between the target and the surrounding background. In this work, we exploit the advantages of the both approaches to achieve a robust tracker. We construct a subspace to represent the target and the neighboring background, and simultaneously propagate their class labels via the learned subspace. Moreover, we propose a novel criterion to identify the target from numerous target candidates on each frame, which takes into account both discrimination reliability and representation accuracy. In addition, with the proposed criterion, the ambiguity in the class labels of the neighboring background samples, which often influences the reliability of discriminative tracking model, is effectively alleviated, while the training set is still kept small. Extensive experiments demonstrate that our tracker performs favourably against many other state-of-the-art trackers.</Data></Cell>
    <Cell><Data ss:Type="String">Yao Sui*, Tsinghua University; Yafei Tang, China Unicom; Li Zhang, Tsinghua University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">50</Data></Cell>
    <Cell><Data ss:Type="String">A Groupwise Multilinear Correspondence Optimization for 3D Faces</Data></Cell>
    <Cell><Data ss:Type="String">Multilinear face models are widely used to model the space of human faces with expressions. For databases of 3D human faces of different identities performing multiple expressions, these statistical shape models decouple identity and expression variations. To compute a high-quality multilinear face model, the quality of the registration of the database of 3D face scans used for training is essential. Meanwhile, a multilinear face model can be used as an effective prior to register 3D face scans, which are typically noisy and incomplete. Inspired by the minimum description length approach, we propose the first method to jointly optimize a multilinear model and the registration of the 3D scans used for training. Given an initial registration, our approach fully automatically improves the registration by optimizing an objective function that measures the compactness of the multilinear model, resulting in a sparse model. We choose a continuous representation for each face shape that allows to use a quasi-Newton method in parameter space for optimization. We evaluate our approach on two standard 3D face databases and show its robustness to noise in the initialization. Furthermore, we show that our approach is computationally more efficient and leads to correspondences of higher quality than existing methods based on linear statistical models.</Data></Cell>
    <Cell><Data ss:Type="String">Timo Bolkart*, Saarland University; Stefanie Wuhrer, Inria</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">51</Data></Cell>
    <Cell><Data ss:Type="String">Real-Time Pose Estimation Piggybacked on Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">We present an object detector coupled with pose estimation directly in a single compact and simple model, where the detector shares extracted image features with the pose estimator. The output of the classification of each candidate window consists of both object score and likelihood map of poses. This extension introduces negligible overhead during detection so that the detector is still capable of real time operation. We evaluated the proposed approach on the problem of vehicle detection. We used existing datasets with viewpoint/pose annotation (WCVP, 3D objects, KITTI).  Besides that, we collected a new traffic surveillance dataset COD20k which fills certain gaps of the existing datasets and we make it public. The experimental results show that the proposed approach is comparable with state-of-the-art approaches in terms of accuracy, but it is considerably faster - easily operating in real time (Matlab with C++ code). The source codes and the collected COD20k dataset are made public along with the paper.</Data></Cell>
    <Cell><Data ss:Type="String">Roman Juranek, ; Marketa Dubska*, Brno University of Technology; Adam Herout, Brno University of Technology; Pavel Zemcik, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">55</Data></Cell>
    <Cell><Data ss:Type="String">On Linear Structure from Motion for Light Field Cameras</Data></Cell>
    <Cell><Data ss:Type="String">We present a novel approach to relative pose estimation which is tailored to 4D light field cameras. From the relationships between scene geometry and light field structure and an analysis of the light field projection in terms of Pluecker ray coordinates, we deduce a set of linear constraints on ray space correspondences between a light field camera pair. These can be applied to infer relative pose of the light field cameras and thus obtain a point cloud reconstruction of the scene. While the proposed method has interesting relationships to pose estimation for generalized cameras based on ray-to-ray correspondence, our experiments demonstrate that our approach is both more accurate and computationally more efficient. It also compares favourably to direct linear pose estimation based on aligning the 3D point clouds obtained by reconstructing depth for each individual light field. To further validate the method, we employ the pose estimates to merge light fields captured with hand-held consumer light field cameras into refocusable panoramas.</Data></Cell>
    <Cell><Data ss:Type="String">Ole Johannsen, University of Konstanz; Antonin Sulc, University of Konstanz; Bastian Goldluecke*, University of Konstanz</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">76</Data></Cell>
    <Cell><Data ss:Type="String">Category-blind Human Action Recognition: A Practical Recognition System</Data></Cell>
    <Cell><Data ss:Type="String">Existing human action recognition systems for 3D sequences obtained from the depth camera are designed to cope with only one action category, either single-person action or two-person interaction, and are difficult to be extended to scenarios where both action categories co-exist. In this paper, we propose the category-blind human recognition method (CHARM) which can recognize a human action without making assumptions of the action category. In our CHARM approach, we represent a human action (either a single-person action or a two-person interaction) class using a co-occurrence of motion primitives. Subsequently, we classify an action instance based on matching its motion primitive co-occurrence patterns to each class representation. The matching task is formulated as maximum clique problems. We conduct extensive evaluations of CHARM using three datasets for single-person actions, two-person interactions, and their mixtures. Experimental results show that CHARM performs favorably when compared with several state-of-the-art single-person action and two-person interaction based methods without making explicit assumptions of action category.</Data></Cell>
    <Cell><Data ss:Type="String">Wenbo Li*, Lehigh University; Longyin Wen, Chinese Academy of Sciences; Mooi Choo Chuah, Lehigh University; Siwei Lyu, SUNY Albany</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">81</Data></Cell>
    <Cell><Data ss:Type="String">Learning Affordance for Direct Perception in Autonomous Driving</Data></Cell>
    <Cell><Data ss:Type="String">Today, there are two major paradigms for vision-based  autonomous driving systems: mediated perception approaches  that parse an entire scene to make a driving decision,  and behavior reflex approaches that directly map an  input image to a driving action by a regressor. In this paper,  we propose a third paradigm: a direct perception based approach  to estimate the affordance for driving. We propose  to map an input image to a small number of key perception  indicators that directly relate to the affordance of a  road/traffic state for driving. Our representation provides  a set of compact yet complete descriptions of the scene to  enable a simple controller to drive autonomously. Falling  in between the two extremes of mediated perception and behavior  reflex, we argue that our direct perception representation  provides the right level of abstraction. To demonstrate  this, we train a deep Convolutional Neural Network  (CNN) using 12 hours of human driving in a video game  and show that our model can work well to drive a car in  a very diverse set of virtual environments. Finally, we also  train another CNN for car distance estimation on the KITTI  dataset, results show that the direct perception approach  can generalize well to real driving images.</Data></Cell>
    <Cell><Data ss:Type="String">Chenyi Chen, Princeton University; Ari Seff, ; Alain Kornhauser, Princeton University; Jianxiong Xiao*, Princeton University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">87</Data></Cell>
    <Cell><Data ss:Type="String">A Comprehensive Multi-illuminant Dataset for Benchmarking of the Intrinsic Image Algorithms</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we provide a new, real-world ground-truth dataset for intrinsic image research. Prior real-world ground-truth datasets have been restricted to rather simple illumination conditions and scene geometries, or have been enhanced using image synthesis methods.    The dataset provided in this paper is based on complex, multi-illuminant real-world scenarios under multi-colored illumination conditions and challenging cast shadows. We provide full per-pixel intrinsic ground-truth data for these scenarios, i.e. reflectance, specularity, shading, and illumination for scenes as well as preliminary depth information. Furthermore, we thoroughly evaluate state-of-the-art intrinsic image recovery methods, using our real-world data.</Data></Cell>
    <Cell><Data ss:Type="String">Shida Beigpour*, University of Siegen; Andreas Kolb, University of Siegen; Sven Kunz, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">91</Data></Cell>
    <Cell><Data ss:Type="String">Photometric Stereo in a Scattering Medium</Data></Cell>
    <Cell><Data ss:Type="String">Photometric stereo is widely used for 3D reconstruction.  However, its use in scattering media such as water, biological  tissue and fog has been limited until now, because of  forward scattered light from both the source and object, as  well as light scattered back from the medium (backscatter).  Here we make three contributions to address the key modes  of light propagation, under the common single scattering  assumption for dilute media. First, we show through extensive  simulations that single-scattered light from a source  can be approximated by a point light source with a single  direction. This alleviates the need to handle light source  blur explicitly. Next, we model the blur due to scattering of  light from the object. We measure the object point-spread  function and introduce a simple deconvolution method. Finally,  we show how imaging fluorescence emission where  available, eliminates the backscatter component and increases  the signal-to-noise ratio. Experimental results in a  water tank, with different concentrations of scattering media  added, show that deconvolution produces higher-quality  3D reconstructions than previous techniques, and that when  combined with fluorescence, can produce results similar to  that in clear water even for highly turbid media.</Data></Cell>
    <Cell><Data ss:Type="String">Zak Murez*, UCSD; Ravi Ramamoorthi, UCSD; Tali Treibitz, University of Haifa; David Kriegman, University of California at San Diego</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">93</Data></Cell>
    <Cell><Data ss:Type="String">Selective Encoding for Recognizing Unreliably Localized Faces</Data></Cell>
    <Cell><Data ss:Type="String">Current face verification systems rely on precise face detection and registration. However, these two components are fallible under unconstrained scenarios (e.g., mobile face authentication) due to partial occlusions, pose variations, lighting conditions and limited view-angle coverage of mobile cameras. We address the unconstrained face verification problem by encoding face images directly without any explicit models of detection or registration. We propose a selective encoding framework which injects relevance information (e.g., foreground/background probabilities) into each cluster of a descriptor codebook. An additional selector component also discards distractive image patches and improves spatial robustness. We evaluate our framework using Gaussian mixture models and Fisher vectors on challenging face verification datasets. We apply selective encoding to Fisher vector features, which in our experiments degrade quickly with inaccurate face localization; our framework improves robustness with no extra test time computation. We also apply our approach to mobile based active face authentication task, demonstrating its utility in real scenarios.</Data></Cell>
    <Cell><Data ss:Type="String">Ang Li*, University of Maryland; Vlad Morariu, University of Maryland; Larry Davis, &quot;University of Maryland, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">97</Data></Cell>
    <Cell><Data ss:Type="String">Temporal Subspace Clustering for Human Motion Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Subspace clustering is an effective technique for segmenting data drawn form multiple subspaces. However, it neglects the temporal information that are critical in time series data, such as human motion. We propose a novel temporal subspace clustering (TSC) approach in this paper. We improve the subspace clustering technique from two aspects. First, a temporal Laplacian regularization is designed, which encodes the sequential relationships in time series data. Second, to obtain expressive codings, we learn a non-negative dictionary from data. An efficient optimization algorithm is presented to jointly learn the representation codings and dictionary. After constructing an affinity graph using the codings, multiple temporal segments can be grouped via spectral clustering. Experimental results on two action and gesture datasets demonstrate the effectiveness of our approach compared to the state-of-the-art subspace clustering methods. In particular, TSC improves the average clustering accuracy by at least 10%.</Data></Cell>
    <Cell><Data ss:Type="String">Sheng Li*, Northeastern University; Kang Li, Northeastern University; Yun Fu, Northeastern University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">101</Data></Cell>
    <Cell><Data ss:Type="String">Pedestrian Travel Time Estimation in Crowded Scenes</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we target on the problem of estimating the statistic of pedestrian travel time within a period from an entrance to a destination in a crowded scene. Such estimation is based on the global distributions of crowd densities and velocities instead of complete trajectories of pedestrians, which cannot be obtained in crowded scenes. The proposed method is motivated by our statistical investigation on the correlations between travel time and global properties of crowded scenes. In particular, active regions are created for each source-destination pair to model the probable walking regions over the corresponding source-destination traffic flow. Two sets of scene features are specially designed for modeling moving and stationary persons inside the active regions and their influences on pedestrian travel time. The estimation on travel time provides valuable information for both scene understanding and pedestrian behavior analysis, but was not sufficiently studied in literature. The effectiveness is demonstrated through several  surveillance applications, including dynamic scene monitoring, localization of regions blocking traffics, and detection of abnormal pedestrian behaviors. Many more interesting and valuable applications are to be explored in the future.</Data></Cell>
    <Cell><Data ss:Type="String">Shuai Yi*, The Chinese University of Hong Kong; Hongsheng Li, CUHK; Xiaogang Wang, The Chinese University of Hong Kong, Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">103</Data></Cell>
    <Cell><Data ss:Type="String">SOWP: Spatially Ordered and Weighted Patch Descriptor for Visual Tracking</Data></Cell>
    <Cell><Data ss:Type="String">A simple yet effective object descriptor for visual tracking is proposed in this paper. We first decompose the bounding box of a target object into multiple patches, which are described by color and gradient histograms. Then, we concatenate the features of the spatially ordered patches to represent the object appearance. Moreover, to alleviate the impacts of background information possibly included in the bounding box, we determine patch weights using random walk with restart (RWR) simulations. The patch weights represent the importance of each patch in the description of foreground information, and are used to construct an object descriptor, called spatially ordered and weighted patch (SOWP) descriptor. We incorporate the proposed SOWP descriptor into the structured output tracking framework. Experimental results demonstrate that the proposed algorithm yields significantly better performance than the state-of-the-art trackers on a recent benchmark dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Han-Ul Kim*, Korea University; Dae-Youn Lee, Korea University ; Jae-Young Sim, Ulsan National Institute of Science and Technology; Chang-Su Kim, Korea University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">107</Data></Cell>
    <Cell><Data ss:Type="String">PatchMatch-based Automatic Lattice Detection for Near-Regular Textures</Data></Cell>
    <Cell><Data ss:Type="String">In this work, we investigate the problem of automatically  inferring the underlying lattice structure of near-regular  texture (NRT) patterns in real-world images. Our technique  is built on top of the Generalized PatchMatch (GPM)  algorithm for finding k-nearest-neighbor (kNN) correspondences  in an image. We use the kNNs computed from GPM  to recover an initial estimate of the 2D wallpaper basis vectors  and seed vertices of the texture lattice. We iteratively  expand this lattice by solving an MRF optimization problem.  We show that we can discretize the space of good solutions  for the MRF using the GPM kNNS, allowing us to efficiently  and accurately minimize the MRF energy function  using the Particle Belief Propagation algorithm. We demonstrate  our technique on a benchmark NRT dataset containing  a wide range of images with geometric and photometric  variations, and show that our method clearly outperforms  the state of the art in terms of both texel detection rate and  texel localization score.</Data></Cell>
    <Cell><Data ss:Type="String">Siying Liu*, UIUC; Tian-Tsong Ng, A-STAR Singapore; Minh Do, University of Illinois at Urbana-Champaign; Kalyan Sunkavalli, Adobe; Eli Shechtman, &quot;Adobe, Seattle, USA&quot;; Nathan Carr, Adobe</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">109</Data></Cell>
    <Cell><Data ss:Type="String">Live Repetition Counting</Data></Cell>
    <Cell><Data ss:Type="String">The task of counting the number of repetitions of approximately the same action in an input video sequence is addressed using convolutional neural networks. The proposed system runs online and not on the complete pre-captured video. It analyzes sequentially blocks of 20 non-consecutive frames. The cycle length within each block is evaluated using a deep network architecture and the information is then integrated over time. A unique property of our method is that it is shown to successfully train on entirely synthetic data, created by synthesizing moving random patches. It, therefore, effectively exploits the high generalization capability of deep neural networks. The entropy of the network's predictions is used in order to automatically start and stop the repetition counter and in order to select the appropriate time scale. Coupled with a region of interest detection mechanism, the system is robust enough to handle real world videos, even when the camera is moving.</Data></Cell>
    <Cell><Data ss:Type="String">Ofir Levy, Tel Aviv University; Lior  Wolf*, &quot;Tel Aviv University, Israel&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">113</Data></Cell>
    <Cell><Data ss:Type="String">A Data-driven Metric for Comprehensive Evaluation of Saliency Models</Data></Cell>
    <Cell><Data ss:Type="String">In the past decades, hundreds of saliency models have been proposed for fixation prediction, along with dozens of evaluation metrics. However, existing metrics, which are often heuristically designed, may draw conflict conclusions in comparing saliency models. As a consequence, it becomes somehow confusing on the selection of metrics in comparing new models with state-of-the-arts. To address this problem, we propose a data-driven metric for comprehensive evaluation of saliency models. Instead of heuristically designing such a metric, we first conduct extensive subjective tests to find how saliency maps are assessed by human being. Based on the user data collected in the tests, nine representative evaluation metrics are directly compared by quantizing their performances in assessing saliency maps. Moreover, we propose to learn a data-driven metric by using Convolutional Neural Network. Compared with existing metrics, experimental results show that the data-driven metric performs the most consistent with human being in evaluating saliency maps as well as saliency models.</Data></Cell>
    <Cell><Data ss:Type="String">Jia Li*, Beihang University; Changqun Xia, Beihang University; Yafei Song, ; Shu Fang, Peking University; Xiaowu Chen, Beihang University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">115</Data></Cell>
    <Cell><Data ss:Type="String">Understanding and Predicting Memorability at a Large-scale</Data></Cell>
    <Cell><Data ss:Type="String">Progress in estimating visual memorability has been limited by the small scale and lack of variety of benchmark data. Here, we introduce a novel experimental procedure to objectively measure human memory, building the largest annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources). Using Convolutional Neural Networks (CNNs), we show that fine-tuned deep features outperform all other features by a large margin, reaching a rank correlation of 0.64, near human consistency (0.68). Analysis of the responses of the high-level CNN layers shows which objects and regions are positively, and negatively, correlated with memorability, allowing us to create memorability maps for each image and provide a concrete method to perform image memorability manipulation. This work demonstrates that one can now robustly estimate the memorability of images from many different classes, positioning memorability and deep memorability features as prime candidates to estimate the utility of information for cognitive systems. </Data></Cell>
    <Cell><Data ss:Type="String">Aditya Khosla*, MIT; Akhil Raju, MIT; Antonio Torralba, MIT; Aude Oliva, &quot;MIT, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">119</Data></Cell>
    <Cell><Data ss:Type="String">Active Learning Revisited: Reusing Past Datasets for Future Tasks</Data></Cell>
    <Cell><Data ss:Type="String">How can we reuse existing knowledge, in the form of available  classifiers, when solving a new and apparently unrelated target task from a set of unlabelled data?   In this work we make a first contribution to answer this question in the context of image classification.   We frame this quest as an active learning problem and use zero-shot models to guide the learning process by linking the new task and the existing classifiers. By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated. On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labelling effort. Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Stratis Gavves*, KU Leuven; Thomas Mensink, University of Amsterdam; Tatiana Tommasi, UNC; Cees Snoek, University of Amsterdam; Tinne Tuytelaars, KU Leuven</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">126</Data></Cell>
    <Cell><Data ss:Type="String">3D Object Reconstruction from Hand-Object Interactions </Data></Cell>
    <Cell><Data ss:Type="String">Recent advances have enabled a plethora of 3d object reconstruction approaches using a single off-the-shelf RGB-D camera.   Although these approaches are successful for a wide range of object classes, they are based on the assumption of objects with a wealth of stable, distinctive geometrical and/or textural features.   However, a lot of objects (household, decorative, toys, mechanical parts, etc) are characterized by a simple,   minimalistic shape and high symmetry (e.g. cylindrical or spherical objects) or even lack of texture.   Existing in-hand scanning systems and 3d reconstruction techniques fail for such symmetric objects in the absence of highly distinctive features.   In this work we show that 3d reconstruction based on low-level features can be facilitated by higher level ones.   Although existing in-hand scanning systems simply ignore information originating from the hand, we show that 3D hand motion capture  can provide strong and reliable features, effectively facilitating the reconstruction of even featureless, highly symmetrical objects.   This work presents the first system that fuses the rich additional information of hands into a 3D reconstruction pipeline, significantly contributing to the state-of-the-art of in-hand scanning.  </Data></Cell>
    <Cell><Data ss:Type="String">Dimitrios Tzionas*, University of Bonn, MPI; Juergen Gall, &quot;University of Bonn, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">130</Data></Cell>
    <Cell><Data ss:Type="String">Learning Concept Embeddings with Combined Human-Machine Expertise</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents our work on ``SNaCK,'' a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. We show that both parts are necessary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing  subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.  </Data></Cell>
    <Cell><Data ss:Type="String">Michael Wilber*, Cornell University; Iljung Kwak, UC San Diego; Serge Belongie, Cornell</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">131</Data></Cell>
    <Cell><Data ss:Type="String">Object Detection Using Generalization and Efficiency Balanced Co-occurrence Features</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a high-accuracy object detector  based on co-occurrence features. Firstly, we introduce  three kinds of local co-occurrence features constructed by the traditional Haar, LBP, and HOG respectively. Then the  boosted detectors are learned, where each weak classifier  corresponds to a local image region with a co-occurrence  feature. In addition, we propose a Generalization and Efficiency Balanced (GEB) framework for boosting training.  In the feature selection procedure, the discrimination ability, the generalization power, and the computation cost of the candidate features are all evaluated for decision. As  a result, the boosted detector achieves both high accuracy  and good efficiency. It also shows performance competitive  with the state-of-the-art methods for pedestrian detection and general object detection tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Haoyu Ren*, Simon Fraser University; Ze-Nian Li, Simon Fraser University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">150</Data></Cell>
    <Cell><Data ss:Type="String">Weakly-Supervised Alignment of Video With Text</Data></Cell>
    <Cell><Data ss:Type="String">Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time stamp for every sentence.  Given vectorial features for both video and text, we propose to cast this task as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels [Bojanowski et al. 2014], we  evaluate our method on a challenging dataset of videos with associated textual descriptions [Regneri et al. 2013], and explore bag-of-words and continuous representations for text.</Data></Cell>
    <Cell><Data ss:Type="String">Piotr Bojanowski*, INRIA; Rémi Lajugie, INRIA; Edouard Grave, Columbia; Francis Bach, INRIA; Ivan Laptev, INRIA Paris; Jean Ponce, ENS; Cordelia Schmid, &quot;INRIA Grenoble, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">163</Data></Cell>
    <Cell><Data ss:Type="String">HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition</Data></Cell>
    <Cell><Data ss:Type="String">In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers  and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively</Data></Cell>
    <Cell><Data ss:Type="String">Zhicheng Yan*, University of Illinois; Hao Zhang, Carnegie Mellon University; Robinson Piramuthu, ; Vignesh Jagadeesh, EBay Research Labs; Dennis DeCoste, EBay Research Labs; Wei Di, ; Yizhou Yu, HKU, UIUC, ZJU</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">176</Data></Cell>
    <Cell><Data ss:Type="String">Deep Multiple Instance Network</Data></Cell>
    <Cell><Data ss:Type="String">This paper investigates novel deep neural network structures to extract fine-grained high resolution details from images. Existing deep convolutional neural networks mostly extracted one instance such as a down-sized crop from each image as a training example. However, the one instance may not always well represent the entire image, which may cause ambiguity during training. In this paper, we propose a deep multiple instance network training approach, which allows us to train models using multiple instances generated from one image as training examples. We achieve this by constructing multiple, shared columns in the neural network and feed multiple instances to each of the columns. More importantly, we propose two novel network layers (statistics and sorting) to support aggregation of those patches. The proposed deep multiple instance network integrates shared feature learning and aggregation function learning into a unified framework. We demonstrate the effectiveness of the deep multiple instance network on three applications: image style recognition, aesthetic quality categorization, and image quality estimation. In particular, our models trained using the proposed networks significantly outperformed the state of the art in all three applications. </Data></Cell>
    <Cell><Data ss:Type="String">Xin Lu*, Penn State Univ; Zhe Lin, &quot;Adobe Systems, Inc.&quot;; Xiaohui Shen, Adobe Research; Radomir Mech, Adobe; James Wang, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">177</Data></Cell>
    <Cell><Data ss:Type="String">A Matrix Decomposition Perspective to Multiple Graph Matching</Data></Cell>
    <Cell><Data ss:Type="String">Graph matching has a wide spectrum of real-world applications and in general is known NP-hard. In many vision tasks, one realistic problem arises for finding the global node mappings across a batch of corrupted weighted graphs. This paper is an attempt to connect graph matching, especially multi-graph matching to the matrix decomposition model and its relevant on-the-shelf convex optimization algorithms. Our method aims to extract the common inliers and their synchronized permutations from disordered weighted graphs in the presence of deformation and outliers. Under the proposed framework, several variants can be derived in the hope of accommodating to other types of noises. Experimental results on both synthetic data and real images empirically show that the proposed paradigm exhibits several interesting behaviors and in many cases performs competitively with the state-of-the-arts.</Data></Cell>
    <Cell><Data ss:Type="String">Junchi Yan*, Shanghai Jiao Tong University; Hongteng Xu, Georgia Institute of Technology; Hongyuan Zha, Georgia Institute of Technology; Xiaokang Yang, Shanghai Jiao Tong University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">191</Data></Cell>
    <Cell><Data ss:Type="String">Multiple Granularity Descriptors for Fine-grained Categorization</Data></Cell>
    <Cell><Data ss:Type="String">Fine-grained categorization, which aims to distinguish subordinate-level categories such as bird species or dog breeds, is an extremely challenging task. This is due to two main issues: how to localize discriminative regions for recognition and how to learn sophisticated features for representation. Neither of them is easy if there are insufficient labeled data.    We leverage the fact that a subordinate-level object already has other labels in its ontology tree. These ``free'' labels can be used to train a series of CNN-based classifiers, each specialized at one grain level. The internal representations of these networks have different region of interests, allowing the construction of a multi-grained descriptor that encode informative and discriminative features that cover all the grain levels.    Our multiple granularity framework can be learned in a fully unsupervised setting, requiring only the weakest, image-level label, avoiding the use of labor-intensive bounding box or part annotations. Experimental results on three challenging fine-grained image datasets demonstrate that our approach outperforms state-of-the-art algorithms, including those requiring strong labels.</Data></Cell>
    <Cell><Data ss:Type="String">Dequan Wang*, Fudan University; Zhiqiang Shen, Fudan University; Jie Shao, The Third Research Institute of the Ministry of Public Security, P.R. China; Wei Zhang, Fudan University; Xiangyang Xue, Fudan University; Zheng Zhang, New York University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">194</Data></Cell>
    <Cell><Data ss:Type="String">Structure from Motion Using Structure-less Resection</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes a new incremental structure from motion (SfM) algorithm based on a novel structure-less camera resection technique. Traditional methods rely on 2D-3D correspondences to compute the pose of candidate cameras using PnP.  In this work, we take the collection of already reconstructed cameras as a generalized camera, and  determine the absolute pose of a candidate pinhole camera from pure 2D correspondences, which we call it  semi-generalized camera pose problem. We present the minimal solvers of the new problem for both calibrated and partially calibrated (unknown focal length) pinhole cameras. By integrating these new algorithms in an incremental SfM system, we go beyond the state-of-art methods with the capability of reconstructing cameras without 2D-3D correspondences.  Large-scale real image experiments show that our new SfM system significantly improves the completeness of 3D reconstruction over the standard approach.</Data></Cell>
    <Cell><Data ss:Type="String">Enliang Zheng*, University of North Carolina a; Changchang Wu, Google</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">195</Data></Cell>
    <Cell><Data ss:Type="String">Minimal Solvers for 3D Geometry from Satellite Imagery</Data></Cell>
    <Cell><Data ss:Type="String">We propose two novel minimal solvers which advance the state of the art in satellite imagery processing.  Our methods are efficient and do not rely on the prior existence of complex inverse mapping functions to correlate 2D image coordinates and 3D terrain.  Our first solver improves on the stereo correspondence problem for satellite imagery, in that we provide an exact image-to-object space mapping (where prior methods were inaccurate).  Our second solver provides a novel mechanism for 3D point triangulation, which has improved robustness and accuracy over prior techniques.  Given the usefulness and ubiquity of satellite imagery, our proposed methods allow for improved results in a variety of existing and future applications.</Data></Cell>
    <Cell><Data ss:Type="String">Enliang Zheng*, University of North Carolina a; Ke Wang, University of North Carolina at Chapel Hill; Enrique Dunn, UNC Chapel Hill; Jan-Michael Frahm, UNC Chapel Hill</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">196</Data></Cell>
    <Cell><Data ss:Type="String">Sparse Dynamic 3D Reconstruction from Unsynchronized Videos</Data></Cell>
    <Cell><Data ss:Type="String">We  target the sparse 3D reconstruction of dynamic objects observed by multiple unsynchronized video cameras with unknown temporal overlap.  To this end, we develop a framework to solve for both the unknown structure and sequencing information across video sequences.  Our proposed compressed sensing framework  poses  the estimation of 3D structure and sequencing  information as the joint problem of learning a dictionary and a sparse encoding.  Moreover, we define our dictionary as the temporally varying 3D structure, while we define sequencing information in terms of the sparse coefficients describing a locally linear 3D structural interpolation.  %We solve this problem in an iterative and alternating  manner, where we optimize for 3D structure while fixing sequencing information, and {\em vice versa}.  Our formulation optimizes a biconvex cost function that leverages a compressed sensing formulation and enforces both structural dependency coherence across video streams, as well as motion smoothness across estimates from common video sources.  Experimental results demonstrate the effectiveness of our approach in both synthetic data and captured imagery.</Data></Cell>
    <Cell><Data ss:Type="String">Enliang Zheng*, University of North Carolina a; Dinghuang Ji, Unc; Enrique Dunn, UNC Chapel Hill; Jan-Michael Frahm, UNC Chapel Hill</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">202</Data></Cell>
    <Cell><Data ss:Type="String">Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis</Data></Cell>
    <Cell><Data ss:Type="String">In perspective cameras, images of a frontal-parallel 3D object preserve its aspect ratio invariant to its depth. Such an invariance is useful in photography but is unique to perspective projection. In this paper, we show that alternative non-perspective cameras such as the crossed-slit or XSlit cameras exhibit a different depth-dependent aspect ratio (DDAR) property that can be used to 3D recovery. We first conduct a comprehensive analysis to characterize DDAR, infer object depth from its AR, and model recoverable depth range, sensitivity, and error. We show that repeated shape patterns in real Manhattan World scenes can be used for 3D reconstruction using a single XSlit image. We also extend our analysis to model slopes of lines. Specifically, parallel 3D lines exhibit depth-dependent slopes (DDS) on their images which can also be used to infer their depths. We validate our analyses using real XSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that DDAR and DDS provide important depth cues and enable effective single-image scene reconstruction.</Data></Cell>
    <Cell><Data ss:Type="String">Wei Yang*, University of Delaware; Haiting Lin, Udel; Sing Bing Kang, Microsoft Research; Jingyi Yu, &quot;University of Delaware, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">208</Data></Cell>
    <Cell><Data ss:Type="String">Learning Temporal Embeddings for Complex Video Analysis</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose to learn temporal embeddings of video frames for complex video analysis. Large quantities of unlabeled video data can be easily obtained from the Internet. These videos possess the implicit weak label that they are sequences of temporally and semantically coherent images. We leverage this information to learn temporal embeddings for video frames by associating frames with the temporal context that they appear in. To do this, we propose a scheme for incorporating temporal context based on past and future frames in videos, and compare this to other contextual representations. In addition, we show how data augmentation using multi-resolution samples and hard negatives helps to significantly improve the quality of the learned embeddings. We evaluate various design decisions for learning temporal embeddings, and show that our embeddings can improve performance for multiple video tasks such as retrieval, classification, and temporal order recovery in unconstrained Internet video. </Data></Cell>
    <Cell><Data ss:Type="String">Vignesh Ramanathan*, Stanford University; Kevin Tang, Stanford University; Greg Mori, &quot;Simon Fraser University, Canada&quot;; Fei-Fei Li, Stanford University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">219</Data></Cell>
    <Cell><Data ss:Type="String">Deep Dynamic Convolutional Networks</Data></Cell>
    <Cell><Data ss:Type="String">In this work,  we develop a novel method for automatically learning aspects of the structure of a deep model, in order to improve its  performance, especially when labeled training data are scarce. We propose a dynamic convolutional neural network (dynCNN) model. The dynCNN automatically adapts its structure to provided training data,  achieves an optimal balance among model complexity, data fidelity and training loss, and thus  offers  better generalization performance.     The proposed dynCNN captures complicated data distribution in an unsupervised generative way. Therefore, dynCNN can exploit unlabeled data &#45;- which can be collected at low cost &#45;- to learn its  structure. After determining the structure,   dynCNN further learns its parameters according to specified tasks, in an end-to-end fashion, and produces  discriminative yet compact representations.      We  evaluate the  performance of dynCNN, on fully- and semi-supervised  image classification tasks; dynCNN surpasses standard CNN models on benchmark datasets, with much smaller  size and  higher efficiency.</Data></Cell>
    <Cell><Data ss:Type="String">Jiashi Feng*, UC Berkeley; Trevor Darrell, &quot;UC Berkeley, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">223</Data></Cell>
    <Cell><Data ss:Type="String">Fast and Effective L_0 Gradient Minimization by Region Fusion</Data></Cell>
    <Cell><Data ss:Type="String">L_0 gradient minimization can be applied to an input signal to control the number of non-zero gradients.  This is useful in reducing small gradients generally associated with signal noise, while preserving important signal features.  In computer vision, L_0 gradient minimization has found applications in image denoising, 3D mesh denoising, and image enhancement.  Minimizing the L_0 norm, however, is an NP-hard problem because of its non-convex property. As a result, existing methods rely on approximation strategies to perform the minimization.  In this paper, we present a new method to perform L_0 gradient minimization that is fast and effective. Our method uses a descent approach based on region fusion that converges faster than other methods while providing a better approximation of the optimal L_0 norm.  In addition, our method can be applied to both 2D images and 3D mesh topologies.  The effectiveness of our approach is demonstrated on a number of examples.</Data></Cell>
    <Cell><Data ss:Type="String">Rang Man Ho Nguyen*, National University of Singapo; Michael Brown, &quot;National University of Singapore, Singapore&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">225</Data></Cell>
    <Cell><Data ss:Type="String">Guided Long-Short Term Memory for Image Caption Generation</Data></Cell>
    <Cell><Data ss:Type="String">In this work we focus on the problem of image caption generation. We propose an extension of the long short term memory (LSTM) model, which we coin Guided LSTM or G-LSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim of guiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search in order to prevent it from favoring short sentences. On various benchmark datasets, we obtain results that are on par with or even outperform the current state-of-the-art.</Data></Cell>
    <Cell><Data ss:Type="String">Xu Jia*, KU Leuven; Stratis Gavves, KU Leuven; Basura Fernando, The Australian National University; Tinne Tuytelaars, KU Leuven</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">227</Data></Cell>
    <Cell><Data ss:Type="String">Towards Computational Baby Learning: A Weakly-supervised Approach for Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">Intuitive observations show that a baby may inherently possess the capability of recognizing a new visual concept (e.g., chair, dog) by learning from only very few positive instances taught by parent(s) or others, and this recognition capability can be gradually further improved by exploring and/or interacting with the real instances in the physical world. Inspired by these observations, we propose a computational model for weakly-supervised object detection, based on prior knowledge modelling, exemplar learning and learning with video contexts. The prior knowledge is modeled with a pre-trained Convolutional Neural Network (CNN). When very few instances of a new concept are given, an initial concept detector is built by exemplar learning over the deep features {the pre-trained CNN. %Simulating the baby's interaction with physical world,   The well-designed tracking solution is then used to discover more diverse instances from the massive online weakly labeled} videos. Once a positive instance is detected/identified with high score in each video, more {instances} possibly from different view-angles and/or different distances are tracked and accumulated.  Then the concept detector can be fine-tuned based on these new instances. This process can be repeated again and again till we obtain a very mature concept detector. Extensive experiments on Pascal VOC-07/10/12 object detection datasets well demonstrate the effectiveness of our framework. It can beat the state-of-the-art full-training based performances by learning from very few samples for each object category, along with about 20,000 weakly labeled videos.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaodan Liang, Sun Yat-sen University; Si Liu, ; Yunchao Wei, ; Luoqi Liu, ; Liang Lin*, ; Shuicheng Yan, National University of Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">233</Data></Cell>
    <Cell><Data ss:Type="String">Generic Promotion of Diffusion-Based Salient Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">In this work, we propose a generic scheme to promote any diffusion-based salient object detection algorithm by original ways to re-synthesize the diffusion matrix and construct the seed vector. We first make a novel analysis of the working mechanism of the diffusion matrix, which reveals the close relationship between saliency diffusion and spectral clustering. Following this analysis, we propose to re-synthesize the diffusion matrix from the most discriminative eigenvectors after adaptive re-weighting. Further, we propose to generate the seed vector based on the readily available diffusion maps, avoiding extra computation for color-based seed search.  As a particular instance, we use inverse normalized Laplacian matrix as the original diffusion matrix and promote the corresponding salient object detection algorithm, which leads to superior performance as experimentally demonstrated.</Data></Cell>
    <Cell><Data ss:Type="String">Peng Jiang*, SHANDONG UNIVERSITY; Nuno Vasconcelos, UC San Diego, USA; Jingliang Peng, Shandong University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">234</Data></Cell>
    <Cell><Data ss:Type="String">Nighttime Haze Removal with Glow and Multiple Light Colors</Data></Cell>
    <Cell><Data ss:Type="String">This paper focuses on dehazing nighttime images.  Most existing dehazing methods use models that are formulated to describe haze in daylight.  Daytime models assume a single uniform light color attributed to a light source not directly visible in the scene.  Nighttime scenes, however, commonly include visible lights sources with varying colors visible.  Moreover, these light sources often introduce noticeable amounts of glow that is not present in daytime haze.   To address these effects, we introduce a new nighttime haze model that accounts for the varying light sources and their glow.  Our model is a linear combination of three terms: the direct transmission, airlight and glow. The glow term represents light directly from the light sources that is scattered before reaching the camera.  Based on the model, we propose a framework that first reduces the effect of the glow in the image, resulting in an nighttime image that consists of direct transmission and airlight only.  we then compute a spatially varying atmospheric light map that encodes light colors locally. This atmospheric map is used to predict the transmission, which we use to obtain our nightime scene reflection image. We demonstrate the efficacy of our nighttime dehaze model and correction method on a number of examples and compare our results with existing daytime and nighttime dehazing methods' results.</Data></Cell>
    <Cell><Data ss:Type="String">YU LI*, NUS; Robby Tan, SIM University; Michael Brown, &quot;National University of Singapore, Singapore&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">235</Data></Cell>
    <Cell><Data ss:Type="String">Learning Optical Flow with Convolutional Neural Networks</Data></Cell>
    <Cell><Data ss:Type="String">Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations.    Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.</Data></Cell>
    <Cell><Data ss:Type="String">Philipp Fischer*, University of Freiburg; Alexey Dosovitskiy, University of Freiburg; Eddy Ilg, University of Freiburg; Philip Häusser, Technische Universität München; Caner Hazırbaş, Technische Universität München; Vladimir Golkov, Technische Universität München; Patrick van der Smagt, Technische Universität München; Daniel Cremers, TUM; Thomas Brox, &quot;University of Freiburg, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">239</Data></Cell>
    <Cell><Data ss:Type="String">An Efficient Minimal Solution for Multi-Camera Motion</Data></Cell>
    <Cell><Data ss:Type="String">We propose an efficient method for estimating the motion of a multi-camera rig from a minimal set of feature correspondences.  Existing methods to the multi-camera relative pose problem require extra correspondences, are slow to compute, and/or produce a multitude of solutions.  Our solution uses a first-order approximation to relative pose in order to simplify the problem and produce an accurate estimate quickly.  The solver is applicable to sequential multi-camera motion estimation and is fast enough for real-time implementation in a random sampling framework.  Our experiments show that our approach is both stable and efficient on challenging test sequences.</Data></Cell>
    <Cell><Data ss:Type="String">Jonathan Ventura*, UC Colorado Springs; Clemens Arth, Graz University of Technology; Vincent Lepetit, TU Graz</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">245</Data></Cell>
    <Cell><Data ss:Type="String">Just Noticeable Differences in Visual Attributes</Data></Cell>
    <Cell><Data ss:Type="String">We explore the problem of predicting &quot;just noticeable differences&quot; in a visual attribute.  While some pairs of images have a clear ordering for an attribute (e.g., A is more sporty than B), for others the difference may be indistinguishable to human observers.  However, existing relative attribute models are unequipped to infer partial orders on novel data.   Attempting to map relative attribute ranks to equality predictions is non-trivial, particularly since the span of indistinguishable pairs in attribute space may vary in different parts of the feature space.  We develop a Bayesian local learning strategy to infer when images are indistinguishable for a given attribute.  On the UT-Zap50K shoes and LFW-10 faces datasets, we outperform a variety of alternative methods.  In addition, we show the practical impact on fine-grained visual search.</Data></Cell>
    <Cell><Data ss:Type="String">Aron Yu*, UT Austin ; Kristen Grauman, University of Texas at Austin</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">258</Data></Cell>
    <Cell><Data ss:Type="String">Confidence Preserving Machine for Facial Action Unit Detection</Data></Cell>
    <Cell><Data ss:Type="String">Automatic facial action unit detection from video is a long-standing problem in facial expression analysis. Most existing methods are supervised in nature, and max-margin classifiers based on SVM are among the most popular ones. Standard classifiers use a single margin to separate positive from negative classes. A single margin is typically insufficient to separate ambiguous samples that are close to the margin. Typically, the classification errors are in frames with a low intensity AU, or frames with large pose changes.  To improve classification performance in these cases, this paper proposes the Confidence Preserving Machine (CPM). CPM is built on the intuition that easy and hard samples should not be recognized simultaneously, but in an easy-to-hard order. Since we are not provided whether a sample is easy or hard, CPM builds dual classifiers with two margins to separate easy and hard data. The dual classifiers identify the easy data, and then the predictions are propagated from easy to hard ones. Experiments on state-of-the-art datasets  such as  GFT, SUNY, and DISFA, illustrate the benefit of our method over state-of-the-art semi-supervised learning, transfer learning, and boosting methods.</Data></Cell>
    <Cell><Data ss:Type="String">Jiabei Zeng*, Beihang University; Wen-Sheng Chu, CMU; Fernando De la Torre, &quot;Carnegie Mellon University, USA&quot;; Jeffrefy Cohn, University of Pittsburgh; Zhang Xiong, Beihang University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">260</Data></Cell>
    <Cell><Data ss:Type="String">Learning Semi-Supervised Representation Towards a Unified Optimization Framework for Semi-Supervised Learning</Data></Cell>
    <Cell><Data ss:Type="String">State of the art approaches for Semi-Supervised Learning (SSL) usually follow a two-stage framework &#45;- constructing an affinity matrix from the data and then propagating the partial labels on this affinity matrix to infer those unknown labels. While such a two-stage framework has been successful in many applications, solving two subproblems separately is still suboptimal because it does not exploit the fact that the affinity and the labels depend upon each other. In this paper, we attempt to formulate the two stages of SSL into a unified optimization framework &#45;- to the best of our knowledge, this is the first time &#45;- which learns both the affinity matrix and the unknown labels simultaneously. To this end, we propose a novel semi-supervised self-expressiveness model in which the given labels and the estimated labels are incorporated to induce the affinity. We solve the unified optimization problem via an alternating direction method of multipliers combined with label propagation. Extensive experiments on a synthetic data set and several benchmark data sets demonstrate the effectiveness of our approach.</Data></Cell>
    <Cell><Data ss:Type="String">Chun-Guang Li*, Beijing Univ.of Posts&amp;Telecom.; Zhouchen Lin, &quot;Peking University, China&quot;; Honggang Zhang, Beijing Univ. of Posts &amp; Telecomunications; Jun Guo, Beijing Univ. of Posts&amp;Telecom</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">262</Data></Cell>
    <Cell><Data ss:Type="String">Conformal and Low-Rank Sparse Representation for Image Restoration</Data></Cell>
    <Cell><Data ss:Type="String">Obtaining an appropriate dictionary is the key point when sparse representation is applied to computer vision or image processing problems such as image restoration. It is expected that preserving data structure during sparse coding and dictionary learning can enhance the recovery performance. However, many existing dictionary learning methods handle training samples individually, while missing relationships between samples, which result in dictionaries with redundant atoms but poor representation ability. In this paper, we propose a novel sparse representation approach called conformal and low-rank sparse representation (CLRSR) for image restoration problems. To achieve a more compact and representative dictionary, conformal property is introduced by preserving the angles of local geometry formed by neighboring samples in the feature space. Furthermore, imposing low-rank constraint on the coefficient matrix can lead more faithful subspaces and capture the global structure of data. We apply our CLRSR model to several image restoration tasks to demonstrate the effectiveness.</Data></Cell>
    <Cell><Data ss:Type="String">Jianwei Li*, Beihang University; Xiaowu Chen, Beihang University; Dongqing Zou, Beihang University; Bo Gao, Beihang University; Bin Zhou, Beihang University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">271</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Synchrony Discovery in Human Interaction</Data></Cell>
    <Cell><Data ss:Type="String">Humans are inherently social. Human interaction, therefore, plays an important and natural role to understand human behavior. Most computational methods emphasize on learning individual behaviors, and usually require adequate labelled training data. This paper presents an unsupervised approach to discover interpersonal synchrony, referred as to individuals' overlapping common events during social interaction. Unlike a naive approach that exhaustively evaluates temporal regions with different lengths and locations, we present a branch-and-bound (B\&amp;B) approach that allows us to avoid exhaustive search yet guarantees a globally optimal solution. The proposed method is general, taking any signals represented as histograms and applicable to more than two sequences. In addition, we derive three new bounding functions, and provide efficient extensions for multi-synchrony detection and acceleration using a warm-start strategy and parallelism. We evaluate the effectiveness of our approach in multiple databases, including human actions using the CMU Mocap dataset, spontaneous facial behaviors using group-formation task and parent-infant interaction dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Wen-Sheng Chu*, CMU; Jiabei Zeng, Beihang University; Fernando De la Torre, &quot;Carnegie Mellon University, USA&quot;; Jeffrefy Cohn, University of Pittsburgh; Daniel Messinger, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">272</Data></Cell>
    <Cell><Data ss:Type="String">Patch Group Based Nonlocal Self-Similarity Prior Learning for Image Denoising</Data></Cell>
    <Cell><Data ss:Type="String">Patch based image modeling has achieved a great success in low level vision such as image denoising. In particular, the use of image nonlocal self-similarity (NSS) prior, which refers to the fact that a local patch often has many nonlocal similar patches to it across the image, has significantly enhanced the denoising performance. Despite the success of NSS in image restoration, in most existing methods only the NSS of input degraded image is exploited, while how to utilize the NSS of clean natural images is still an open problem. In this paper, we propose a patch group (PG) based NSS prior learning scheme to learn explicit NSS models from natural images for high performance denoising. PGs are extracted from training images by putting nonlocal similar patches into groups, and a PG based Gaussian Mixture Model (PG-GMM) learning algorithm is developed to learn the NSS prior. We demonstrate that, owe to the learned PG-GMM, a simple weighted sparse coding model, which has a closed-form solution, can be used to perform image denoising effectively, resulting in high PSNR measure, fast speed, and particularly the best visual quality among all competing methods.</Data></Cell>
    <Cell><Data ss:Type="String">Jun Xu, The Hong Kong Polytechnic University; Lei Zhang*, The Hong Kong Polytechnic University; Wangmeng Zuo, Harbin Institute of Technology; David Zhang, Hong Kong Polytechnic University; Xiangchu Feng, Xidian University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">275</Data></Cell>
    <Cell><Data ss:Type="String">Automatic Thumbnail Generation Based on Visual Representativeness and Foreground Recognizability</Data></Cell>
    <Cell><Data ss:Type="String">We present an automatic thumbnail generation technique based on two essential considerations: how well they visually represent the original photograph, and how well the foreground can be recognized after the cropping and downsizing steps of thumbnailing. These factors, while important for the image indexing purpose of thumbnails, have largely been ignored in previous methods, which instead are designed to highlight salient content while disregarding the effects of downsizing. We propose a set of image features for modeling these two considerations of thumbnails, and learn how to balance their relative effects on thumbnail generation through training on image pairs composed of photographs and their corresponding thumbnails created by an expert photographer. Experiments show the effectiveness of this approach on a variety of images, as well as its advantages over related techniques.</Data></Cell>
    <Cell><Data ss:Type="String">Jingwei Huang, Tsinghua University; Huarong Chen*, Tsinghua University; Bin Wang, Tsinghua University; Stephen  Lin, &quot;Microsoft Research Asia, China&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">283</Data></Cell>
    <Cell><Data ss:Type="String">Extraction of Illusory Baselines from Distorted Document Images Using Curvilinear Projection</Data></Cell>
    <Cell><Data ss:Type="String">Baselines of a document image are a bundle of illusory horizontal and parallel lines, to which the printed contents of document, \eg, text lines, tables or inserted photos, are aligned. Accurate baseline extraction is of great importance in the geometric correction of curved document images. In this paper, we propose an efficient method for accurate extraction of these illusory visual clues from a curved document image. Our method comes from two basic observations that the baselines of documents do not intersect with each other and that within a narrow strip, the baselines can be well approximated by linear segments. Based upon these observations, we propose a curvilinear projection based method and model the estimation of curved baselines as a constrained sequential optimization problem. A dynamic programming algorithm is then developed to efficiently solve the problem. The proposed method can extract the complete baselines through each pixel of document images in a high accuracy. It is also scripts insensitive and high robust to image noises, non-textual objects, image resolutions and image quality degradation like blurring and non-uniform illumination. Extensive experiments on a number of real-captured curved document images demonstrate the effectiveness of the proposed method.</Data></Cell>
    <Cell><Data ss:Type="String">Gaofeng MENG*, NLPR, CASIA; Zuming HUANG, ; Yonghong SONG, ; Shiming XIANG, ; Chunhong PAN, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">291</Data></Cell>
    <Cell><Data ss:Type="String">Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we tackle two key aspects of multiple target tracking problem: 1) designing an accurate affinity measure to associate detections and 2) implementing an efficient and accurate (near) online multiple target tracking algorithm. As the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI and MOT datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10% higher MOTA) over the state-of-the-arts.</Data></Cell>
    <Cell><Data ss:Type="String">Wongun Choi*, NEC Labs</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">293</Data></Cell>
    <Cell><Data ss:Type="String">Joint Camera Clustering and Surface Segmentation for Large-scale Multi-view Stereo</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose an optimal decomposition approach to large-scale multi-view stereo from an initial sparse reconstruction.  The success of the approach depends on the introduction of surface-segmentation-based camera clustering rather than sparse-point-based camera clustering, which suffers from the problems of non-uniform reconstruction coverage ratio and high redundancy. In details, we introduce three criteria for clustering camera and surface segmentation for reconstruction, and then we formulate these criteria into an energy minimization problem under constraints. To solve this problem, we propose a joint optimization in a hierarchical framework to obtain the final surface segments and corresponding optimal camera clusters. On each level of the hierarchical framework, the camera clustering problem is formulated as a parameter estimation problem of a probability model solved by a general expectation-maximization algorithm and the surface segmentation problem is formulated as a Markov Random Field model based on the probability estimated by the previous camera clustering process. The experiments on several Internet datasets and aerial photo datasets demonstrate that the proposed approach method generates more uniform and complete dense reconstruction with less redundancy, resulting in more efficient multi-view stereo algorithm.</Data></Cell>
    <Cell><Data ss:Type="String">Runze Zhang, HKUST; Shiwei Li, HKUST; Tian Fang*, HKUST; Siyu Zhu, HKUST; Long Quan, &quot;The Hong Kong University of  Science and Technology, Hong Kong&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">294</Data></Cell>
    <Cell><Data ss:Type="String">SALICON: Bridging the Semantic Gap in Saliency Prediction</Data></Cell>
    <Cell><Data ss:Type="String">Saliency in Context (SALICON) is an ongoing effort that aims at understanding and predicting visual attention. Conventional saliency models typically rely on low-level image statistics to predict human fixations. While these models perform significantly better than chance, there is still a large gap between model prediction and human behavior. This gap is largely due to the limited capability in predicting eye-fixations with strong semantic contents, the so-called semantic gap. This paper presents a focused study to fill the semantic gap with an effective architecture based on Deep Neural Network (DNN). It leverages the representational power of high-level semantics encoded in DNNs pretrained for object recognition. Two key components are fine-tuning the DNNs with an objective function based on the saliency evaluation metrics, and integrating information at different image scales. We compare our method with $14$ saliency models on $6$ public eye tracking benchmark datasets. Results demonstrate that our DNNs can automatically learn features particularly for saliency prediction that surpass by a big margin the state-of-the-art. In addition, our model ranks top to date under all seven metrics on the MIT300 challenge set.</Data></Cell>
    <Cell><Data ss:Type="String">Xun Huang*, National University of Singapore; Chengyao Shen, National University of Singapo; Xavier Boix, NUS; Qi Zhao, National University of Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">302</Data></Cell>
    <Cell><Data ss:Type="String">A Novel Sparsity Measure for Tensor Recovery</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a new sparsity regularizer for measuring the low-rank structure underneath a tensor. The proposed sparsity measure not only has a natural physical meaning and can be interpreted as the size of the fundamental Kronecker basis to express the tensor, but also unifies the sparsity measures throughout vector to matrix. Furthermore, by embedding the sparsity measure into the tensor completion and tensor robust PCA frameworks, we formulate new models to enhance their capability in tensor recovery. Through introducing convex/non-convex relaxation forms of the proposed sparsity measure, we also adopt the alternating direction method of multipliers (ADMM) for solving the proposed models. Experiments implemented on synthetic and multispectral image data sets substantiate the effectiveness of the proposed methods.</Data></Cell>
    <Cell><Data ss:Type="String">Qian Zhao*, Xi'an Jiaotong University; Deyu Meng, Xi'an Jiaotong University; Xu Kong, ; Qi Xie, Xi'an Jiaotong University; Wenfei Cao, ; Yao Wang, ; Zongben Xu, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">303</Data></Cell>
    <Cell><Data ss:Type="String">Single-shot Specular Surface Reconstruction with Gonio-plenoptic Imaging </Data></Cell>
    <Cell><Data ss:Type="String">We present a gonio-plenoptic imaging system that realizes a single-shot shape measurement for specular surfaces. The system is comprised of a collimated illumination source and a plenoptic camera. Unlike a conventional plenoptic camera, our system captures the BRDF variation of the object surface in a single image in addition to the light field information from the scene, which allows us to recover very fine 3D structures of the surface. The shape of the surface is reconstructed based on the reflectance property of the material rather than the parallax between different views. Since only a single-shot is required to reconstruct the whole surface, our system is able to capture dynamic surface deformation in a video mode. We also describe a novel calibration technique that maps the light field viewing directions from the object space to subpixels on the sensor plane. The proposed system is evaluated using a concave mirror with known curvature, and is compared to a parabolic mirror scanning system as well as a multi-illumination photometric stereo approach based on simulations and experiments.</Data></Cell>
    <Cell><Data ss:Type="String">Lingfei Meng*, Ricoh Innovations Corporation; Liyang Lu, Rice University; Noah Berdard, Ricoh Innovations Corporation; Kathrin Berkner, Ricoh Innovations Corporation</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">307</Data></Cell>
    <Cell><Data ss:Type="String">Oriented Object Proposals</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a new approach to generate oriented object proposals (OOPs) to reduce the detection error caused by various orientations of the object. To this end, we propose to efficiently locate object regions according to pixelwise object probability, rather than measuring the objectness from a set of sampled windows. We formulate the proposal generation problem as a generative probabilistic model such that object proposals of different shapes (i.e., sizes and orientations) can be produced by locating the local maximum likelihoods. The new approach has three main advantages. First, it helps the object detector handle objects of different orientations. Second, as the shapes of the proposals may vary to fit the objects, the resulting proposals are tighter than the sampling windows with fixed sizes. Third, it avoids massive window sampling, and thereby reducing the number of proposals while maintaining a high recall. Experiments on the PASCAL VOC 2007 dataset show that the proposed OOP outperforms the state-of-the-art fast methods. Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios. Generating OOPs is very fast and takes only 0.5s per image.</Data></Cell>
    <Cell><Data ss:Type="String">Shengfeng He*, City University of Hong Kong; Rynson Lau, City University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">311</Data></Cell>
    <Cell><Data ss:Type="String">VQA: Visual Question Answering</Data></Cell>
    <Cell><Data ss:Type="String">We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring many real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple choice format. We provide a dataset containing 100,000's of images and questions and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.</Data></Cell>
    <Cell><Data ss:Type="String">Stanislaw Antol*, Virginia Tech; Aishwarya Agrawal, Virginia Tech; Jiasen Lu, Virginia Tech; Margaret Mitchell, Microsoft Research; Dhruv Batra, Virginia Tech; Larry Zitnick, Microsoft Research Redmond, USA; Devi Parikh, Virginia Tech, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">314</Data></Cell>
    <Cell><Data ss:Type="String">Learning Shape, Motion and Elastic Models in Force Space</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we address the problem of simultaneously recovering the 3D shape and pose of a deformable and potentially elastic object from 2D motion. This is a highly ambiguous problem typically tackled by using  low-rank shape and trajectory constraints.  We show that formulating the problem in terms of a low-rank force space that induces the deformation, allows for a better physical interpretation of the resulting priors and a more accurate representation of the actual object's behavior. However, this comes at the price of, besides force and pose, having to estimate the elastic model of the object. For this, we use an Expectation Maximization strategy, where each of these parameters are successively learned within partial M-steps, while robustly dealing with missing observations. We thoroughly validate the approach on both mocap and real sequences, showing more accurate 3D reconstructions than state-of-the-art, and additionally providing an estimate of the full elastic model with no a priori information.</Data></Cell>
    <Cell><Data ss:Type="String">Antonio Agudo*, I3A - Universidad de Zaragoza; Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC)</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">315</Data></Cell>
    <Cell><Data ss:Type="String">Improving Image Classification with Location Context</Data></Cell>
    <Cell><Data ss:Type="String">With the widespread availability of cellphones and cameras that have GPS capabilities, it is common for images being uploaded to the Internet today to have GPS coordinates associated with them. In addition to research that tries to predict GPS coordinates from visual features, this also opens up the door to problems that are conditioned on the availability of GPS coordinates. In this work, we tackle the problem of performing image classification with location context, in which we are given the GPS coordinates for images in both the train and test phases. We explore different ways of encoding and extracting features from the GPS coordinates, and show how to naturally incorporate these features into a Convolutional Neural Network (CNN), the current state-of-the-art for most image classification and recognition problems. We also show how it is possible to simultaneously learn the optimal pooling radii for a subset of our features within the CNN framework. To evaluate our model and to help promote research in this area, we identify a set of location-sensitive concepts and annotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has GPS coordinates with these concepts, which we make publicly available. By leveraging location context, we are able to achieve almost a 7% gain in mean average precision.</Data></Cell>
    <Cell><Data ss:Type="String">Kevin Tang*, Stanford University; Manohar Paluri, ; Fei-Fei Li, Stanford University; Rob Fergus, New York University; Lubomir Bourdev, Facebook Inc.</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">316</Data></Cell>
    <Cell><Data ss:Type="String">Context-aware diffusion for label propagation on graphs</Data></Cell>
    <Cell><Data ss:Type="String">Existing approaches for diffusion on graphs, e.g., for label propagation, are mainly focused on isotropic diffusion, which is induced by the commonly used graph Laplacian regularizer. Inspired by the success of diffusivity tensors for anisotropic diffusion in image processing, we presents anisotropic diffusion on graphs and the corresponding label propagation algorithm. We develop positive definite diffusivity operators on the vector bundles of Riemannian manifolds, and discretize them to diffusivity operators on graphs. This enables us to easily define new robust diffusivity operators which significantly improve semi-supervised learning performance over existing diffusion algorithms. </Data></Cell>
    <Cell><Data ss:Type="String">Kwang In Kim*, Lancaster University; James Tompkin, Harvard SEAS; Hanspeter Pfister, Harvard University; Christian Theobalt, MPI Informatics</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">317</Data></Cell>
    <Cell><Data ss:Type="String">Depth-based hand pose estimation: data, methods, and challenges</Data></Cell>
    <Cell><Data ss:Type="String">Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.</Data></Cell>
    <Cell><Data ss:Type="String">James Supancic III*, UC Irvine; Deva Ramanan, UC Irvine; Gregory Rogez, UC Irvine; Yi Yang, Baidu IDL; Jamie Shotton, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">320</Data></Cell>
    <Cell><Data ss:Type="String">Adaptive Dither Voting for Robust Spatial Verification</Data></Cell>
    <Cell><Data ss:Type="String">Hough voting in a geometric transformation space allows us to realize spatial verification, but remains sensitive to feature detection errors because of the inflexible quantization of single feature correspondences. To handle this problem, we propose a new method, called adaptive dither voting, for robust spatial verification. For each correspondence, instead of hard-mapping it to a single transformation, the method augments its description by using multiple dithered transformations that are deterministically generated by the other correspondences. The method reduces the probability of losing correspondences during transformation quantization, and provides high robustness as regards mismatches by imposing three geometric constraints on the dithering process. We also propose exploiting the non-uniformity of a Hough histogram as the spatial similarity to handle multiple matching surfaces. Extensive experiments conducted on four datasets show the superiority of our method. The method outperforms its state-of-the-art counterparts in both accuracy and scalability, especially when it comes to the retrieval of small, rotated objects.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaomeng Wu*, Nippon Telegraph and Telephone Corporation; Kunio Kashino, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">325</Data></Cell>
    <Cell><Data ss:Type="String">Learning Deconvolution Network for Semantic Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained with no external data through ensemble with the fully convolutional network.</Data></Cell>
    <Cell><Data ss:Type="String">Hyeonwoo Noh, POSTECH; Seunghoon  Hong, POSTECH; Bohyung Han*, POSTECH, Pohang, Korea</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">326</Data></Cell>
    <Cell><Data ss:Type="String">Robust RGB-D Odometry Using Point and Line Features</Data></Cell>
    <Cell><Data ss:Type="String">Lighting variation and uneven feature distribution are main challenges for indoor RGB-D visual odometry where color information is often combined with depth information. To meet the challenges, we fuse point and line features to form a robust odometry algorithm. Line features are abundant indoors and less sensitive to lighting change than points. We extract 3D points and lines from RGB-D data, analyze their measurement uncertainties, and compute camera motion using maximum likelihood estimation. We prove that fusing points and lines produces smaller motion estimate uncertainty than using either feature type alone. In experiments we compare our method with state-of-the-art methods including a keypoint-based approach and a dense visual odometry  algorithm. Our method outperforms the counterparts under both constant and varying lighting conditions. Specifically, our method achieves an average translational error that is 34.9\% smaller than the counterparts, when tested using public datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Yan Lu, Texas A&amp;M University; Dezhen Song*, Texas A&amp;M University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">327</Data></Cell>
    <Cell><Data ss:Type="String">TransCut: Transparent Object Segmentation from a Light-Field Image</Data></Cell>
    <Cell><Data ss:Type="String">The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary.  We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.</Data></Cell>
    <Cell><Data ss:Type="String">Yichao Xu*, Kyushu University; Hajime Nagahara, Kyushu University; Atsushi  Shimada, ; Rin-ichiro  Taniguchi, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">328</Data></Cell>
    <Cell><Data ss:Type="String">HICO: A Benchmark for Recognizing Human-Object Interactions in Images</Data></Cell>
    <Cell><Data ss:Type="String">We introduce a new benchmark &quot;Humans Interacting with Common Objects&quot; (HICO) for recognizing human-object interactions (HOI). We demonstrate the key features of HICO: a diverse set of interactions with common object categories, a list of well-defined, sense-based HOI categories, and an exhaustive labeling of co-occurring interactions with an object category in each image. We perform an in-depth analysis of representative current approaches and show that DNNs enjoy a significant edge. In addition, we show that semantic knowledge can significantly improve HOI recognition, especially for uncommon categories. </Data></Cell>
    <Cell><Data ss:Type="String">Yu-Wei Chao*, University of Michigan; Zhan Wang, University of Michigan; Yugeng He, University of Michigan; Jiaxuan Wang, University of Michigan; Jia Deng, University of Michigan</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">329</Data></Cell>
    <Cell><Data ss:Type="String">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</Data></Cell>
    <Cell><Data ss:Type="String">Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%) on this visual recognition challenge.</Data></Cell>
    <Cell><Data ss:Type="String">Kaiming He*, Microsoft Research Asia; Xiangyu Zhang, Xi'an Jiaotong University; Shaoqing Ren, USTC; Jian Sun, Microsoft Research China</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">334</Data></Cell>
    <Cell><Data ss:Type="String">Learning-to-Rank based on Subsequences</Data></Cell>
    <Cell><Data ss:Type="String">We present a supervised learning to rank algorithm that effectively orders images by exploiting the structure in image sequences especially focusing on image re-ranking applications. Most often in the supervised learning to rank  literature, ranking is approached either by analyzing pairs of images or by optimizing a list-wise surrogate loss function on full sequences. In this work we propose MidRank, which learns from moderately sized sub-sequences instead. These sub-sequences contain useful structural ranking information that leads to better learnability during training and better generalization during testing. By exploiting sub-sequences, the proposed MidRank improves ranking accuracy considerably on an extensive array of image re-ranking applications and datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Basura Fernando*, The Australian National University; Stratis Gavves, KU Leuven; Damien Muselet, Universite Jean Monnet; Tinne Tuytelaars, KU Leuven</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">336</Data></Cell>
    <Cell><Data ss:Type="String">Learning Nonlinear Spectral Filters for Color Image Reconstruction</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents the idea of learning optimal filters for color image reconstruction based on a novel concept of nonlinear spectral image decompositions recently proposed by Guy Gilboa. We use a multiscale image decomposition approach based on total variation regularization and Bregman iterations to represent the input data as the sum of image layers containing features at different scales. Filtered images can be obtained by weighted linear combinations of the different frequency layers. We introduce the idea of learning optimal filters for the task of image denoising, and propose the idea of mixing high frequency components of different color channels. Our numerical experiments demonstrate that learning the optimal weights can significantly improve the results in comparison to the standard variational approach, and achieves state-of-the-art image denoising results.</Data></Cell>
    <Cell><Data ss:Type="String">Julia Diebold*, TU Munich; Michael Moeller, ; Daniel Cremers, TUM; Guy Gilboa, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">337</Data></Cell>
    <Cell><Data ss:Type="String">Alternating Co-Quantization for Cross-modal Hashing</Data></Cell>
    <Cell><Data ss:Type="String">  This paper addresses the problem of unsupervised learning of binary hash codes for efficient cross-modal retrieval. Many unimodal hashing studies have proven that both similarity preservation of data and maintenance of quantization quality are essential for improving retrieval with binary hash codes. However, most existing cross-modal hashing methods mainly have focused on the former, and the latter still remains almost untouched.     We propose a novel method to minimize the binary quantization errors, which is tailored to cross-modal hashing. Our approach, named Alternating Co-Quantization (ACQ), alternately seeks orthogonal binary quantizers for each modality space with the help of connections to other modality data, which give minimal quantization errors while preserving data similarities. ACQ can be coupled with many existing cross-modal dimension reduction methods such as Canonical Correlation Analysis (CCA) and substantially boosts their retrieval performance in the Hamming space. Experiments demonstrate that ACQ outperforms several state-of-the-art methods, even when combined with simple CCA.</Data></Cell>
    <Cell><Data ss:Type="String">Go Irie*, NTT; Hiroyuki Arai, NTT; Yukinobu Taniguchi, Tokyo University of Science</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">340</Data></Cell>
    <Cell><Data ss:Type="String">Beyond White: Ground Truth Colors for Color Constancy Correction</Data></Cell>
    <Cell><Data ss:Type="String">A limitation in color constancy research is the inability to established ground truth colors for evaluating corrected images. Many existing data sets contain images of scenes with a color chart included, however, only the chart's neutral colors (grayscale patches) are used to provide the ground truth for illumination estimation and correction.  This is because the corrected neutral colors are known to lie along the achromatic line in the camera's color space (i.e. R=G=B) ; the corrected RGB values of the other color patches are not known.  As a result, most methods estimate a 3*3 diagonal matrix that ensures only the neutral colors are correct.  In this paper, we describe how to overcome this limitation.   Specifically, we show that under certain illuminants, the diagonal 3*3 matrix provides nearly optimal results for all the colors in a scene.   This means that we can use images under certain illuminations to provide the ground truth colors for the entire data set.  This allows us to estimate full 3*3 matrices using all the patches on the color chart.  Working from this new ground truth data, we describe how to modify existing algorithms to perform better image correction.</Data></Cell>
    <Cell><Data ss:Type="String">Dongliang Cheng*, NUS; Brian Price, ; Scott Cohen, Adobe; Michael Brown, &quot;National University of Singapore, Singapore&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">343</Data></Cell>
    <Cell><Data ss:Type="String">Conditional Random Fields as Recurrent Neural Networks</Data></Cell>
    <Cell><Data ss:Type="String">Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network which combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based graphical modelling. To this end, we formulate Conditional Random Fields as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.</Data></Cell>
    <Cell><Data ss:Type="String">Shuai Zheng*, University of Oxford; Sadeep Jayasumana, University of Oxford; Bernardino Romera-Paredes, University of Oxford; Vibhav Vineet, Stanford University; Zhizhong Su, Baidu Research; Dalong Du, Baidu; Chang Huang, Baidu; philip Torr, University of Oxford</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">346</Data></Cell>
    <Cell><Data ss:Type="String">A Versatile Scene Model with Differentiable Visibility Applied to Generative Pose Estimation</Data></Cell>
    <Cell><Data ss:Type="String">Generative reconstruction methods compute   the 3D configuration (such as pose and/or geometry) of a shape by optimizing the overlap of the projected 3D shape model with images.  Proper handling of occlusions is a big challenge, since the visibility function that indicates if a surface point is seen from a camera can often not be formulated in closed form, and is in general discrete and non-differentiable at occlusion boundaries.  We present a new scene representation that enables an analytically differentiable closed-form formulation of surface visibility.  This yields smooth, analytically differentiable, and efficient to optimize pose similarity energies with rigorous occlusion handling.  In contrast to previous methods that use non-differentiable or only locally differentiable approximations of visibility,   our visibility formulation is differentiable everywhere in the scene. This yields less multi-modal energy functions and experimentally verified improved convergence of numerical optimization.   The underlying idea is a new image formation model that represents opaque objects by a translucent medium with a smooth Gaussian density distribution which turns visibility into a smooth phenomenon.  We demonstrate the advantages of our versatile scene model in several generative pose estimation problems, namely marker-less   multi-object pose estimation, marker-less motion capture with few cameras, and image-based 3D geometry estimation.</Data></Cell>
    <Cell><Data ss:Type="String">Helge Rhodin*, MPI for informatics; Nadia Robertini, MPI for Informatics; Christian Richardt, Saarland University; Hans-Peter Seidel, ; Christian Theobalt, MPI Informatics</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">350</Data></Cell>
    <Cell><Data ss:Type="String">Depth Recovery from Light Field Using Focal Stack Symmetry</Data></Cell>
    <Cell><Data ss:Type="String">We describe a technique to recover depth from a light  field (LF) using two proposed features of the LF focal s-  tack. One feature is the property that non-occluding pix-  els exhibit symmetry along the focal depth dimension cen-  tered at the in-focus slice. The other is a data consistency  measure based on analysis-by-synthesis, i.e., the difference  between the synthesized focal stack given the hypothesized  depth map and that from the LF. These terms are used in  an iterative optimization framework to extract scene depth.  Experimental results on real Lytro and Raytrix data demon-  strate that our technique outperforms state-of-the-art solu-  tions and is significantly more robust to noise and under-  sampling.</Data></Cell>
    <Cell><Data ss:Type="String">Haiting Lin*, Udel; Can Chen, University of Delaware; Sing Bing Kang, Microsoft Research; Jingyi Yu, &quot;University of Delaware, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">352</Data></Cell>
    <Cell><Data ss:Type="String">Parsimonious Labeling</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we deal with the problem of multi-label inference over a given MRF/CRF with high-order interactions. The problem of inference plays an important role in computer vision. Despite the fact that using high-order interactions can improve our modelling capabilities, lack of efficient inference algorithms with provable optimality guarantees compel us to model our problem with pairwise interactions only.    The contributions of this paper are two folds. Firstly, we propose a family of high-order clique potentials, called diversities, which can be used to model variety of high-order interactions. Secondly, we propose a novel hierarchical move making algorithm (Hierarchical Pn-Potts model) which can optimize diversities in an efficient manner while giving optimality guarantees.   </Data></Cell>
    <Cell><Data ss:Type="String">Puneet Dokania*, CentraleSupelec and INRIA Saclay; M. Pawan Kumar, CentraleSupelec</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">355</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Learning of Visual Representations using Videos</Data></Cell>
    <Cell><Data ss:Type="String">Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a ConvNet? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of ConvNets. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that we track millions of patches in these videos. Visual tracking provides the key supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this ConvNet representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves $~52\%$ mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of $~53.6\%$. We also show that our unsupervised network can perform competitive in other tasks such as surface-normal estimation.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaolong Wang*, Carnegie Mellon University; Abhinav Gupta, &quot;Carnegie Mellon University, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">356</Data></Cell>
    <Cell><Data ss:Type="String">Semantic Pose using Deep Networks Trained on Synthetic RGB-D</Data></Cell>
    <Cell><Data ss:Type="String">  In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel on a GPU in seconds.</Data></Cell>
    <Cell><Data ss:Type="String">Jeremie Papon*, University of Goettingen; Markus Schoeler, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">357</Data></Cell>
    <Cell><Data ss:Type="String">Localize me Anywhere, Anytime: A Multi-task Point-Retrieval Approach</Data></Cell>
    <Cell><Data ss:Type="String">Image-based localization is an essential complement to GPS localization. Current image-based localization methods are based on either 2D-to-3D or 3D-to-2D to find the correspondences, which ignore the real scene geometric attributes. The main contribution of our paper is that we use a 3D model reconstructed by a short video as the query to realize 3D-to-3D localization under a multi-task point retrieval framework. Firstly, the use of a 3D model as the query enables us to efficiently select location candidates. Furthermore, the reconstruction of 3D model exploits the correlations among different images, based on the fact that images captured from different views for SfM share information through matching features. By exploring shared information (matching features) across multiple related tasks (images of the same scene captured from different views), the visual feature's view-invariance property can be improved in order to get to a higher point retrieval accuracy. More specifically, we use multi-task point retrieval framework to explore the relationship between descriptors and the 3D points, which extracts the discriminant points for more accurate 3D-to-3D correspondences retrieval. We further apply multi-task learning (MTL) retrieval approach on thermal images to prove that our MTL retrieval framework also provides superior performance for the thermal domain. This application is exceptionally helpful to cope with the localization problem in an environment with limited light sources. </Data></Cell>
    <Cell><Data ss:Type="String">Guoyu Lu*, University of Delaware; Yan Yan, University of Trento; Li Ren, University of Delaware; Jingkuan Song, University of Trento; Nicu  Sebe, &quot;University of Trento, Italy&quot;; Chandra Kambhamettu, U Delaware</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">362</Data></Cell>
    <Cell><Data ss:Type="String">Depth Map Estimation and Colorization of Anaglyph Images Using Local Color Prior and Reverse Intensity Distribution</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we present a joint iterative anaglyph stereo matching and colorization framework for obtaining a set of disparity maps and colorized images. Conventional stereo matching algorithms fail when addressing anaglyph images that do not have similar intensities on their two respective view images. To resolve this problem, we propose two novel data costs using local color prior and reverse intensity distribution factor for obtaining accurate depth maps. To colorize an anaglyph image, each pixel in one view is warped to another view using the obtained disparity values of non-occluded regions. A colorization algorithm using optimization is then employed with additional constraint to colorize the remaining occluded regions. Experimental results confirm that the proposed unified framework is robust and produces accurate depth maps and colorized stereo images.</Data></Cell>
    <Cell><Data ss:Type="String">Williem Williem, ; In Kyu Park*, Inha University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">367</Data></Cell>
    <Cell><Data ss:Type="String">Volumetric Bias in Energy-based Segmentation: Secrets and Solutions</Data></Cell>
    <Cell><Data ss:Type="String">Many popular optimization methods for segmentation and reconstruction compute ML models estimating appearance or geometry of segments, e.g. Zhu-Yuille 1996, Chan-Vese 2001, GrabCut 2004, Delong et al. 2012. We observe that the standard likelihoods term in these formulations corresponds to a probabilistic K-means energy. In the learning community it is known that this energy has a strong bias to clusters of equal size, which can be expressed as a penalty for KL divergence from a uniform distribution of cardinalities [Kearns et al. 1997]. However, this volumetric bias is not well understood in computer vision. We demonstrate significant artifacts in standard segmentation and reconstruction methods due to this bias. Moreover, we propose binary and multi-label optimization techniques that either (a) remove this bias or (b) replace it by KL divergence term for any given target volumes distribution. Our general ideas apply to many continuous or discrete formulations of segmentation, stereo, and other problems.</Data></Cell>
    <Cell><Data ss:Type="String">Yuri Boykov*, University of Western Ontario; Hossam Isack, UWO; Carl Olsson, Lund University; Ismail Ben Ayed, Western University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">370</Data></Cell>
    <Cell><Data ss:Type="String">Learning a Discriminative Model for the Perception of Realism using Composite Images</Data></Cell>
    <Cell><Data ss:Type="String">What makes an image appear realistic? In this work, we are answering this question from a data-driven perspective by learning the perception of visual realism directly from large amounts of data. In particular, we train a Convolutional Neural Network (CNN) model that distinguishes natural photographs from automatically generated composite images. The model learns to predict visual realism of a scene without any human annotations pertaining to it. Our model outperforms previous works that rely on hand-crafted heuristics, for the task of classifying realistic vs. non-realistic photos. Furthermore, we apply our learned model to compute optimal parameters of a composition method, to maximize the visual realism score predicted by our CNN model. We demonstrate its advantage against existing methods via a human perception study.</Data></Cell>
    <Cell><Data ss:Type="String">Jun-yan Zhu*, UC Berkeley; Philipp Krahenbuhl, UC Berkeley; Eli Shechtman, &quot;Adobe, Seattle, USA&quot;; Alexei Efros, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">372</Data></Cell>
    <Cell><Data ss:Type="String">Learning Data-driven Reflectance Priors for Intrinsic Image Decomposition</Data></Cell>
    <Cell><Data ss:Type="String">The paper proposes a data-driven approach for performing intrinsic image decomposition, which is the process of inferring the confound factors of reflectance and shading from an input image.   This is posed as a two-stage learning problem.  First, we train a model to predict relative reflectance ordering between image patches ('same', 'brighter','darker'), producing a data-driven reflectance prior. Second, we show how to naturally integrate this learned prior into existing energy minimization frameworks for intrinsic image decomposition. We compare our method to the state-of-the-art approach of Bell \etal,  demonstrating the benefits of the simple relative reflectance prior, especially for scenes under challenging illumination conditions. </Data></Cell>
    <Cell><Data ss:Type="String">Tinghui Zhou*, UC Berkeley; Philipp Krahenbuhl, UC Berkeley; Alexei Efros, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">374</Data></Cell>
    <Cell><Data ss:Type="String">Multi-kernel Correlation Filter for Visual Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Correlation filter based trackers are ranked top in terms of performances. Nevertheless, they only employ a single kernel at a time. In this paper, we will derive a multi-kernel correlation filter (MKCF) based tracker which fully takes advantage of the invariance-discriminative power spectrums of various features to further improve the performance. Moreover, it may easily introduce location and representation errors to search several discrete scales for the correct one of the object bounding box. In this paper, we will propose a novel and efficient scale estimation method based on optimal search and approximate evaluation of features in order to improve tracking accuracy.</Data></Cell>
    <Cell><Data ss:Type="String">Ming Tang*, NLPR, Inst. of Auto, CAS; JIayi Feng, NLPR,IA,CAS</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">377</Data></Cell>
    <Cell><Data ss:Type="String">Joint Probabilistic Data Association Revisited</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we revisit the joint probabilistic data association (JPDA) technique and propose a novel solution based on recent developments in finding the m-best solutions to an integer linear program. The key advantage of this approach is that it makes JPDA computationally tractable in applications with high target and/or clutter density, such as spot tracking in fluorescence microscopy sequences and pedestrian tracking in surveillance footage. We also show that our JPDA algorithm embedded in a simple tracking framework is surprisingly competitive with state-of-the-art global tracking methods in these two applications, while needing considerably less processing time.</Data></Cell>
    <Cell><Data ss:Type="String">Seyed Hamid Rezatofighi*, The University of Adelaide; Anton Milan, University of Adelaide; Zhen  Zhang, Northwestern Polytechnical University, Xian, China; Qinfeng Shi, The University of Adelaide; Anthony Dick, University of Adelaide; Ian Reid, &quot;University of Adelaide, Australia&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">380</Data></Cell>
    <Cell><Data ss:Type="String">RGB-Guided Hyperspectral Image Upsampling</Data></Cell>
    <Cell><Data ss:Type="String">Hyperspectral imaging usually lack of spatial resolution due to limitations of hardware design of imaging sensors. On the contrary, latest imaging sensors capture a RGB image with resolution of multiple times larger than a hyperspectral image. In this paper, we present an algorithm to enhance and upsample the resolution of hyperspectral images. Our algorithm consists of two stages: spatial upsampling stage and spectrum substitution stage. The spatial upsampling stage is guided by a high resolution RGB image of  the same scene, and the spectrum substitution stage utilizes sparse coding to locally refine the upsampled hyperspectral image through dictionary substitution. Experiments show that our algorithm is highly effective and has outperformed state-of-the-art matrix factorization based approaches.</Data></Cell>
    <Cell><Data ss:Type="String">Hyeokhyen Kwon, KAIST; Yu-Wing Tai*, &quot;Korea Advanced Institute of Science and Technology, Korea&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">386</Data></Cell>
    <Cell><Data ss:Type="String">Mining And-Or Graphs for Graph Matching</Data></Cell>
    <Cell><Data ss:Type="String">In this study, we re-build the theory of graph mining on the technical basis of graph matching, and extend its application scope to visual tasks. Given a set of attributed relational graphs (ARGs), we propose to use a hierarchical And-Or Graph (AoG) to represent the common subgraph pattern, and develop a general method to mine the maximal-size AoG from unlabeled ARGs. This method can be regarded as a general solution to mining models from different kinds of unannotated visual data with various visual variations without exhaustive search of objects. The generality and broad applicability of our method are demonstrated in experiments.</Data></Cell>
    <Cell><Data ss:Type="String">Quanshi Zhang*, UCLA; Yingnian Wu, ; Song-Chun Zhu, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">387</Data></Cell>
    <Cell><Data ss:Type="String">Learning Deep Representation  with Large-scale Attributes</Data></Cell>
    <Cell><Data ss:Type="String">Learning strong feature representations from large scale supervision has achieved remarkable success in computer vision as the emergence of deep learning techniques. It is driven by big visual data with rich annotations. This paper contributes a large-scale object attribute database that contains  rich attribute annotations (over 300 attributes) for $\sim$180k samples and 499 object classes. Based on the ImageNet object detection dataset, it annotates the rotation, viewpoint, object part location, part occlusion, part existence, common attributes, and class-specific attributes. Then we use this dataset to train deep representations and extensively evaluate how these attributes are useful on the general object detection task. In order to make better use of the attribute annotations, a deep learning scheme is proposed by modeling the relationship of attributes and hierarchically clustering them into semantically meaningful mixture types. Experimental results show that the attributes are helpful in learning better features and improving the object detection accuracy by 2.6\% in mAP on the ILSVRC 2014 object detection dataset and 2.4\% in mAP on PASCAL VOC 2007 object detection dataset. Such improvement is well generalized across datasets. </Data></Cell>
    <Cell><Data ss:Type="String">Wanli Ouyang*, Chinese University of Hong Kong; Hongyang Li, The Chinese University of Hong Kong; Xingyu Zeng, The Chinese University of Hong Kong; Xiaogang Wang, The Chinese University of Hong Kong, Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">400</Data></Cell>
    <Cell><Data ss:Type="String">Efficient Video Segmentation using Parametric Graph Partitioning</Data></Cell>
    <Cell><Data ss:Type="String">Video segmentation is the task of grouping similar pixels in the spatio-temporal domain, and has become an important preprocessing step for subsequent video analysis. Most video segmentation and supervoxel methods output a hierarchy of segmentations, but whereas this provides useful multiscale information it also adds difficulty in selecting the most appropriate level for a task. In this work, we propose an efficient and  robust video segmentation framework based on parametric graph partitioning (PGP), a fast, almost parameter free,  graph partitioning method, that identifies and removes between-cluster edges to form node clusters. Apart from its computationally efficiency, PGP performs clustering of the spatio-temporal volume without requiring a prespecified  cluster number or bandwidth parameters, thus making video segmentation more practical to use in applications. The PGP framework also allows processing sub-volumes which further improves performance,  contrary to other streaming video segmentation methods where sub-volume processing reduces performance. We evaluate the PGP method using the  SegTrack v2 and Chen Xiph.org datasets, and show that it outperforms related state-of-the-art algorithms in 3D segmentation metrics and running time.</Data></Cell>
    <Cell><Data ss:Type="String">Chen-Ping Yu*, Stony Brook University; Hieu Le, ; Greg Zelinsky, ; Dimitris Samaras, SUNY Stonybrook</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">402</Data></Cell>
    <Cell><Data ss:Type="String">Tracking-by-Segmentation with Online Gradient Boosting Decision Tree</Data></Cell>
    <Cell><Data ss:Type="String">We propose an online tracking algorithm that adaptively models target appearances based on an online gradient boosting decision tree. Our algorithm is particularly useful for non-rigid and/or articulated objects since it handles various deformations of the target effectively by integrating a classifier operating on individual patches and provides segmentation masks of the target as final results. The posterior of the target state is propagated over time by particle filtering, where the likelihood is computed based mainly on patch-level confidence map associated with a latent target state corresponding to each sample. Once tracking is completed in each frame, our gradient boosting decision  tree is updated to adapt new data in a recursive manner. For effective evaluation of segmentation-based tracking algorithms, we construct a new ground-truth that contains pixel-level annotation of segmentation mask. We evaluate the performance of our tracking algorithm based on the measures for segmentation masks, where our algorithm illustrates superior accuracy compared to the state-of-the-art segmentation-based tracking methods.</Data></Cell>
    <Cell><Data ss:Type="String">Jeany Son, POSTECH; Ilchae Jung, POSTECH; Kayoung Park, POSTECH; Bohyung Han*, POSTECH, Pohang, Korea</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">404</Data></Cell>
    <Cell><Data ss:Type="String">Projection onto the Manifold of Elongated Structures for Accurate Extraction</Data></Cell>
    <Cell><Data ss:Type="String">Detection of elongated structures in 2D images and 3D image stacks is a critical  prerequisite  in many  applications and  Machine Learning-based  approaches have  recently been  shown to  deliver superior  performance.  However,  these methods  essentially classify individual locations and do not explicitly model the strong  relationship  that exists  between  neighboring ones.   As  a result,  isolated  erroneous responses, discontinuities, and topological  errors are present in the  resulting score maps.    We solve this  problem by projecting patches  of the score map  to their nearest  neighbors in  a set  of ground  truth training  patches.  Our  algorithm induces  global spatial consistency on the classifier  score map and returns results that  are  provably topologically  correct.   We apply  our  algorithm to  challenging  datasets  in four  different domains  and show  it outperforms  state-of-the-art  methods.</Data></Cell>
    <Cell><Data ss:Type="String">Amos Sironi*, EPFL; Vincent Lepetit, TU Graz; Pascal  Fua, &quot;EPFL, Switzerland&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">405</Data></Cell>
    <Cell><Data ss:Type="String">Naive Bayes Super-Resolution Forest</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a fast, high-performance method for super resolution with external learning. The first contribution leading to the excellent performance is a bimodal tree for clustering, which successfully exploits the antipodal invariance of the coarse-to-high-res mapping of natural image patches and provides scalability to finer partitions of the underlying coarse patch space. During training an ensemble of such bimodal trees is computed, providing different linearizations of the mapping. The second and main contribution is a fast inference algorithm, which selects the most suitable mapping function within the tree ensemble for each patch by adopting a Local Naive Bayes formulation. The experimental validation shows promising scalability properties that reflect the suitability of the proposed model, which may also be generalized to other tasks. The resulting method is beyond one order of magnitude faster and performs objectively and subjectively better than the current state of the art.</Data></Cell>
    <Cell><Data ss:Type="String">Jordi Salvador*, Technicolor; Eduardo Pérez-Pellitero, Technicolor</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">414</Data></Cell>
    <Cell><Data ss:Type="String">The One Triangle Three Parallelograms Sampling Strategy and Its Application in Shape Regression</Data></Cell>
    <Cell><Data ss:Type="String">The purpose of this paper is threefold. Firstly, the paper introduces the One Triangle Three Parallelograms (OTTP) sampling strategy, which can be viewed as a way to index pixels from a given shape and image. Secondly, a framework for cascaded shape regression, including the OTTP sampling, is presented. In short, this framework involves binary pixel tests for appearance features combined with shape features followed by a large linear system for each regression stage in the cascade. The proposed solution is found to produce state-of-the-art results on the task of facial landmark estimation. Thirdly, the dependence of accuracy of the landmark predictions and the placement of the mean shape within the detection box is discussed and a method to visualize it is presented.</Data></Cell>
    <Cell><Data ss:Type="String">Mikael Nilsson*, Lund University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">415</Data></Cell>
    <Cell><Data ss:Type="String">Exploring Causal Relationships in Visual Object Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Causal relationships can often be found in visual object  tracking between the motions of the camera and that of the  tracked object. This object motion may be an effect of the  camera motion, e.g. an unsteady handheld camera. But it  may also be the cause, e.g. the cameraman framing the ob-  ject. In this paper we explore these relationships, and pro-  vide statistical tools to detect and quantify them; these are  based on transfer entropy and stem from information the-  ory. The relationships are then exploited to make predic-  tions about the object location. The approach is shown to be  an excellent measure for describing such relationships. On  the VOT2013 dataset the prediction accuracy is increased  by 62 % over the best non-causal predictor. We show that  the location predictions are robust to camera shake and sud-  den motion, which is invaluable for any tracking algorithm  and demonstrate this by applying causal prediction to two  state-of-the-art trackers. Both of them benefit, Struck gain-  ing a 7 % accuracy and 22 % robustness increase on the  VTB1.1 benchmark, becoming the new state-of-the-art.  </Data></Cell>
    <Cell><Data ss:Type="String">Karel Lebeda*, University of Surrey; Simon Hadfield, University of Surrey; Richard Bowden, University of Surrey UK</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">418</Data></Cell>
    <Cell><Data ss:Type="String">Exploiting high level scene cues in stereo reconstruction</Data></Cell>
    <Cell><Data ss:Type="String">We present a novel approach to the 3D reconstruction which is inspired by the human visual system. This system unifies standard appearance matching and triangulation techniques with higher level reasoning and scene understanding, in order to resolve ambiguities between different interpretations of the scene. The types of reasoning integrated in the approach includes recognising common configurations of surface normals and semantic edges (e.g. convex, concave and occlusion boundaries). We also recognise the coplanar, collinear and symmetric structures which are especially common in man made environments.</Data></Cell>
    <Cell><Data ss:Type="String">Simon Hadfield*, University of Surrey; Richard Bowden, University of Surrey UK</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">419</Data></Cell>
    <Cell><Data ss:Type="String">Entropy Minimization for Convex Relaxation Approaches</Data></Cell>
    <Cell><Data ss:Type="String">Despite their enormous success in solving hard combinatorial problems, convex relaxation approaches often suffer from the fact that the computed solutions are far from binary and that subsequent heuristic binarization may substantially degrade the quality of computed solutions.  In this paper, we propose a novel relaxation technique which incorporates the entropy of the objective variable as a measure of relaxation tightness. We show both theoretically and experimentally that augmenting the objective function with an entropy term gives rise to more binary solutions and consequently solutions with a substantially tighter optimality gap.  We use difference of convex function (DC) programming as an efficient and provably convergent solver for the arising convex-concave minimization problem.  We evaluate this approach on three prominent non-convex computer vision challenges: multi-label inpainting, image segmentation and spatio-temporal multi-view reconstruction.  These experiments show that our approach consistently yields better solutions with respect to the original integral optimization problem</Data></Cell>
    <Cell><Data ss:Type="String">Mohamed Souiai*, University; Martin Oswald, postdoc now; Youngwook  Kee, Kaist; Junmo Kim, KAIST; Marc Pollefeys, ETH; Daniel Cremers, TUM</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">421</Data></Cell>
    <Cell><Data ss:Type="String">Boosting Object Proposals: From Pascal to COCO</Data></Cell>
    <Cell><Data ss:Type="String">Computer vision in general, and object proposals in particular, are nowadays strongly influenced by the databases on which researchers evaluate the performance of their algorithms. This paper studies the transition between the Pascal Visual Object Challenge dataset, which has been the benchmark of reference for the last years, and the updated, bigger, and more challenging Microsoft Common  Objects in Context. We first review and deeply analyze the new challenges, and opportunities, that this database poses. We then survey the current state of the art in object proposals and evaluate it focusing on how it generalizes to the new dataset. In sight of these results, we propose some new lines of research to take advantage of the new benchmark, and we explore an improvement on the state of the art that leads to a +5.8% improvement.</Data></Cell>
    <Cell><Data ss:Type="String">Jordi Pont-Tuset*, ETHZ; Luc Van Gool, KTH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">424</Data></Cell>
    <Cell><Data ss:Type="String">A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free; thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art.</Data></Cell>
    <Cell><Data ss:Type="String">Sotirios Chatzis*, Cyprus University of Technolog; Dimitrios Kosmopoulos, University of Texas at Arlington</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">430</Data></Cell>
    <Cell><Data ss:Type="String">Point triangulation through polyhedron collapse using the L-infinity norm</Data></Cell>
    <Cell><Data ss:Type="String">Multi-camera triangulation of feature points based on a minimisation of the overall L2 reprojection error can get stuck in suboptimal local minima or require slow global optimisation.  For this reason, researchers have proposed optimising the L-infinity norm of the L2 single view reprojection errors, which avoids the problem of local minima entirely.  In this paper we present a novel method for L-infinity triangulation that minimizes the L-infinity norm of the L-infinity reprojection errors: this apparently small difference leads to a much faster but equally accurate solution which is related to the MLE under the assumption of uniform noise.  The proposed method adopts a new optimisation strategy based on solving simple quadratic equations. This stands in contrast with the fastest existing methods, which solve a sequence of more complex auxiliary Linear Programming or Second Order Cone Problems.  The proposed algorithm performs well: for triangulation, it achieves the same accuracy as existing techniques while executing faster and being straightforward to implement.</Data></Cell>
    <Cell><Data ss:Type="String">Simon Donné*, iMinds - IPI - UGent; Bart Goossens, UGent/TELIN-IPI-iMinds; Wilfried Philips, iMinds-IPI-UGent</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">432</Data></Cell>
    <Cell><Data ss:Type="String">Continuous Pose Estimation with a Spatial Ensemble of Fisher Regressors</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we treat the problem of continuous pose estimation for object categories as a regression problem on the basis of only 2D training information. While regression is a natural framework for continuous problems, regression methods so far achieved inferior results with respect to 3D-based and 2D-based classification-and-refinement approaches. This may be attributed to their weakness to high intra-class variability as well as to noisy matching procedures and lack of geometrical constraints.    We propose to apply regression to Fisher-encoded vectors computed from large cells by learning an array of Fisher regressors. The array structure makes our algorithm flexible to variations in class appearance and geometry, while Fisher encoding permits to introduce geometrical information in the approach. We formulate our problem as a MAP inference problem, where we use the prior term to introduce discriminativeness into our generative approach, whereas the likelihood term is a function of the prediction error generated by the ensemble of Fisher regressors.    We test our algorithm on three publicly available datasets that envisage several difficulties, such as high intra-class variability, truncations, occlusions, and motion blur, obtaining state-of-the-art results.</Data></Cell>
    <Cell><Data ss:Type="String">Michele Fenzi*, Leibniz University Hannover; Laura Leal-Taixe, ETH Zurich; Joern Ostermann, ; Tinne Tuytelaars, KU Leuven</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">435</Data></Cell>
    <Cell><Data ss:Type="String">Secrets of GrabCut and Kernel K-means</Data></Cell>
    <Cell><Data ss:Type="String">The log-likelihood energy term in popular model-fitting segmentation methods, e.g. [e.g. Zhu-Yuille 1996, Chan-Vese 2001, GrabCut 2004], is presented as a generalized “probabilistic” K-means energy [Kearns-Mansour-Ng 1997] for color space clustering. This interpretations reveals some limitations, e.g. over-fitting. We propose an alternative approach to color clustering using kernel K-means with well-known properties such as non-linear separation and scalability to higher-dimensional feature spaces. Similarly to log-likelihoods, our kernel energy term for color space clustering can be combined with image grid regularization, e.g. boundary smoothness, and minimized using bound-optimization and max-flow algorithm. In contrast to fitting histograms or GMMs [Zhu-Yuille 1996, GrabCut 2004], at each step our algorithm uses Parzen densities for segment appearance. In contrast to implicit entropy minimization [Delong-Osokin-Isack-Boykov 2012], our approach is related to Gini and average association criteria. Using Nash theorem our analysis suggests principled adaptive kernel selection strategies to counter Breiman bias in these criteria. Our  general kernel-based approach improves sensitivity to color in synthetic and real images, and opens the door for many extensions and applications.</Data></Cell>
    <Cell><Data ss:Type="String">Meng Tang, UWO; Ismail Ben Ayed, Western University; Yuri Boykov*, University of Western Ontario</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">436</Data></Cell>
    <Cell><Data ss:Type="String">Deep Learning Strong Parts for Pedestrian Detection</Data></Cell>
    <Cell><Data ss:Type="String">Recent advances in pedestrian detection are attained by transferring Convolutional Neural Networks (CNN) features pre-trained on ImageNet to pedestrian images. Although these features are able to handle variations such as poses, viewpoints, and lightings, they fail when pedestrians with complex occlusion patterns are presented. Occlusion handling is the most critical problem in pedestrian detection. Unlike previous deep models that directly learn single full body detector, we propose DeepParts which consists of extensive part detectors. DeepParts has several appealing properties. First, DeepParts can be trained only on weakly labeled data, \ie, pedestrian bounding boxes without part annotation. Second, Deep parts is able to handle low IoU positive proposals that shifts away from ground truth. Third, each part detector in DeepParts is already a strong pedestrian detector that can detect pedestrian by observing only part of a proposal. Extensive experiments in Caltcech dataset demonstrate the effectivenss of DeepParts, which yields a new stae-of-the-art miss rate of 11.89%, outperforming the second best method by 10%.</Data></Cell>
    <Cell><Data ss:Type="String">Yonglong Tian*, CUHK</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">437</Data></Cell>
    <Cell><Data ss:Type="String">POP image fusion - derivative domain image fusion without reintegration</Data></Cell>
    <Cell><Data ss:Type="String">There are many applications where multiple images are fused to form a single summary greyscale or colour output, including computational photography (e.g. RGB-NIR), diffusion tensor imaging (medical), and remote sensing. Often, and intuitively, image fusion is carried out in the derivative domain. Here, a new composite fused derivative is found that best accounts for the detail across all images and then the resulting gradient field is reintegrated. However, the reintegration step generally hallucinates new detail (not appearing in any of the input  image bands) including halo and bending artifacts. In this paper we avoid these hallucinated details by avoiding the reintegration step.    Our work builds directly on the work of Socolinsky and Wolff who derive their equivalent gradient field from the per-pixel Di Zenzo structure tensor which is defined as the inner product of the image Jacobian. We show that the x- and y- derivatives of the projection of the original image onto the Principal characteristic vector of the Outer Product (POP) of the Jacobian generates the same equivalent gradient field. In so doing, we have derived a fused image that has the derivative structure we seek. Of course, this projection will be meaningful only where the Jacobian has non-zero derivatives, so we diffuse the projection directions using a bilateral filter before we calculate the fused image. The resulting POP fused image has maximal fused detail but avoids hallucinated artifacts. Experiments demonstrate our method delivers state of the art image fusion performance.  </Data></Cell>
    <Cell><Data ss:Type="String">Graham Finlayson, University of East Anglia; Alex Hayes*, University of East Anglia</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">458</Data></Cell>
    <Cell><Data ss:Type="String">Learning Social Relation Traits from Face Images</Data></Cell>
    <Cell><Data ss:Type="String">Social relation defines the association, eg, warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos.</Data></Cell>
    <Cell><Data ss:Type="String">Zhanpeng Zhang*, The Chinese University of HK; Ping Luo, The Chinese University of Hong Kong; Chen-Change Loy, the Chinese University of Hong Kong; Xiaoou Tang, The Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">459</Data></Cell>
    <Cell><Data ss:Type="String">Robust Heart Rate Measurement from Video Using Select Random Patches</Data></Cell>
    <Cell><Data ss:Type="String">The ability to remotely measure heart rate from videos without requiring any special setup is beneficial to many applications. In recent years, a number of papers on heart rate (HR) measurement from videos have been proposed. However, these methods typically require the human subject to be stationary and for the illumination to be controlled. For methods that do take into account motion and illumination changes, strong assumptions are still made about the environment (e.g. background can be used for illumination rectification). In this paper, we propose an HR measurement method that is robust to motion, illumination changes, and does not require use of an environment's background. We present conditions under which cardiac activity extraction from local regions of the face can be treated as a linear Blind Source Separation problem and propose a simple but robust algorithm for selecting good local regions. The independent HR estimates from multiple local regions are then combined in a majority voting scheme that robustly recovers the HR. We validate our algorithm on a large database of challenging videos.</Data></Cell>
    <Cell><Data ss:Type="String">Antony Lam*, Saitama University; Yoshinori Kuno, Saitama University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">466</Data></Cell>
    <Cell><Data ss:Type="String">Dense Optical Flow Prediction from a Static Image</Data></Cell>
    <Cell><Data ss:Type="String">Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present predictive convolutional neural networks (P-CNN). Given a static image, P-CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our P-CNN model leverages the data in tens of thousands  of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Since P-CNNs make no assumptions about the underlying scene they can predict future optical flow on a diverse set of scenarios. In terms of quantitative performance, P-CNN outperforms all previous approaches by large margins.    </Data></Cell>
    <Cell><Data ss:Type="String">Jacob Walker*, Carnegie Mellon University; Abhinav Gupta, &quot;Carnegie Mellon University, USA&quot;; Martial Hebert, Carnegie Mellon University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">468</Data></Cell>
    <Cell><Data ss:Type="String">Adaptive Spatial-Spectral Dictionary Learning for Hyperspectral Image Denoising</Data></Cell>
    <Cell><Data ss:Type="String">Hyperspectral imaging is beneficial in a diverse range of applications from diagnostic medicine, to agriculture, to surveillance to name a few. However, hyperspectral images often times suffer from degradation due to the limited light, which introduces noise into the imaging process. In this paper, we propose an effective model for hyperspectral image (HSI) denoising that considers underlying characteristics of HSIs: sparsity across the spatial-spectral domain, high correlation across spectra, and non-local self-similarity over space. We first exploit high correlation across spectra and non-local self-similarity over space in the noisy HSI to learn an adaptive spatial-spectral dictionary. Then, we employ the local and non-local sparsity of the HSI under the learned spatial-spectral dictionary to design an HSI denoising model, which can be effectively solved by an iterative numerical algorithm with parameters that are adaptively adjusted for different clusters and different noise levels. Experimental results on HSI denoising show that the proposed method can provide substantial improvements over the current state-of-the-art HSI denoising methods in terms of both objective metric and subjective visual quality.</Data></Cell>
    <Cell><Data ss:Type="String">Ying Fu*, The University of Tokyo; Antony Lam, Saitama University; Imari Sato, &quot;National Institute of Informatics, Japan&quot;; Yoichi Sato, Univ of Tokyo</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">480</Data></Cell>
    <Cell><Data ss:Type="String">Fully Connected Guided Image Filtering</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a linear time fully connected guided filter by introducing the minimum spanning tree (MST) to the guided filter (GF). Since the intensity based filtering kernel of GF is apt to overly smooth edges and the fixed-shape local box support region adopted by GF is not geometric-adaptive, our filter introduces an extra spatial term, the tree similarity, to the filtering kernel of GF and substitutes the box window with the implicit support region by establishing all-pairs-connections among pixels in the image and assigning the spatial-intensity-aware similarity to these connections. The adaptive implicit support region composed by the pixels with large kernel weights in the entire image domain has a big advantage over the predefined local box window in presenting the structure of an image for the reason that: 1, MST can efficiently present the structure of an image; 2, the kernel weight of our filter considers the tree distance defined on the MST. Due to above reasons, our filter can achieve better edge-preserving results. We demonstrate the strength of the proposed filter in several applications. Experimental results show that our method produces better results than state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Longquan Dai*,  Institute of Automation Chine; Mengke Yuan, CASIA; Feihu  Zhang, NLPR; Xiaopeng Zhang, Institute of Automation Chines</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">485</Data></Cell>
    <Cell><Data ss:Type="String">Adaptively Unified Semi-Supervised Dictionary Learning with Active Points</Data></Cell>
    <Cell><Data ss:Type="String">Semi-supervised dictionary learning aims to construct a dictionary by utilizing both labeled and unlabeled data. To enhance the discriminative capability of the learned dictionary, numerous discriminative terms have been proposed by evaluating either the prediction loss or the class separation criterion on the coding vectors of labeled data, but with rare consideration of the power of the coding vectors corresponding to unlabeled data. In this paper, we present a novel semi-supervised dictionary learning method, which uses the informative coding vectors of both labeled and unlabeled data, and adaptively emphasizes the high confidence coding vectors of unlabeled data to enhance the dictionary discriminative capability simultaneously. By doing so, we integrate the discrimination of dictionary, the induction of classifier to new testing data and the transduction of labels to unlabeled data into a unified framework. To solve the proposed problem, an effective iterative algorithm is designed. Experimental results on a series of benchmark databases show that our method outperforms other state-of-the-art dictionary learning methods in most cases.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaobo Wang*, Tianjin University; Xiaojie Guo, IIE, CAS; Yangjun Lu, Towson Univeristy; Shiming XIANG, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">492</Data></Cell>
    <Cell><Data ss:Type="String">Segment Graph Based Image Filtering: Fast Structure-Preserving Smoothing</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we design a new edge-aware structure, named segment graph, to represent the image and further develop a novel double weighted average image filter (SGF) based on the segment graph. In our SGF, we use the tree distance on the segment graph to define the internal weight function of the filtering kernel, which enables the filter to smooth out high-contrast details and textures while preserving major image structures very well. While for the external weight function, we introduce a user specified smoothing window to balance the smoothing effects from each node of the segment graph. Moreover, we also set a threshold to adjust the edge-preserving performance. These advantages make the SGF become more flexible in various applications and overcome the &quot;halo&quot; and &quot;leak&quot; problems appeared in most of the state-of-the-art approaches. Finally and importantly, we develop a linear algorithm for the implementation of our SGF, which has an O(N) time complexity for both gray-scale and high dimensional images, regardless of the kernel size and the intensity range. Typically, as one of the fastest edge-preserving filters, our CPU implementation achieves 0.15s per megapixel when performing filtering for 3-channel color images. The strength of the proposed filter is demonstrated by various applications, including stereo matching, optical flow, joint depth map upsampling, edge-preserving smoothing, edges detection, image abstraction and texture editing.</Data></Cell>
    <Cell><Data ss:Type="String">Feihu  Zhang*, NLPR; Longquan Dai,  Institute of Automation Chine; Shiming XIANG, ; Xiaopeng Zhang, Institute of Automation Chines</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">493</Data></Cell>
    <Cell><Data ss:Type="String">Video Matting via Sparse and Low-rank Representation</Data></Cell>
    <Cell><Data ss:Type="String">We introduce a novel method of video matting via sparse and low-rank representation. Previous matting methods \cite{Chenxw2012,Qifeng12} introduced a nonlocal prior to estimate the alpha matte and have achieved impressive results for some data. However, on one hand, searching inadequate or excessive samples may miss good samples or introduce noise; on the other hand, it is difficult to construct consistent nonlocal structures for pixels with similar feature, yielding spatially and temporally inconsistent video mattes.  In this paper, we proposed a novel video matting method to achieve spatially and temporally consistent matting result. Toward this end, a sparse and low-rank representation model is introduced to pursue consistent nonlocal structures for pixels with similar feature. The sparse representation is used to adaptively select best samples and accurately construct the nonlocal structures for all pixels, while the low-rank representation is used to globally ensure consistent nonlocal structures for pixels with similar features. The two representations are combined for generating consistent video mattes.  Experimental results show that our method achieved high quality results in a variety of challenging examples featuring illumination changes and feature ambiguity, topology changes, transparency variation, dis-occlusion, fast motion and motion blur.</Data></Cell>
    <Cell><Data ss:Type="String">Dongqing Zou*, Beihang University; Xiaowu Chen, Beihang University; Guangying Cao, ; Xiaogang Wang, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">501</Data></Cell>
    <Cell><Data ss:Type="String">A Versatile Learning-based 3D Temporal Tracker: Scalable, Robust, Online</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes a temporal tracking algorithm based on random forest that uses depth images to estimate and track the 3D pose of a rigid object in real-time. Compared to the state of the art aimed at the same goal, our algorithm holds important attributes such as high robustness against holes and occlusion, low computational cost of both learning and tracking stages, and low memory consumption. These are obtained (a) by a novel formulation of the learning strategy, based on a dense sampling of the camera viewpoints and learning independent trees from a single image for each camera view; as well as, (b) by an insightful occlusion handling strategy that enforces the forest to recognize the object's local and global structures. Due to these attributes, we report state-of-the-art tracking accuracy on benchmark datasets, and accomplish remarkable scalability with the number of targets, being able to simultaneously track the pose of over a hundred objects at 30~fps with an off-the-shelf CPU. In addition, the fast learning time enables us to extend our algorithm as a robust online tracker for model-free 3D objects under different viewpoints and appearance changes, as demonstrated by the experiments in terms of online 3D object tracking and 3D head pose estimation.</Data></Cell>
    <Cell><Data ss:Type="String">David Joseph Tan*, Technische Universität München; Federico Tombari, University of Bologna; Slobodan Ilic, Siemens AG; Nassir  Navab, Johns Hopkins University and TU Munich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">502</Data></Cell>
    <Cell><Data ss:Type="String">Dual-Feature Warping-based Motion Model Estimation</Data></Cell>
    <Cell><Data ss:Type="String">To break down the geometry assumptions of traditional motion models (e.g., homography, affine), warping-based motion model recently becomes very popular and is adopted in many latest applications (e.g., image stitching, video stabilization). With high degrees of freedom, the accuracy of model heavily relies on data-terms (keypoint correspondences). In some low-texture environments (e.g., indoor) where keypoint feature is insufficient or unreliable, the warping model is often erroneously estimated.    In this paper we propose a simple and effective approach by considering both keypoint and line segment correspondences as data-term. Line segment is a prominent feature in artificial environments and it can supply sufficient geometrical and structural information of scenes, which not only helps guild to a correct warp in low-texture condition, but also prevents the undesired distortion induced by warping. The combination aims to complement each other and benefit for a wider range of scenes. Our method is general and can be ported to many existing applications. Experiments demonstrate that using dual-feature yields more robust and accurate result especially for those low-texture images.</Data></Cell>
    <Cell><Data ss:Type="String">Shiwei Li*, HKUST; Lu Yuan, Microsoft Research Asia; Jian Sun, Microsoft Research China; Long Quan, &quot;The Hong Kong University of  Science and Technology, Hong Kong&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">507</Data></Cell>
    <Cell><Data ss:Type="String">Flowing ConvNets for Human Pose Estimation in Videos</Data></Cell>
    <Cell><Data ss:Type="String">The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow.    To this end we propose a new network architecture that: (i) regresses a confidence heatmap of joint position predictions; (ii) incorporates optical flow at a mid-layer to align heatmap predictions from neighbouring frames; and (iii) includes a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map.    We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, and one that regresses joint coordinates directly.    The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Tomas Pfister*, University of Oxford; James Charles, University of Leeds; Andrew Zisserman, Oxford</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">518</Data></Cell>
    <Cell><Data ss:Type="String">Robust Principal Component Analysis on Graphs</Data></Cell>
    <Cell><Data ss:Type="String">Principal Component Analysis (PCA) is the most widely used tool for linear dimensionality reduction and clustering. Still it is highly sensitive to outliers and does not scale well with respect to the number of data samples. Robust PCA solves the first issue with a sparse penalty term. The second issue can be handled with the matrix factorization model, which is however non-convex. Besides, PCA based clustering can also be enhanced by using a graph of data similarity. In this article, we introduce a new model called 'Robust PCA on Graphs' which incorporates spectral graph regularization into the Robust PCA framework. Our proposed model benefits from 1) the robustness of principal components to occlusions and missing values, 2) enhanced low-rank recovery, 3) improved clustering property due to the graph smoothness assumption on the low-rank matrix, and 4) convexity of the resulting optimization problem. Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with  corruptions clearly reveal that our model outperforms 10 other state-of-the-art models in its clustering and low-rank recovery tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Nauman Shahid*, EPFL; Vassilis Kalofolias, EPFL; Xavier Bresson, EPFL; Michael Bronstein, Università della Svizzera Italiana; Pierre Vandergheynst, EPFL</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">523</Data></Cell>
    <Cell><Data ss:Type="String">Projection Bank: From High-dimensional Data to Medium-length Binary Codes</Data></Cell>
    <Cell><Data ss:Type="String">Recently, very high-dimensional feature representations, e.g., Fisher Vector, have achieved excellent performance for visual recognition and retrieval. However, these lengthy representations always cause extremely heavy computational and storage costs and even become unfeasible in some large-scale applications. A few existing techniques can transfer very high-dimensional data into binary codes, but they still require the reduced code length to be relatively long to maintain acceptable accuracies. To target a better balance between computational efficiency and accuracies, in this paper, we propose a novel embedding method called Binary Projection Bank (BPB), which can effectively reduce the very high-dimensional representations to medium-dimensional binary codes without sacrificing accuracies. Instead of using conventional linear or bilinear projections, the proposed method learns a bank of small projections via the max-margin constraint to optimally preserve the intrinsic data similarity. We have systematically evaluated the proposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing competitive retrieval and recognition accuracies compared with state-of-the-art approaches, but leading to a significantly lower memory footprint and coding complexities.</Data></Cell>
    <Cell><Data ss:Type="String">Li Liu, Northumbria University; Mengyang Yu, Northumbria University; Ling Shao*, Northumbria University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">524</Data></Cell>
    <Cell><Data ss:Type="String">Robust Optimization for Deep Regression</Data></Cell>
    <Cell><Data ss:Type="String">Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. We demonstrate faster convergence and better generalization of our robust loss function for the task of human pose estimation, on four publicly available datasets. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in these datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Vasileios Belagiannis*, Technische Universität München; Christian Rupprecht, Technische Unitversität München; Gustavo Carneiro, University of Adelaide; Nassir  Navab, Johns Hopkins University and TU Munich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">526</Data></Cell>
    <Cell><Data ss:Type="String">Deeply Improved Sparse Coding for Image Super-Resolution</Data></Cell>
    <Cell><Data ss:Type="String">Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super-resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.</Data></Cell>
    <Cell><Data ss:Type="String">Zhaowen Wang*, Adobe Research; Ding Liu, UIUC; Jianchao Yang, Snapchat; Wei Han, UIUC; Thomas Huang, UIUC</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">532</Data></Cell>
    <Cell><Data ss:Type="String">Robust Model-based 3D Head Pose Estimation</Data></Cell>
    <Cell><Data ss:Type="String">We introduce a method for accurate three dimensional head pose estimation using a commodity depth camera. We perform pose estimation by registering a morphable face model to the measured depth data, using a combination of particle swarm optimization (PSO) and the iterative closest point (ICP) algorithm, which minimize a cost function that includes a 3D registration and a 2D overlap term. The pose is estimated on the fly without requiring an explicit initialization or training phase. Our method handles large pose angles and partial occlusions by dynamically adapting to the reliable visible parts of the face. It is robust and generalizes to different depth sensors without modification. On the Biwi Kinect dataset, we achieve best-in-class performance, with average angular errors of 2.1, 2.1 and 2.4 degrees for yaw, pitch, and roll, respectively, and an average translational error of 5.9 mm, while running at 6 fps on a graphics processing unit.  </Data></Cell>
    <Cell><Data ss:Type="String">Gregory Meyer, Nvidia Research; Shalini Gupta*, Nvidia Research; Iuri Frosio, Nvidia Research; Dikpal Reddy, Light.co; Jan Kautz, NVIDIA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">533</Data></Cell>
    <Cell><Data ss:Type="String">Learning to track for spatio-temporal action localization</Data></Cell>
    <Cell><Data ss:Type="String">We propose an effective approach for action localization, both in the spatial and temporal domains, in realistic videos.   The approach starts from detecting proposals at frame-level, and proceeds to scoring them using a combination  of static and motion state-of-the-art features extracted from CNNs. We then track a selection of proposals throughout the video,  using a tracking-by-detection approach that leverages a combination of instance-level and class-specific learned detectors.   The tracks are scored using a spatio-temporal motion histogram (STMH), a novel descriptor at the track level, in combination with the CNN features.   Finally, we perform temporal localization of the action using a sliding-window approach.     We present experimental results on the UCF-Sports and J-HMDB action localization datasets, where our approach outperforms   the state of the art with a margin of 15% and 7% respectively in mAP.   Furthermore, we present the first experimental results on the challenging UCF-101 localization dataset with 24 classes,   where we also obtain a promising performance. </Data></Cell>
    <Cell><Data ss:Type="String">Philippe Weinzaepfel*, Inria; Zaid Harchaoui, INRIA; Cordelia Schmid, &quot;INRIA Grenoble, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">541</Data></Cell>
    <Cell><Data ss:Type="String">Pose Induction for Novel Object Categories</Data></Cell>
    <Cell><Data ss:Type="String">We address the task of predicting pose for objects of unannotated object categories from a small seed set of annotated object classes. We present a generalized classifier that can reliably induce pose given a single instance of a novel category. In case of availability of a large collection of novel instances, our approach then jointly reasons over all instances to improve the initial estimates. We empirically validate the various components of our algorithm and quantitatively show that our method produces reliable pose estimates. We also show qualitative results on a diverse set of classes and further demonstrate the applicability of our system for learning shape models of novel object classes.</Data></Cell>
    <Cell><Data ss:Type="String">Shubham Tulsiani*, UC Berkeley; Joao Carreira, UC Berkeley; Jitendra Malik, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">544</Data></Cell>
    <Cell><Data ss:Type="String">Multi-class Multi-annotator Active Learning with Robust Gaussian Process for Visual Recognition</Data></Cell>
    <Cell><Data ss:Type="String">Active learning is an effective way to relieve the tedious work of manual annotation in many applications of visual recognition. However, less research attention has been focused on multi-class active learning. In this paper, we propose a novel Gaussian process classifier model with multiple annotators for multi-class visual recognition. Expectation propagation (EP) is adopted for efficient approximate Bayesian inference of our probabilistic model for classification. Based on the EP approximation inference, a generalized Expectation Maximization (GEM) algorithm is derived to estimate both the parameters for instances and the quality of each individual annotator. Also, we incorporate the idea of reinforcement learning to actively select both the informative samples and the high-quality annotators, which better explores the trade-off between exploitation and exploration. The experiments clearly demonstrate the efficacy of the proposed model.</Data></Cell>
    <Cell><Data ss:Type="String">Chengjiang Long*, Stevens Institute of Technolog; Gang Hua, Stevens Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">549</Data></Cell>
    <Cell><Data ss:Type="String">Robust Facial Landmark Detection under Significant Head Poses and Occlusion</Data></Cell>
    <Cell><Data ss:Type="String">There have been tremendous improvements for facial  landmark detection on general “in-the-wild” images. However,  it is still challenging to detect the facial landmarks on  images with severe occlusion and images with large head  poses (e.g. profile face). In fact, the existing algorithms  usually can only handle one of them. In this work, we propose  a unified robust cascade regression framework that can  handle both images with severe occlusion and images with  large head poses. Specifically, the method iteratively predicts  the landmark occlusions and the landmark locations.  For occlusion estimation, instead of directly predicting the  binary occlusion labels, we introduce a supervised regression  method that gradually updates the landmark visibility  probabilities in each iteration to achieve robustness. In addition,  we explicitly add occlusion pattern as a constraint to  improve the performance of occlusion prediction. For landmark  detection, we combine the landmark visibility probabilities,  the local appearances, and the local shapes to iteratively  update their positions. The experimental results  show that the proposed method is significantly better than  state-of-the-art works on images with severe occlusion and  images with large head poses. It is also comparable to other  methods on general “in-the-wild” images.</Data></Cell>
    <Cell><Data ss:Type="String">Yue Wu*, RPI; Qiang Ji, RPI</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">550</Data></Cell>
    <Cell><Data ss:Type="String">What Makes Kevin Spacey Look Like Kevin Spacey</Data></Cell>
    <Cell><Data ss:Type="String">We reconstruct a controllable model of a person from a large photo collection that captures his or her persona, i.e., physical appearance and behavior.  The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned.  Moreover, we show the ability to drive or puppeteer the captured person B using any other video of a different person A.  In this scenario, B acts out the role of person A, but retains his/her own personality and character.  Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem.  We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video.</Data></Cell>
    <Cell><Data ss:Type="String">Supasorn Suwajanakorn*, University of Washington; Ira Kemelmacher, University of Washington; Steve Seitz, Washington/Google</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">552</Data></Cell>
    <Cell><Data ss:Type="String">Convolutional Color Constancy</Data></Cell>
    <Cell><Data ss:Type="String">Color constancy is the problem of inferring the color of the light that illuminated a scene, usually so that the illumination color can be removed. Because this problem is underconstrained, it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast, in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a log-chrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images, our model is able to improve performance on standard benchmarks by nearly 40%.</Data></Cell>
    <Cell><Data ss:Type="String">Jonathan Barron*, Google</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">555</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Object Discovery and Tracking in Video Collections</Data></Cell>
    <Cell><Data ss:Type="String">This paper addresses the problem of automatically localizing dominant objects as spatio-temporal tubes in a noisy collection of videos with minimal or even no supervision. We formulate the problem as a combination of two complementary processes: discovery and tracking. The first one establishes correspondences between prominent regions across videos, and the second one associates similar object regions within the same video. Interestingly, our discovery and tracking procedure does much more than finding the spatio-temporal tubes associated with dominant objects: It also discovers the implicit neighborhood structure of frames associated with instances of the same class, which is a role normally left to supervisory information in the form of class labels in conventional image and video understanding methods. Indeed, as demonstrated by our experiments, our method can handle videos collections featuring multiple object classes, and substantially outperforms the state of the art in colocalization, even though it tackles a broader problem with much less supervision.</Data></Cell>
    <Cell><Data ss:Type="String">Suha Kwak*, INRIA; Minsu Cho, INRA; Jean Ponce, ENS; Cordelia Schmid, &quot;INRIA Grenoble, France&quot;; Ivan Laptev, INRIA Paris</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">560</Data></Cell>
    <Cell><Data ss:Type="String">Differential Recurrent Neural Networks for Action Recognition</Data></Cell>
    <Cell><Data ss:Type="String">The long short-term memory (LSTM) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any time-series or sequential data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics of various actions.  Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN).  We demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2D and 3D human action datasets. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states.  </Data></Cell>
    <Cell><Data ss:Type="String">Vivek Veeriah, ; Naifan Zhuang, University of Central Florida; Guo-Jun Qi*, University of Central Florida</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">562</Data></Cell>
    <Cell><Data ss:Type="String">Learning Ordinal Relationships for Mid-Level Vision</Data></Cell>
    <Cell><Data ss:Type="String">We propose a framework which infers mid-level visual properties of an image by learning about ordinal relationships. Instead of estimating metric quantities directly, the system proposes ordinal relationship estimates for pairs of points in the input image. These probabilistic ordinal measurements are then aggregated and globalized to create a full output map of continuous metric measurements.    Estimating order relationships between pairs of points has several advantages over metric estimation: it requires solving a simpler problem than metric regression; humans are better at making relative judgements so data collection is easier, and ordinal relationships are invariant to monotonic transformations of the data, thereby increasing the robustness of the system and providing qualitatively different information.      We demonstrate that this framework works well on two important mid-level vision tasks: intrinsic image decomposition and depth from a single RGB image. We train two separate systems with the same architecture on data from two different modalities. We provide an analysis of the resulting models, showing that they learn a number of simple rules to make ordinal decisions. We apply the same algorithm to two different mid-level vision problems:  depth estimation, with good results, and intrinsic image decomposition, with state-of-the-art results.</Data></Cell>
    <Cell><Data ss:Type="String">Daniel Zoran*, MIT; Phillip Isola, MIT; Dilip Krishnan, Google; William Freeman, &quot;MIT, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">565</Data></Cell>
    <Cell><Data ss:Type="String">Hierarchical Convolutional Features for Visual Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Visual object tracking is challenging as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we exploit features extracted from deep convolutional neural networks trained for object recognition problems to improve tracking accuracy and robustness. The outputs of the last convolutional layers encode the semantic information of targets and such representations are robust to significant appearance variations. However, their low spatial resolution is too coarse to precisely locate targets. In contrast, earlier layers provide more precise localization but are not invariant to appearance changes because they capture less semantics. We interpret the hierarchies of convolutional layers as a non-linear counterpart of a image pyramid and exploit multiple levels of abstraction as well as spatial scale for visual tracking. Specifically, we adaptively learn correlation filters on each convolutional layer to encode the target appearance. We search the maximum response of each layer in a coarse to fine manner to locate targets. Extensive experimental results on a large-scale benchmark dataset show that the proposed algorithm performs favorably against state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Chao Ma*, Shanghai Jiao Tong University; Jia-Bin Huang, ; Xiaokang Yang, Shanghai Jiao Tong University; Ming-Hsuan Yang, UC Merced, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">566</Data></Cell>
    <Cell><Data ss:Type="String">Top Rank Supervised Binary Coding for Visual Search</Data></Cell>
    <Cell><Data ss:Type="String">In recent years, binary coding techniques are becoming increasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been shown that supervised binary coding techniques which leverage supervised information can significantly enhance the coding quality, and hence greatly benefit visual search tasks. Typically, a modern binary coding method seeks to learn a group of coding functions that compress data samples into binary codes. However, few methods pursued the coding functions such that the precision at the top of a ranking list according to Hamming distances of the generated binary codes is optimized. In this paper, we propose a novel supervised binary coding approach, namely Top Rank Supervised Binary Coding (Top-RSBC), which explicitly focus on optimizing the precision of top positions in Hamming distance ranking list towards preserving the supervision information. The core idea is to train the disciplined coding functions by which the mistakes at the top of a Hamming-distance ranking list are penalized more than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective to a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online learning algorithm to optimize the surrogate objective efficiently . Thoroughly empirical studies based upon three benchmark image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over the state-of-the-arts.</Data></Cell>
    <Cell><Data ss:Type="String">Dongjin Song, UC San Diego; Rongrong Ji, Xiamen Unviersity; David Meyer, UC San Diego; John Smith, IBM T. J. Watson Research Center; Wei Liu*, IBM</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">567</Data></Cell>
    <Cell><Data ss:Type="String">Optimizing the Viewing Graph for Structure-from-Motion</Data></Cell>
    <Cell><Data ss:Type="String">The viewing graph represents a set of views that are related by pairwise relative geometries. In the context of Structure-from-Motion (SfM), the viewing graph is the input to the incremental or global estimation pipeline. Much effort has been put towards developing robust algorithms to overcome potentially inaccurate relative geometries in the viewing graph during SfM. In this paper, we take a fundamentally different approach to SfM and instead focus on improving the quality of the viewing graph before applying SfM. Our main contribution is a novel optimization that improves the quality of the relative geometries in the viewing graph by enforcing loop consistency constraints with the epipolar point transfer. We show that this optimization greatly improves the accuracy of relative poses in the viewing graph and removes the need for any of the complex filtering steps that are typically performed by global SfM methods. In addition, the optimized viewing graph can be used to efficiently calibrate cameras at scale. We combine our viewing graph optimization and focal length calibration into a global SfM pipeline that is dramatically simpler than existing approaches. To our knowledge, ours is the first global SfM pipeline capable of handling uncalibrated image sets.</Data></Cell>
    <Cell><Data ss:Type="String">Chris Sweeney*, UC Santa Barbara; Torsten Sattler, ETH Zurich; Matthew Turk, UC Santa Barbara USA; Tobias Hollerer, University of California Santa Barbara; Marc Pollefeys, ETH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">570</Data></Cell>
    <Cell><Data ss:Type="String">Distilling the Visual World: Foveated imaging for visual discovery</Data></Cell>
    <Cell><Data ss:Type="String">We propose a new method for turning an Internet-scale corpus of categorized  images into a small set of human-interpretable discriminative visual elements  using powerful tools based on deep learning.  A key challenge with deep learning methods is  generating human interpretable models. To address this, we propose a  new technique that generates bubble images&#45;&#45;-images where most  of the content has been obscured&#45;&#45;-and we use these bubble images as training  data for a  convolutional neural network. These bubble images result in models  that are much more amenable to identifying interpretable visual  elements. We apply our algorithm to a wide variety of datasets,  including two new Internet-scale datasets of people and places, and show  applications to visual mining and discovery. Our method is simple,  scalable, and produces visual elements that are highly representative  compared to prior work.</Data></Cell>
    <Cell><Data ss:Type="String">Kevin Matzen*, Cornell University; Noah Snavely, Cornell</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">571</Data></Cell>
    <Cell><Data ss:Type="String">Adaptive Hashing for Fast Similarity Search</Data></Cell>
    <Cell><Data ss:Type="String">With the staggering growth in image and video datasets, algorithms that provide fast similarity search and compact storage are crucial. Hashing methods that map the data into Hamming space have shown promise; however, many of these methods employ a batch-learning strategy in which the computational cost and memory requirements may become intractable and infeasible with larger and larger datasets. To overcome these challenges, we propose an online learning algorithm based on stochastic gradient descent in which the hash functions are updated iteratively with streaming data. In experiments with three image retrieval benchmarks, our online algorithm attains retrieval accuracy that is comparable to competing state-of-the-art batch-learning solutions, while our formulation is orders of magnitude faster and being online it is adaptable to the variations of the data. Moreover, our formulation yields improved retrieval performance over a recently reported online hashing technique, Online Kernel Hashing.</Data></Cell>
    <Cell><Data ss:Type="String">Fatih Cakir*, Boston University; Stan Sclaroff, Boston University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">574</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Domain Adaptation for  Zero-Shot Learning</Data></Cell>
    <Cell><Data ss:Type="String">Zero-shot learning (ZSL) can be considered as a special case of transfer learning where the source and target domains have different tasks/label spaces and the target domain is unlabelled, providing little guidance for the knowledge transfer. A ZSL method typically assumes that the two domains share a common semantic representation space, where a visual feature vector extracted from an image/video can be  projected/embedded using a projection function. Existing approaches learn the projection function from the source domain and apply it without adaptation to the target domain. They are thus based on naive knowledge transfer and the learned projections are prone to the domain shift problem. In this paper a novel ZSL method is proposed based on unsupervised domain adaptation. Specifically, we formulate a novel regularised sparse coding framework which uses the target domain class labels' projections in the semantic space  to regularise the learned target domain projection thus effectively overcoming the projection domain shift problem. Extensive experiments on three object and action recognition benchmark datasets show that the proposed ZSL method significantly outperforms the state-of-the-arts.</Data></Cell>
    <Cell><Data ss:Type="String">Elyor Kodirov*, QMUL; Tao Xiang, Queen Mary University of London; Zhenyong Fu, QMUL; Shaogang Gong, QMUL</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">576</Data></Cell>
    <Cell><Data ss:Type="String">Joint Object and Part Segmentation using Deep Learned Potentials</Data></Cell>
    <Cell><Data ss:Type="String">   Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-channel fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art by a large margin on both tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Peng Wang*, UCLA; Xiaohui Shen, Adobe Research; Zhe Lin, &quot;Adobe Systems, Inc.&quot;; Scott Cohen, Adobe; Brian Price, ; Alan Yuille, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">579</Data></Cell>
    <Cell><Data ss:Type="String">Know Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models</Data></Cell>
    <Cell><Data ss:Type="String">Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for  unsafe road conditions and alert drivers if they perform a dangerous maneuver.  However, many accidents are unavoidable because by the time drivers are alerted, it is already too late.  Anticipating maneuvers beforehand can alert drivers before they perform the  maneuver and also give ADAS more time to avoid or prepare for the danger.    In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the driving context from both inside and outside of the car. We propose an Autoregressive Input-Output HMM to model the contextual information alongwith the maneuvers. We evaluate our approach on a  diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate  maneuvers 3.5 seconds before they occur with over 80\% F1-score in real-time.</Data></Cell>
    <Cell><Data ss:Type="String">Ashesh Jain*, Cornell University; Hema Koppula, Cornell University; Bharad Raghavan, Stanford University; Shane Soh, Stanford University; Ashutosh Saxena, Cornell University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">580</Data></Cell>
    <Cell><Data ss:Type="String">Intrinsic Scene Decomposition from RGB-D images</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we address the problem of computing an intrinsic decomposition of the colors of a surface into an albedo and a shading term. The surface is reconstructed from a single or multiple RGB-D images of a static scene obtained from different views. We thereby extend and improve existing works in the area of intrinsic image decomposition.     In a variational framework, we formulate the problem as a minimization of an energy composed of two terms: a data term and a regularity term. The first term is related to the image formation process and expresses the relation between the albedo, the surface normals, and the incident illumination. We use an affine shading model, a combination of a Lambertian model, and an ambient lighting term. This model is relevant for Lambertian surfaces. When available,  multiple views can be used to handle view-dependent non-Lambertian reflections. The second term contains an efficient combination of l2 and l1-regularizers on the illumination vector field and albedo respectively. Unlike most previous approaches, especially Retinex-like techniques, these terms do not depend on the image gradient or texture, thus reducing the mixing shading/reflectance artifacts and leading to better results.     The obtained non-linear optimization problem is efficiently solved using a cyclic block coordinate descent algorithm.     Our method outperforms a range of state-of-the-art algorithms on a popular benchmark dataset. </Data></Cell>
    <Cell><Data ss:Type="String">Mohammed Hachama*, KAUST; Bernard Ghanem, KAUST; Peter Wonka, KAUST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">582</Data></Cell>
    <Cell><Data ss:Type="String">Photometric Stereo with Small Angular Variations</Data></Cell>
    <Cell><Data ss:Type="String">Some of the most successful photometric stereo setups require large angular variations in illumination directions, which results in acquisition rigs that have large spatial extent. For many applications, especially involving mobile devices, it is important that the device be spatially compact. This natually implies smaller angular variations in the illumination directions. This paper studies the effect of small angular variations in illumination directions to photometric stereo.  We explore both theoretical justification and practical issues in the design of a compact and portable photometric stereo device in which a camera is surrounded by a ring of point light sources. We first derive the relationship between the estimation error of surface normal and the baseline of the point light sources. Armed with this theoretical insight, we develop a small baseline photometric stereo prototype to experimentally examine the theory and its practicality.</Data></Cell>
    <Cell><Data ss:Type="String">Jian Wang*, Carnegie Mellon University; Yasuyuki Matsushita, Osaka University; Boxin Shi, Singapore University of Technology and Design; Aswin Sankaranarayanan, Carnegie Mellon University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">586</Data></Cell>
    <Cell><Data ss:Type="String">Occlusion-aware depth estimation using light-field cameras</Data></Cell>
    <Cell><Data ss:Type="String">Consumer-level and high-end light-field cameras are now widely available.  Recent work has demonstrated practical methods for passive depth estimation from light-field images.  However, most previous approaches do not explicitly model occlusions, and therefore cannot capture sharp transitions around object boundaries.  A common assumption is that a pixel exhibits photo-consistency when focused to its correct depth, i.e., all viewpoints converge to a single (Lambertian) point in the scene.  This assumption does not hold in the presence of occlusions, making most current approaches unreliable precisely where accurate depth information is most important - at depth discontinuities.      In this paper, we develop a depth estimation algorithm that treats occlusion explicitly; the method also enables identification of occlusion edges, which may be useful in other applications.  We show that, although pixels at occlusions do not preserve photo-consistency in general, they are still consistent in approximately half the viewpoints. Moreover, the line separating the two view regions (correct depth vs. occluder) has the same orientation as the occlusion edge has in the spatial domain. By treating these two regions separately, depth estimation can be improved. Occlusion predictions can also be computed and used for regularization. Experimental results show that our method outperforms current state-of-the-art light-field depth estimation algorithms, especially near occlusion boundaries.</Data></Cell>
    <Cell><Data ss:Type="String">Ting-Chun Wang*, UC Berkeley; Alexei Efros, UC Berkeley; Ravi Ramamoorthi, UCSD</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">587</Data></Cell>
    <Cell><Data ss:Type="String">3D Hand Pose Estimation Using Randomized Decision Forest with Segmentation Index Points</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a real-time 3D hand pose estimation algorithm using the randomized decision forest framework. Our algorithm takes a depth image as input and generates a set of skeletal joints as output. Previous decision forest-based methods often give labels to all pixels in a point cloud at a very early stage and vote for the joints' locations. By contrast, our algorithm only tracks a set of more flexible virtual landmark points, named segmentation index points (SIPs), before reaching the final decision at the leaf node. Roughly speaking, an SIP represents the centroid of a subset of skeletal joints, which are to be located at the leaves of the branch expanded from the SIP. Inspired by recent latent regression forest(LRF)-based hand pose estimation framework (Tang et al. 2014), we integrate SIP into the framework with several important improvements: First, we devise a new forest growing strategy, whose decision is made using a randomized feature guided by SIPs. Second, we  speed-up the training procedure since only SIPs, not the skeletal joints, are estimated in non-leaf nodes. Third, the experimental results on public benchmark datasets show clearly the advantage of the proposed algorithm over previous state-of-art methods, and our algorithm runs at 55.5 fps on a normal CPU without parallelism.</Data></Cell>
    <Cell><Data ss:Type="String">PEIYI LI*, Temple University; Haibin Ling, Temple University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">589</Data></Cell>
    <Cell><Data ss:Type="String">Oriented Light-Field Windows for Scene Flow</Data></Cell>
    <Cell><Data ss:Type="String">2D spatial pixel windows are used for comparing pixel values in computer vision applications such as correspondence for optical flow and 3D reconstruction, bilateral filtering, and image segmentation. However, pixel window comparisons can suffer from varying defocus blur and perspective at different depths, and can also lead to a loss of precision. In this paper, we leverage the recent use of light-field cameras to propose alternative {\em oriented light-field windows} that enable more robust and accurate pixel comparisons. For Lambertian surfaces focused to the correct depth, the 2D distribution of angular rays from a pixel remains consistent. We build on this idea to develop an oriented 4D light-field window that accounts for shearing (depth), translation (matching), and windowing. Our main application is to scene flow, a generalization of optical flow to the 3D vector field describing the motion of each point in the scene. We show significant benefits of oriented light-field windows over standard 2D spatial windows. We also demonstrate additional applications of oriented light-field windows for bilateral filtering and image segmentation.</Data></Cell>
    <Cell><Data ss:Type="String">Pratul Srinivasan*, UC Berkeley; Michael Tao, UC Berkeley; Ren Ng, UC Berkeley; Ravi Ramamoorthi, UCSD</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">599</Data></Cell>
    <Cell><Data ss:Type="String">Extended Depth of Field Catadioptric Imaging Using Focal Sweep</Data></Cell>
    <Cell><Data ss:Type="String">Catadioptric imaging systems use curved mirrors to capture wide fields of view. However, due to the curvature of the mirror, these systems tend to have very limited depth of field (DOF), with the point spread function (PSF) varying dramatically over the field of view and as a function of scene depth. In recent years, focal sweep has been used extensively to extend the DOF of conventional imaging systems. It has been shown that focal sweep produces an integrated point spread function (IPSF) that is nearly space-invariant and depth-invariant, enabling the recovery of an extended depth of field (EDOF) image by deconvolving the focal sweep image with a single IPSF. In this paper, we use focal sweep to extend the DOF of a catadioptric imaging system. We show that while the IPSF is spatially varying when a curved mirror is used, it remains quasi depth-invariant over the wide field of view of the imaging system. We have developed a focal sweep system where mirrors of different shapes can be used to capture wide field of view EDOF images. In particular, we show experimental results using spherical and paraboloidal mirrors.</Data></Cell>
    <Cell><Data ss:Type="String">Ryunosuke Yokoya*, Columbia University; Shree Nayar, &quot;Columbia University, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">602</Data></Cell>
    <Cell><Data ss:Type="String">Single Image 3D Without a Single 3D Image</Data></Cell>
    <Cell><Data ss:Type="String">Do we really need 3D labels in order to learn how to predict 3D? In this paper, we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label. Rather than use explicit supervision, we use the regularity of indoor scenes to learn the mapping in a completely unsupervised manner. We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D labels are unavailable, precluding supervised learning. Despite never seeing a 3D label, our method produces competitive results.</Data></Cell>
    <Cell><Data ss:Type="String">David Fouhey*, Carnegie Mellon University; Muhammad Wajahat Hussain, University of Zaraogoza; Abhinav Gupta, &quot;Carnegie Mellon University, USA&quot;; Martial Hebert, Carnegie Mellon University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">605</Data></Cell>
    <Cell><Data ss:Type="String">Wide-Area Image Geolocalization with Aerial Reference Imagery</Data></Cell>
    <Cell><Data ss:Type="String">We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales.  To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States.  Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.</Data></Cell>
    <Cell><Data ss:Type="String">Scott Workman*, University of Kentucky; Richard Souvenir, UNC Charlotte; Nathan Jacobs, University of Kentucky</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">615</Data></Cell>
    <Cell><Data ss:Type="String">Thin Structure Estimation with Curvature Regularization</Data></Cell>
    <Cell><Data ss:Type="String">Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples.</Data></Cell>
    <Cell><Data ss:Type="String">Dmitrii Marin, UWO; Yuri Boykov*, University of Western Ontario; Yuchen Zhong, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">620</Data></Cell>
    <Cell><Data ss:Type="String">Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation</Data></Cell>
    <Cell><Data ss:Type="String">This paper focus on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise.   The network structure consists of a convolutional neural network for image feature extraction, followed by two   sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function.  Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks.   We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods.  Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration.</Data></Cell>
    <Cell><Data ss:Type="String">Sijin Li*, City University of Hong; Weichen Zhang, City University of Hong Kong; Antoni Chan, &quot;City University of Hong Kong, Hong Kong&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">623</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Semantic Parsing of Video Collections</Data></Cell>
    <Cell><Data ss:Type="String">Human communication typically has an underlying structure. This is reflected in the fact that in many user generated videos, a starting point, ending, and certain objective steps between these two can be identified. In this paper, we propose a method for parsing a video into such semantic steps in an unsupervised way. The proposed method is capable of providing a semantic ``storyline'' of the video composed of its objective steps. We accomplish this utilizing both visual and language cues in a joint generative model. The proposed method can also provide a textual description for each of identified semantic steps and video segments. We evaluate this method on a large number of complex YouTube videos and show results of unprecedented quality for this new and impactful problem.</Data></Cell>
    <Cell><Data ss:Type="String">Ozan Sener*, Cornell University; Amir Zamir, Stanford University; Silvio Savarese, Stanford University, USA; Ashutosh Saxena, Cornell University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">625</Data></Cell>
    <Cell><Data ss:Type="String">Learning Spatiotemporal Features with 3D Convolutional Networks</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes a simple, yet effective approach for spatio-temporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatio-temporal feature learning than 2D ConvNets; 2) A homogeneous architecture with small $3 \times 3 \times 3$ kernel is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), significantly outperforms state-of-the-art methods on 4 different video analysis tasks and 6 different benchmarks with a simple linear SVM. In addition, it is compact: achieving $52.8\%$ accuracy on UCF101 dataset with only 10 dimensions and very efficient to compute: $91$ times faster than the current best hand-crafted features and approximately $2$ orders of magnitudes faster than deep-learning based video classification method using optical flows. Finally, it is conceptually simple and easy to use.</Data></Cell>
    <Cell><Data ss:Type="String">Du Tran*, Dartmouth College; Lubomir Bourdev, Facebook Inc.; Rob Fergus, New York University; Lorenzo Torresani, &quot;Dartmouth College, USA&quot;; Manohar Paluri, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">636</Data></Cell>
    <Cell><Data ss:Type="String">Cross-domain Image Retrieval with a Dual Attribute-aware Ranking Network</Data></Cell>
    <Cell><Data ss:Type="String">We address the problem of cross-domain image retrieval, considering the following practical application: given a user photo depicting a clothing image, our goal is to retrieve the same or attribute-similar clothing items from online shopping stores. This is a challenging problem due to the large  discrepancy between online shopping images, usually taken in ideal lighting/pose/background conditions, and user photos captured in uncontrolled conditions. To address this problem, we propose a Dual Attribute-aware Ranking Network (DARN) for retrieval feature learning. More specifically, DARN consists of two sub-networks, one for each domain, whose retrieval feature representations are driven by semantic attribute learning. We show that this attribute-guided learning is a key factor for retrieval accuracy improvement. In addition, to further align with the nature of the retrieval problem, we impose a triplet visual similarity constraint for learning to rank across the two sub-networks. Another contribution of our work is a large-scale dataset which makes the network learning feasible. We exploit customer review websites to crawl a large set of online shopping images and corresponding offline user photos with fine-grained clothing attributes, i.e., around 450,000 online shopping images and about 90,000 exact offline counterpart images of those online ones. All these images are collected from real-world consumer websites reflecting the diversity of the data modality, which makes this dataset unique and rare in the academic community. We extensively evaluate the retrieval performance of networks in different configurations. The top-20 retrieval accuracy is doubled when using the proposed DARN  other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268).</Data></Cell>
    <Cell><Data ss:Type="String">Junshi Huang*, National University of Singapo; Rogerio Feris, IBM Research Center, USA; Qiang Chen, ; Shuicheng Yan, National University of Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">637</Data></Cell>
    <Cell><Data ss:Type="String">Conditional Convolutional Neural Network for Modality-aware Face Recognition</Data></Cell>
    <Cell><Data ss:Type="String">Faces in the wild are usually captured with various poses, illuminations and occlusions, and thus inherently multimodally distributed in many tasks. We propose a conditional Convolutional Neural Network, named as c-CNN, to handle multimodal face recognition. Different from traditional CNN that adopts fixed convolution kernels, samples in c-CNN are processed with dynamically activated sets of kernels. In particular, convolution kernels within each layer are only sparsely activated when a sample is passed through the network. For a given sample, the activations of  convolution kernels in a certain layer are conditioned on its present intermediate representation and the activation status in the lower layers. The activated kernels across layers define the sample-specific adaptive routes that reveal the distribution of underlying modalities. Consequently, the proposed framework does not rely on any prior knowledge of modalities in contrast with most existing methods. To substantiate the generic framework, we introduce a special case of c-CNN via incorporating the conditional routing of the decision tree, which is evaluated with two problems of multimodality – multi-view face identification and occluded face verification. Extensive experiments demonstrate consistent improvements over the counterparts unaware of modalities.</Data></Cell>
    <Cell><Data ss:Type="String">Chao Xiong*, Imperial College London; Xiaowei Zhao, ; Danhang Tang, Imperial College London; Shuicheng Yan, National University of Singapore; Tae-Kyun Kim, Imperial College London</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">640</Data></Cell>
    <Cell><Data ss:Type="String">Dynamic Texture Recognition via Structured Tensor Dictionary Learning</Data></Cell>
    <Cell><Data ss:Type="String">Dynamic textures (DT) refer to video sequences with stationary properties, which exhibit repetitive patterns over space and time. This paper aims at investigating the sparse coding based approach to characterizing local DT patterns. Owing to the high dimensionality of DT sequences, existing dictionary learning algorithms are not suitable for our purpose due to their high computational costs as well as poor scalability. To overcome these obstacles, we proposed a structured tensor dictionary learning method for sparse coding, which learns a dictionary structured with orthogonality and separability. The proposed method is very fast and more scalable to high-dimensional data than the existing ones. In addition, based on the proposed dictionary learning method, a DT descriptor is developed, which has better adaptivity, discriminability and scalability than the state-of-the art approaches. These advantages are demonstrated by the experiments on multiple datasets.</Data></Cell>
    <Cell><Data ss:Type="String">YUHUI QUAN*, NUS; Yan Huang, ; Hui Ji, National  University of  Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">642</Data></Cell>
    <Cell><Data ss:Type="String">Activity Auto-Completion: Predicting Human Activities from Partial Videos</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose an activity auto-completion (AAC) model for human activity prediction by formulating activity prediction as a query auto-completion (QAC) problem in information retrieval. First, we extract discriminative patches in frames of videos. A video is represented based on these patches and divided into a collection of segments, each of which is regarded as a character typed in the search box. Then a partially observed video is considered as an activity prefix, consisting of one or more characters. Finally, the missing observation of an activity is predicted as the activity candidates provided by the auto-completion model. The candidates are matched against the activity prefix on-the-fly and ranked by a learning-to-rank algorithm. We validate our method on UT-Interaction Set #1 and Set #2 [19]. The experimental results show that the proposed activity auto-completion model achieves promising performance.</Data></Cell>
    <Cell><Data ss:Type="String">Zhen Xu*, UCAS; Laiyun Qing, UCAS; Jun Miao, ICT, CAS</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">644</Data></Cell>
    <Cell><Data ss:Type="String">Intrinsic Depth: Improving Depth Transfer with Intrinsic Images</Data></Cell>
    <Cell><Data ss:Type="String">We formulate the estimation of dense depth maps from video sequences as a problem of intrinsic image estimation. Our approach synergistically integrates the estimation of multiple intrinsic images including depth, albedo, shading, optical flow, and surface contours. We build upon a data-driven framework for depth estimation that uses label transfer from a database of RGB and depth pairs. We combine this with a method that extracts consistent albedo and shading from video. In contrast to raw RGB values, albedo and shading provide a richer, more physical, foundation for depth transfer. Additionally we train a new edge detector to predict surface boundaries from albedo, shading, and pixel values and use this to improve the estimation of depth boundaries. We also integrate sparse structure-from-motion with our method to improve the metric accuracy of the estimated depth maps. We evaluate our Intrinsic Depth method quantitatively by estimating depth from videos in the SUN3D and NYU RGB-D datasets. We find that combining the estimation of multiple intrinsic images improves depth estimation relative to the baseline method.</Data></Cell>
    <Cell><Data ss:Type="String">Naejin Kong*, MPI for Intelligent Systems; Michael Black, &quot;MPI, Tuebingen, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">647</Data></Cell>
    <Cell><Data ss:Type="String">HARF: Hierarchy-associated Rich Features for Salient Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">The state-of-the-art salient object detection models are able to perform well for relatively simple scenes, yet for more complex ones, they are still difficult to completely highlight salient objects from background, largely due to the lack of sufficiently robust features for saliency prediction. To address such an issue, this paper proposes a novel hierarchy-associated feature construction framework for salient object detection, which is based on integrating elementary features from multi-level regions in a hierarchy. Furthermore, multi-layered deep learning features are introduced and incorporated as elementary features into this framework through a compact integration scheme. This leads to a rich feature representation, which is able to represent the context of the whole object/background and is much more discriminative as well as robust for salient object detection. Extensive experiments on the most widely used and challenging benchmark datasets demonstrate that the proposed approach substantially outperforms the state-of-the-art on salient object detection.</Data></Cell>
    <Cell><Data ss:Type="String">Wenbin Zou*, Shenzhen University, China. Ecole des Ponts ParisTech, France.; Nikos Komodakis, &quot;Ecole des Ponts ParisTech, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">652</Data></Cell>
    <Cell><Data ss:Type="String">An Exploration of Parameter Redundancy in Deep Networks with  Circulant Projections</Data></Cell>
    <Cell><Data ss:Type="String">In this work, we explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with $d$ input nodes, and $d$ output nodes, this method improves the time complexity from $\mathcal{O}(d^2)$ to $\mathcal{O}(d\log{d})$ and space complexity from $\mathcal{O}(d^2)$ to $\mathcal{O}(d)$. The space savings are particularly important for modern deep convolutional neural network architectures, where fully-connected layers typically contain more than 90\% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections.</Data></Cell>
    <Cell><Data ss:Type="String">Yu Cheng, IBM Research; Felix Yu*, ; Rogerio Feris, IBM Research Center, USA; Sanjiv Kumar, Google; Shi-Fu Chang, Columbia</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">653</Data></Cell>
    <Cell><Data ss:Type="String">Low-Rank Tensor Constrained Multiview Subspace Clustering</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we explore the problem of multiview subspace clustering. We introduce a low-rank tensor constraint to explore the complementary information from multiple views and, accordingly, establish a novel method called Low-rank Tensor constrained Multiview Subspace Clustering (LT-MSC). Our method regards the subspace representation matrices of different views as a tensor, which captures dexterously the high order correlations underlying multiview data. Then the tensor is equipped with a low-rank constraint, which models elegantly the cross information  among different views, reduces effectually the redundancy of the learned subspace representations, and improves the accuracy of clustering as well. The inference process of the affinity matrix for clustering is formulated as a tensor nuclear norm minimization problem, constrained with an additional L2,1-norm regularizer and some linear equalities. The minimization problem is convex and thus can be solved efficiently by an Augmented Lagrangian Alternating Direction Minimization (AL-ADM) method. Extensive experimental results on four benchmark datasets show the effectiveness of our proposed LT-MSC method. </Data></Cell>
    <Cell><Data ss:Type="String">Changqing Zhang*, School of Computer Science and; Si Liu, ; Huazhu Fu, Nanyang Technological University; Guangcan Liu,  Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of I; Xiaochun Cao, Chinese Academy of Sciences</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">654</Data></Cell>
    <Cell><Data ss:Type="String">Additive Nearest Neighbor Feature Maps</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we present a concise framework to approximately construct feature maps for nonlinear additive kernels such as the Intersection, Hellinger's, and $\chi^2$ kernels. The core idea is to construct for each individual feature a set of anchor points and assign to every query the feature map of its nearest neighbor or the weighted combination of those of its $k$-nearest neighbors in the anchors. The resultant feature maps can be compactly stored by a group of nearest neighbor (binary) indication vectors along with the anchor feature maps. The approximation error of such an anchored feature mapping approach is analyzed. We evaluate the performance of our approach on large-scale nonlinear support vector machines~(SVMs) learning tasks in the context of visual object classification. Experimental results on several benchmark data sets show the superiority of our method over existing feature mapping methods in achieving reasonable trade-off between training time and testing accuracy.</Data></Cell>
    <Cell><Data ss:Type="String">WANG Zhenzhen*, NUIST; YUAN Xiao-Tong, ; LIU Qingshan, ; Shuicheng Yan, National University of Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">656</Data></Cell>
    <Cell><Data ss:Type="String">Learning to See by Moving</Data></Cell>
    <Cell><Data ss:Type="String">In computer vision, the current dominant paradigm for learning features relies on training neural networks for the task of object recognition using millions of hand labeled images. Is it possible to learn useful features for a diverse set of visual tasks using any other form of supervision ? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that with the same amount of training data, features learnt using egomotion as supervision compare favorably to features learnt using class-label as supervision on the visual tasks like scene recognition, object recognition, visual odometry and keypoint matching. </Data></Cell>
    <Cell><Data ss:Type="String">Pulkit Agrawal*, Berkeley; Joao Carreira, UC Berkeley; Jitendra Malik, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">659</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Cross-modal Synthesis of Subject-specific Scans</Data></Cell>
    <Cell><Data ss:Type="String">Recently, cross-modal synthesis of subject-specific scans has been receiving significant attention from the medical imaging community. Though various synthesis approaches have been introduced in the recent past, most of them are either tailored to a specific application or proposed for the supervised setting, i.e., they assume availability of training data from the same set of subjects in both source and target modalities. But, collecting multiple scans from each subject is undesirable. Hence, to address this issue, we propose a general unsupervised cross-modal image synthesis approach that works without any paired training data. Given a source modality image of a subject, we first generate multiple target modality candidate values for each voxel independently using cross-modal nearest neighbor search. Then, we select the best candidate values jointly for all the voxels by simultaneously maximizing a global mutual information cost function and a local spatial consistency cost function. Finally, we use coupled sparse representation for further refinement of synthesized images. Our experiments on generating T1-MRI brain scans from T2-MRI and vice versa demonstrate that the synthesis capability of the proposed unsupervised approach is comparable to that of the state-of-the-art supervised approach in the literature.</Data></Cell>
    <Cell><Data ss:Type="String">Raviteja Vemulapalli*, University of Maryland; Hien Nguyen, Siemens Corporate Technology; Kevin Zhou, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">665</Data></Cell>
    <Cell><Data ss:Type="String">Robust Non-rigid Motion Tracking and Surface Reconstruction Using L0 Regularization</Data></Cell>
    <Cell><Data ss:Type="String">We present a new motion tracking method to robustly reconstruct non-rigid geometries and motions from single view depth inputs captured by a consumer depth sensor. The idea comes from the observation of the existence of intrinsic articulated subspace in most of non-rigid motions. To take advantage of this characteristic, we propose a novel L0 based motion regularizer with an iterative optimization solver that can implicitly constrain local deformation only on joints with articulated motions, leading to reduced solution space and physical plausible deformations. The L0 strategy is integrated into the available non-rigid motion tracking pipeline, forming the proposed L0-L2 non-rigid motion tracking method that can adaptively stop the tracking error propagation. Extensive experiments over complex human body motions with occlusions, face and hand motions demonstrate that our approach substantially improves tracking robustness and surface reconstruction accuracy.</Data></Cell>
    <Cell><Data ss:Type="String">Kaiwen Guo, Tsinghua University; Feng Xu, Microsoft Research; Yangang Wang, Microsoft Research; Yebin Liu*, Tsinghua University; Qionghai Dai, Tsinghua University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">666</Data></Cell>
    <Cell><Data ss:Type="String">Person Re-identification with Correspondence Structure Learning</Data></Cell>
    <Cell><Data ss:Type="String">This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Experimental results on various datasets demonstrate the effectiveness of our approach.</Data></Cell>
    <Cell><Data ss:Type="String">Yang Shen, ; Weiyao Lin*, Shanghai Jiaotong University; Junchi Yan, Shanghai Jiao Tong University; Mingliang Xu, Zhengzhou University; Jianxin Wu, &quot;Nanjing University, China&quot;; Jingdong Wang, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">667</Data></Cell>
    <Cell><Data ss:Type="String">Understanding deep features with computer-generated imagery</Data></Cell>
    <Cell><Data ss:Type="String">We introduce an approach for analyzing the variation of features generated  by convolutional neural networks (CNNs) trained on large image datasets with respect to scene factors that occur in natural images.   Such factors may include object style, 3D viewpoint, color, and scene lighting configuration.  Our approach analyzes CNN feature responses with respect to different scene factors by controlling for them via rendering using a large database of 3D CAD models.  The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors.   We perform a linear decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components.   In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three trained CNNs: AlexNet~\cite{Krizhevsky12}, Places~\cite{Zhou14}, and Oxford VGG~\cite{Chatfield14}.   We observe important differences across the different networks and CNN layers with respect to  different scene factors and object categories.  Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images.  </Data></Cell>
    <Cell><Data ss:Type="String">Mathieu Aubry*, ENPC; Bryan Russell, Adobe Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">668</Data></Cell>
    <Cell><Data ss:Type="String">Deep Colorization</Data></Cell>
    <Cell><Data ss:Type="String">This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a \emph{high-quality fully-automatic} colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.</Data></Cell>
    <Cell><Data ss:Type="String">Zezhou Cheng*, Shanghai Jiao Tong University; Qingxiong Yang, City University of Hong Kong; Bin Sheng, Shanghai Jiao Tong University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">669</Data></Cell>
    <Cell><Data ss:Type="String">Attribute-Graph: A Graph based approach to Image Ranking</Data></Cell>
    <Cell><Data ss:Type="String">We propose a novel image representation, termed Attribute-Graph, to rank images by their semantic similarity to a given query image. An Attribute-Graph is an undirected fully connected graph, incorporating both local and global image characteristics. The graph nodes characterise the objects as well as the overall scene context using mid-level semantic attributes, while the edges capture the object topology. We demonstrate the effectiveness of Attribute-Graphs by applying them to the problem of image ranking. We benchmark the performance of our algorithm on the `rPascal' and `rImageNet' datasets, which we have created in order to evaluate the ranking performance on complex queries containing multiple objects. Our experimental evaluation shows that modelling images as Attribute-Graphs results in improved ranking performance over existing techniques.</Data></Cell>
    <Cell><Data ss:Type="String">Nikita Prabhu*, Indian Institute of Science; Venkatesh Babu Radhakrishnan, Indian Institute of Science</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">674</Data></Cell>
    <Cell><Data ss:Type="String">Constrained Convolutional Neural Networks for Weakly Supervised Semantic Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">We present an approach to learn a dense pixel-wise labeling from image-level tags.  Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network classifier.  We propose a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a Convolutional Neural Network.  Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization.  The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks.  Extensive experiments demonstrate the generality of our new learning framework.  The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation.  We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.</Data></Cell>
    <Cell><Data ss:Type="String">Deepak Pathak*, UC Berkeley; Philipp Krahenbuhl, UC Berkeley; Evan Shelhamer, UC Berkeley; Trevor Darrell, &quot;UC Berkeley, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">675</Data></Cell>
    <Cell><Data ss:Type="String">Personalized Age Progression with Aging Dictionary</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we aim to automatically render aging faces in a personalized way. Basically, a set of age-group specific dictionaries are learned, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups, and a linear combination of these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each subject may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular subject, yet much easier and more practical to get face pairs from neighboring age groups. Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts  in term of personalized aging progression, as well as the performance gain for cross-age face verification by synthesizing aging faces. </Data></Cell>
    <Cell><Data ss:Type="String">Xiangbo Shu*, Nanjing University of Science and Technology ; jinhui Tang, ; Hanjiang Lai, Na­tion­al Uni­ver­si­ty of Sin­ga­pore; Luoqi Liu, ; Shuicheng Yan, National University of Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">677</Data></Cell>
    <Cell><Data ss:Type="String">From Facial Part Responses to Face Detection: A Deep Learning Approach</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a novel deep convolutional network (DCN) that achieves the best performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method (which is also based on deep model) by a large margin of 2.59%. Importantly, we consider finding faces from a new perspective through scoring facial part responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical running speed.</Data></Cell>
    <Cell><Data ss:Type="String">Shuo Yang*, CUHK; Ping Luo, The Chinese University of Hong Kong; Chen-Change Loy, the Chinese University of Hong Kong; Xiaoou Tang, The Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">688</Data></Cell>
    <Cell><Data ss:Type="String">Accurate Camera Calibration Robust to Defocus using a Smartphone</Data></Cell>
    <Cell><Data ss:Type="String">We propose a novel camera calibration method for defocused images using a smartphone under the assumption that the defocus blur is modeled as a convolution of a sharp image with a Gaussian point spread function (PSF). In contrast to existing calibration approaches which require well-focused images, the proposed method achieves accurate camera calibration with severely defocused images. This robustness to defocus is due to the proposed set of unidirectional binary patterns, which simplifies 2D Gaussian deconvolution to a 1D Gaussian deconvolution problem with multiple observations. By capturing the set of patterns consecutively displayed on a smartphone, we formulate the feature extraction as a deconvolution problem to estimate features in sub-pixel accuracy and the blur kernel in each feature location. We also compensate the error in camera parameters due to refraction of the glass panel of the display device. We evaluate the performance of the proposed method on synthetic and real data. Even under severe defocus, our method shows accurate camera calibration result.</Data></Cell>
    <Cell><Data ss:Type="String">Hyowon Ha*, KAIST; Yunsu Bok, KAIST; Kyungdon Joo, KAIST; Jiyoung Jung, KAIST; In So Kweon, KAIST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">689</Data></Cell>
    <Cell><Data ss:Type="String">High Quality Structure from Small Motion for Rolling Shutter Cameras</Data></Cell>
    <Cell><Data ss:Type="String">We present a practical 3D reconstruction method to obtain high-quality dense depth map from narrow-baseline image sequences captured by commercial digital cameras, such as DSLRs or mobile phones. Depth estimation from small motion has gained interest as means of various photographic editing, but important limitations present themselves in the form of depth uncertainty due to narrow baseline and rolling shutter. To address these problems, we introduce a novel 3D reconstruction method from narrow-baseline image sequences that effectively handles the effects of a rolling shutter occurred from most of commercial digital cameras. Additionally, we present a depth propagation method to fill in the holes associated with the unknown pixels based on our novel geometric guidance model. Both qualitative and quantitative experimental results show that our new algorithm consistently generates better 3D depth maps compared to those by the state-of-the-art method.</Data></Cell>
    <Cell><Data ss:Type="String">Sunghoon Im*, KAIST; Hyowon Ha, KAIST; Gyeongmin Choe, KAIST; Hae-Gon Jeon, KAIST; Kyungdon Joo, KAIST; In So Kweon, KAIST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">691</Data></Cell>
    <Cell><Data ss:Type="String">Higher-order CRF Structural Segmentation of 3D Reconstructed Surfaces</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a structural segmentation algorithm to partition multi-view stereo reconstructed surfaces of large-scale urban environments into structural segments. Each segment corresponds to a structural component describable by a surface primitive of up to the second order. This segmentation is to be used in subsequent urban object modeling, vectorization, and recognition.    To overcome the high geometrical and topological noise levels in the 3D reconstructed urban surfaces, we formulate the structural segmentation as a higher-order Conditional Random Field (CRF) labeling problem. It not only incorporates classical lower-order 2D and 3D local cues, but also encodes contextual geometric regularities to disambiguate the noisy local cues.  A general higher-order CRF is difficult to solve. We develop a bottom-up progressive approach through a patch-based surface representation, which iteratively evolves from the initial mesh triangles to the final segmentation. Each iteration alternates between performing a prior discovery step, which finds the contextual regularities of the patch-based representation, and an inference step that leverages the regularities as higher-order priors to construct a more stable and regular segmentation.    The efficiency and robustness of the proposed method is extensively demonstrated on real reconstruction models, yielding significantly better performance than classical mesh segmentation methods.</Data></Cell>
    <Cell><Data ss:Type="String">Jingbo Liu*, HKUST; Jinglu Wang, HKUST; Tian Fang, HKUST; Chiew-Lan Tai, HKUST; Long Quan, &quot;The Hong Kong University of  Science and Technology, Hong Kong&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">695</Data></Cell>
    <Cell><Data ss:Type="String">Semi- and Weakly-supervised Human Pose Estimation</Data></Cell>
    <Cell><Data ss:Type="String">For human pose estimation, this paper proposes three semi- and weakly-supervised learning schemes.  All of these schemes initially learn conventional pose model(s) for pose estimation from a small number of training images with human pose annotations. For the first semi-supervised learning scheme, this pose model detects candidate poses in training images with NO human annotation. From these candidate poses, only true-positives are selected by a classifier using a pose feature representing the configuration of all body parts. The accuracies of these candidate pose estimation and true-positive pose selection from training images with no pose annotation are improved by action labels provided to these images in the second semi- and weakly-supervised learning scheme. In addition to the first and second schemes, the third semi- and weakly-supervised learning scheme selects more true-positive poses that are significantly different from any annotated poses. This pose selection is achieved by pose clustering using outlier detection with extended Dirichlet process mixtures. The proposed schemes are validated with large-scale datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Norimichi Ukita*, NAIST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">702</Data></Cell>
    <Cell><Data ss:Type="String">Image Matting with KL-Divergence Based Sparse Sampling</Data></Cell>
    <Cell><Data ss:Type="String">Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Levent Karacan, Hacettepe University; Aykut Erdem*, Hacettepe University; Erkut Erdem, Hacettepe University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">706</Data></Cell>
    <Cell><Data ss:Type="String">BodyPrint: Pose Invariant 3D Shape Matching of Human Bodies</Data></Cell>
    <Cell><Data ss:Type="String">3D human body shape matching has large potential on many real world applications, especially with the recent advances in the 3D range sensing technology. We address this problem by proposing a novel holistic human body shape descriptor called BodyPrint. To compute the bodyprint for a given body scan, we fit a deformable human body mesh and project the mesh parameters to a low-dimensional subspace which improves discriminability across different persons. Experiments are carried out on three real-world human body datasets to demonstrate that BodyPrint is robust to pose variation as well as missing information and sensor noise. It improves the matching accuracy significantly compared to conventional 3D shape matching techniques using local features. To facilitate practical applications where the shape database may grow over time, we also extend our learning framework to handle online updates.</Data></Cell>
    <Cell><Data ss:Type="String">Jiangping Wang*, University of Illinois; Kai Ma, Siemens Corporate Research, NJ, USA; Terrence Chen, Siemens Corporate Research; Vivek Singh, Siemens Corporate Research, NJ, USA; Thomas Huang, UIUC</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">708</Data></Cell>
    <Cell><Data ss:Type="String">Convolutional Channel Features For Pedestrian, Face and Edge Detection</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we revisit the multiple channel features approach proposed by Dollar et al., which has shown excellent performances in various computer vision tasks. Enlightened by the ConvNets, we introduce an extended version of multiple channel features called Convolutional Channel Features (CCF), which transfers low-level features from off-the-shelf ConvNet models to feed the boosting classifiers based on decision trees. With the combination of CNN features and decision trees, CCF benefits from the rich capacity, robustness and sparsity in feature representation, as well as more efficiency in computation and storage during inference and learning process. Similar to multiple channel features, CCF is capable to solve diverse vision problems in a sliding window manner, and the computation cost in CCF multi-scale feature pyramid construction can be further reduced with power law based approximation in nearby scales and patchwork for shared convolution. We investigate into a large design space of CCF and show with experiments that CCF achieves leading performances in pedestrian detection, face detection, edge detection and object proposal generation.</Data></Cell>
    <Cell><Data ss:Type="String">Bin Yang*, NLPR, CASIA; Junjie Yan, &quot;National Laboratory of Pattern Recognition, Chinese Academy of Sciences&quot;; Zhen Lei, casia; Stan Li, Chinese Academy of Sciences</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">722</Data></Cell>
    <Cell><Data ss:Type="String">Efficient PSD Constrained Asymmetric Metric Learning for Person Re-identification</Data></Cell>
    <Cell><Data ss:Type="String">Person re-identification is becoming a hot research topic due to its value in both machine learning research and video surveillance applications. For this challenging problem, distance metric learning is shown to be effective in matching person images. However, existing approaches either require a heavy computation due to the positive semidefinite (PSD) constraint, or ignore the PSD constraint and learn a free distance function that makes the learned metric potentially noisy. We argue that the PSD constraint provides a useful regularization to smooth the solution of the metric, and hence the learned metric is more robust than without the PSD constraint. Another problem with some existing metric learning algorithms is that the number of positive sample pairs is very limited, and the learning process is largely dominated by the large amount of negative sample pairs. To address the above issues, we derive a logistic metric learning approach with the PSD constraint and an asymmetric sample weighting strategy. Besides, we successfully apply the accelerated proximal gradient approach to find a global minimum solution of the proposed formulation, with a convergence rate of O(1/t^2) where t is the number of iterations. The proposed algorithm termed MLAPG is shown to be computationally efficient and able to perform low rank selection. We applied the proposed method for person re-identification, achieving state-of-the-art performance on three challenging databases (VIPeR, QMUL GRID, and CUHK Campus), compared to existing metric learning methods as well as published results.</Data></Cell>
    <Cell><Data ss:Type="String">Shengcai Liao*, &quot;Institute of Automation, Chinese Academy of Sciences&quot;; Stan Li, Chinese Academy of Sciences</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">723</Data></Cell>
    <Cell><Data ss:Type="String">Local Convolutional Features with Unsupervised Training for Image Retrieval</Data></Cell>
    <Cell><Data ss:Type="String">Deep architectures provide powerful image-level descriptors, one of the main reasons of their success lying in the quality of their local representations. Many tasks, such as stereo-matching or content-based image retrieval require strong local information for robust patch descriptors. Typical image retrieval pipelines consist of three steps: interest point detection, description and aggregation. Our work focuses on the second, and we show that convolutional  architectures can provide powerful local representations that are suited to this task. We review several deep architectures whose parameters are determined through unsupervised learning, for patch and image retrieval tasks. We show how an approach based on the recently introduced Convolutional Kernel Networks (CKNs) [26] can be adapted to patch description, yielding excellent results, as well as easy training. To draw conclusions on the link between patch and image retrieval tasks, we introduce a new dataset, called “RomePatches”, to benchmark both. We show that CKNs yield state-of-the-art performance on it, as well as on other standard datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Mattis Paulin*, Inria; Matthijs Douze, INRIA; Zaid Harchaoui, INRIA; Julien  Mairal, &quot;INRIA, Grenoble, France&quot;; Florent  Perronin, &quot;Xerox, France&quot;; Cordelia Schmid, &quot;INRIA Grenoble, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">736</Data></Cell>
    <Cell><Data ss:Type="String">Visual Madlibs: Fill in the blank Description Generation and Question Answering</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we introduce a new dataset consisting of 359,001 focused natural language descriptions for 10,738 images.  This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images.  Experiments using joint-embedding and deep learning methods show promising results on these tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Licheng Yu*, UNC; Eunbyung Park, ; Alex Berg, &quot;University of North Carolina, USA&quot;; Tamara Berg, University on North carolina</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">737</Data></Cell>
    <Cell><Data ss:Type="String">Separating Fluorescent and Reflective Components by Using a Single Hyperspectral Image</Data></Cell>
    <Cell><Data ss:Type="String">Hyperspectral imaging of scenes with fluorescent substances gives rise to the challenging problem of separating fluorescent and reflective components in the spectral domain. In contrast to existing methods, which require to capture two or more images under varying illuminations, we aim to achieve this separation task by using a single hyperspectral image. After identifying the critical hurdle in single-image component separation, we mathematically design the optimal illumination spectrum, which is shown to contain substantial high-frequency components in the frequency domain. This observation, in turn, leads us to recognize a key difference between reflectance and fluorescence in response to the frequency modulation effect of illumination, which fundamentally explains the feasibility of our method. On the practical side, we successfully find an off-the-shelf lamp as the light source, which is strong in irradiance intensity and cheap in cost. A fast linear separation algorithm is developed as well. Experiments using both synthetic data and real images have confirmed the validity of the selected illuminant and the accuracy of our separation algorithm.</Data></Cell>
    <Cell><Data ss:Type="String">Yinqiang Zheng*, &quot;National Institute of Informatics, Japan&quot;; Ying Fu, The University of Tokyo; Antony Lam, Saitama University; Imari Sato, &quot;National Institute of Informatics, Japan&quot;; Yoichi Sato, Univ of Tokyo</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">740</Data></Cell>
    <Cell><Data ss:Type="String">Interpolation on the manifold of $k$ component GMMs</Data></Cell>
    <Cell><Data ss:Type="String">Probability density functions (PDFs) are fundamental  &quot;objects&quot; in mathematics with numerous applications in  computer vision, machine learning and medical imaging.  The feasibility of basic operations such as computing the  distance between two PDFs and estimating a mean of a set  of PDFs is a direct function of the representation we choose  to work with. In this paper, we study the Gaussian mixture model   (GMM) representation of the PDFs motivated  by its numerous attractive features. (1) GMMs are arguably  more interpretable than, say, square root parameterizations  (2) the model complexity can be explicitly controlled by the  number of components and (3) they are already widely used  in many applications. The main contributions of   this paper are numerical algorithms to enable basic operations on  such objects that strictly respect their underlying geometry.   For instance, when operating with a set of k component GMMs,   a first order expectation is that the result of   simple operations like interpolation and averaging should   provide an object that is also a k component GMM.   The literature provides very little guidance on enforcing such   requirements systematically. It turns out that these tasks are   important internal modules for analysis and processing of a field  of ensemble average propagators (EAPs), common   in diffusion weighted magnetic resonance imaging. We provide  proof of principle experiments showing how the proposed  algorithms for interpolation can facilitate statistical   analysis of such data, essential to many neuroimaging studies.  Separately, we also derive interesting connections of our algorithm   with functional spaces of Gaussians, that may be of independent interest.</Data></Cell>
    <Cell><Data ss:Type="String">Hyunwoo Kim*, UW-Madison; Nagesh Adluru, University of Wisconsin - Madison; Vikas Singh, University of Wisconsin-Madison; Baba Vemuri, &quot;University of Florida, Gainesville, Fl., USA&quot;; Monami Banerjee, University of Florida; Sarah Turner, ; David Fuller, ; John Forder, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">750</Data></Cell>
    <Cell><Data ss:Type="String">Contextual Action Recognition with R*CNN</Data></Cell>
    <Cell><Data ss:Type="String">There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose which is characteristic for the action, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 89% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin.   </Data></Cell>
    <Cell><Data ss:Type="String">Georgia Gkioxari*, ; Ross Girshick, Microsoft Research; Jitendra Malik, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">751</Data></Cell>
    <Cell><Data ss:Type="String">Actions and Attributes from Wholes and Parts</Data></Cell>
    <Cell><Data ss:Type="String">We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system.</Data></Cell>
    <Cell><Data ss:Type="String">Georgia Gkioxari*, ; Ross Girshick, Microsoft Research; Jitendra Malik, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">758</Data></Cell>
    <Cell><Data ss:Type="String">Context-aware CNNs for person detection</Data></Cell>
    <Cell><Data ss:Type="String">Person detection is a key problem for many computer vision tasks. While face detection has reached maturity, detecting people under full variation of camera view-points, human poses, lighting conditions and occlusions is still a difficult challenge. In this work we focus on detecting human heads in natural scenes. Starting from the recent R-CNN object detector, we extend it in two ways. First, we leverage person-scene relations and propose a global CNN model trained to predict positions and scales of heads directly from the full image. Second, we propose a new structured-output loss enabling explicit modeling of pairwise relations among objects in a CNN framework. Our full combined model complements R-CNN with contextual cues derived from the scene. To train and test our model, we introduce a large dataset with 491,106 human heads annotated in 318,314 movie frames. We evaluate our method and demonstrate improvements of person detection compared to several recent baselines on three datasets. We also show improvements of the detection speed provided by our model.</Data></Cell>
    <Cell><Data ss:Type="String">Tuan-Hung Vu, INRIA; Anton  Osokin*, Inria; Ivan Laptev, INRIA Paris</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">760</Data></Cell>
    <Cell><Data ss:Type="String">The Middle Child Problem: Revisiting Parametric Min-cut for Fast Object Proposals</Data></Cell>
    <Cell><Data ss:Type="String">   The recent success of deep architectures for object detection has put object proposal generation methods under limelight. Object proposal methods aim to provide category-agnostic localization for all objects in the scene. One way to generate proposals is to perform parametric min-cuts over seed locations in an image. The main goal of this paper is to illustrate why typical parametric-cut models are ineffective for obtaining objects of medium size, which we name as the middle child problem. We address this problem in a principled way with geodesic distances, which makes energy minimization frameworks quite attractive for generating object proposal segmentations. Secondly, maintaining high recall from a small set of seed regions is non-trivial. We propose a superpixel merging algorithm which can reliably cover a large number of objects of all sizes with a small set of superpixels. Overall, our approach helps showcase the full potential of parametric min-cuts. On Pascal VOC our method proposes ${\small\sim}2,640$ segments with an average overlap of $0.81$, whereas the closest competing methods require ${\small\sim}5,140$ proposals to reach the same accuracy. We show detailed quanitative and qualitative comparisons against 4 state-of-the-art methods on Pascal VOC and Microsoft COCOsegmentation challenges.</Data></Cell>
    <Cell><Data ss:Type="String">Ahmad Humayun*, ; Fuxin Li, Georgia Tech; James Rehg, Georgia Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">767</Data></Cell>
    <Cell><Data ss:Type="String">Temporal Perception and Prediction in Ego-Centric Video</Data></Cell>
    <Cell><Data ss:Type="String">Given a video of an activity, can we predict what will happen next? In this paper we explore two simple tasks related to temporal prediction in egocentric videos of everyday activities. We provide both human experiments to understand how well people can perform on these tasks and computational models for prediction. Experiments indicate that humans and computers can do well on temporal prediction and that personalization to a particular individual or environment provides significantly increased performance. Developing methods for temporal prediction could have far reaching benefits for robots or intelligent agents to anticipate what a person will do, before they do it.</Data></Cell>
    <Cell><Data ss:Type="String">Yipin Zhou*, UNC; Tamara Berg, University on North carolina</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">771</Data></Cell>
    <Cell><Data ss:Type="String">DeepBox: Learning Objectness with Convolutional Networks</Data></Cell>
    <Cell><Data ss:Type="String">Existing object proposal approaches use primarily bottom-up cues to  rank proposals, while we believe that &quot;objectness&quot; is in  fact a high level construct. We argue for a data-driven, semantic approach  for ranking object proposals. Our framework, which we call DeepBox, uses  convolutional neural networks (CNNs) to rerank proposals from a bottom-up method.   We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster.  We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before. Finally, DeepBox has a significant impact on the performance of the object detector, reducing by 40% the total time taken for detection.</Data></Cell>
    <Cell><Data ss:Type="String">WEICHENG KUO*, UC Berkeley; Bharath Hariharan, UC Berkeley; Jitendra Malik, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">778</Data></Cell>
    <Cell><Data ss:Type="String">Frequency-based Environment Matting by Compressive Sensing</Data></Cell>
    <Cell><Data ss:Type="String">Extracting environment mattes using existing approaches often requires either thousands of captured images or a long processing time, or both. In this paper, we propose a novel approach to capturing and extracting the matte of a real scene effectively and efficiently. Grown out of the traditional frequency-based signal analysis, our approach can accurately locate contributing sources. By exploiting the recently developed compressive sensing theory, we simplify the data acquisition process of frequency-based environment matting. Incorporating phase information in a frequency signal into data acquisition further accelerates the matte extraction procedure. Compared with the state-of-the-art method, our approach achieves superior performance on both synthetic and real data, while consuming only a fraction of the processing time.</Data></Cell>
    <Cell><Data ss:Type="String">Yiming Qian*, University of Alberta; Minglun Gong, Memorial Univ; Yee Hong Yang, University of Alberta</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">780</Data></Cell>
    <Cell><Data ss:Type="String">Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a novel geometric model fitting method, called Mode-Seeking on Hypergraphs (MSH), to deal with multi-structure data even in the presence of severe outliers. The proposed method formulates geometric model fitting as a mode seeking problem on a hypergraph in which vertices represent model hypotheses and hyperedges denote data points. MSH intuitively detects model instances by a simple and effective mode seeking algorithm. In addition to the mode seeking algorithm, MSH includes a similarity measure between vertices on the hypergraph and a “weight-aware sampling” technique. The proposed method not only alleviates sensitivity to the data distribution, but also is scalable to large scale problems. Experimental results further demonstrate that the proposed method has significant superiority over the state-of-the-art fitting methods on both synthetic data and real images.</Data></Cell>
    <Cell><Data ss:Type="String">Hanzi Wang*, Xiamen University; Guobao Xiao, Xiamen University; Yan Yan, Xiamen University; David  Suter, &quot;University of Adelaide, Australia&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">783</Data></Cell>
    <Cell><Data ss:Type="String">Highly-Expressive Spaces of Well-Behaved Transformations: Keeping It Simple</Data></Cell>
    <Cell><Data ss:Type="String">We propose novel finite-dimensional spaces of well-  behaved Rn → Rn transformations, n ∈ {1, 2, 3}. These  transformations are obtained by fast and highly-accurate  integration of continuous piecewise-affine velocity fields.  The proposed method is simple yet highly expressive and ef-  fortlessly handles several optional constraints (e.g, volume-  preserving and boundary conditions). Importantly, the pro-  posed approach enables rapid likelihood evaluations that  in turn facilitate tractable inference over rich transfor-  mation spaces. Applications of the proposed method in-  clude, but are not limited to: unconstrained optimization  over monotonic functions; modeling cumulative distribu-  tion functions or histograms; time-warping; image regis-  tration; landmark-based warping; real-time diffeomorphic  image editing. Finally, we will release our code.</Data></Cell>
    <Cell><Data ss:Type="String">Oren Freifeld*, MIT; Soren Hauberg, Technical University of Denmark; Nematollah Batmanghelich, MIT; John Fisher III, MIT</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">788</Data></Cell>
    <Cell><Data ss:Type="String">The Layered Deformation Model for Shape Matching</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a shape matching algorithm that traces boundary of specific object with the aid of shape template. It partitions the given template into overlapped parts and uses them for choosing contour fragments from the cluttered environment. However, selected fragments are often messy and cannot form the boundary of specific object. Then the overlapped regions among parts are explored for refining these selections, bridging the gaps among chosen fragments, and finally linking them into the object boundary. The matching procedure is modeled with layered structure that utilizes a latent representation for local deformation.      As for inference, an approximate stage is designed for localizing potential candidates of  objects which are further refined with modified dual decomposition. The modified dual decomposition implements a sequential scheme to refine the local parts, enforcing their shared portions to be consistent among the chosen contour fragments. Results on ETHZ dataset demonstrate the benefits of the proposed algorithm. </Data></Cell>
    <Cell><Data ss:Type="String">Yuanqi Su*, Xi'an Jiaotong University; Yuehu Liu, Xi'an Jiaotong Univeristy; Bonan CUAN, Xi'an Jiaotong University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">793</Data></Cell>
    <Cell><Data ss:Type="String">Pose-Invariant 3D Face Alignment</Data></Cell>
    <Cell><Data ss:Type="String">Face alignment aims to estimate the locations of a set of landmarks for a given image. This problem has received much attention as evidenced by the recent advancement in both the methodology and performance. However, most of the existing works neither explicitly handle face images with arbitrary poses, nor perform large-scale experiments on non-frontal and profile face images. In order to address these limitations, this paper proposes a novel face alignment algorithm that estimates both 2D and 3D landmarks and their 2D visibilities for a face image with an arbitrary pose. By integrating a 3D deformable model, a cascaded coupled-regressor approach is designed to estimate both the camera projection matrix and the 3D landmarks. Furthermore, the 3D model also allows us to automatically estimate the 2D landmark visibilities via surface normals. We gather a substantially larger collection of all-pose face images to evaluate our algorithm and demonstrate superior performances than the state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Amin Jourabloo, Michigan State University; Xiaoming Liu*, Michigan State University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">800</Data></Cell>
    <Cell><Data ss:Type="String">Robust Image Segmentation Using Contour-guided Color Palettes</Data></Cell>
    <Cell><Data ss:Type="String">The contour-guided color palette (CCP) is proposed for robust image segmentation. It efficiently integrates contour and color cues of an image. To find representative colors of an image, we collect color samples from both sides of long contours and conduct the mean-shift (MS) algorithm in the sampled color space to define an image-dependent color palette. This color palette provides a preliminary segmentation in the spatial domain, which is further fine-tuned by post-processing techniques such as leakage avoidance, fake boundary removal, and small region mergence. Segmentation performances of CCP and MS are compared and analyzed. While CCP offers an acceptable standalone segmentation result, it can be further integrated into the framework of layered spectral segmentation to produce a more robust segmentation. The superior performance of CCP-based segmentation algorithm is demonstrated by experiments on the Berkeley Segmentation Dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Xiang Fu*, USC; Chien-Yi Wang, University of Southern California; Chen Chen, University of Southern California; Changhu Wang, Microsoft Research; C.-C. Jay Kuo, University of Southern California</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">805</Data></Cell>
    <Cell><Data ss:Type="String">PQTable: Fast Exact Asymmetric Distance Neighbor Search for Product Quantization using Hash Tables</Data></Cell>
    <Cell><Data ss:Type="String">We propose the product quantization table (PQTable), a product quantization-based hash table that is fast but requires neither parameter tuning nor training steps. The PQTable produces exactly the same results as a linear PQ search, but is 10^2 to 10^5 times faster when tested on the SIFT1B data. In addition, although state-of-the-art performance can be achieved by previous inverted-indexing-based approaches, such methods do require carefully and tediously designed parameter setting and much training, which our method does not. Therefore, PQTable offers a practical and useful solution for real-world problems.</Data></Cell>
    <Cell><Data ss:Type="String">Yusuke Matsui*, The University of Tokyo; Toshihiko Yamasaki, The University of Tokyo; Kiyoharu Aizawa, The University of Tokyo</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">817</Data></Cell>
    <Cell><Data ss:Type="String">Complementary Sets of Shutter Sequences for Motion Deblurring</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we present a novel multi-image motion deblurring method utilizing the coded exposure technique.  The key idea of our work is to capture video frames with a set of complementary fluttering patterns to preserve spatial frequency details.  We introduce an algorithm for generating a complementary set of binary sequences based on the modern communication theory and implement the coded exposure video system with an off-the-shelf machine vision camera.  The effectiveness of our method is demonstrated on various challenging examples with quantitative and qualitative comparisons to other computational image capturing methods used for image deblurring.</Data></Cell>
    <Cell><Data ss:Type="String">Hae-Gon Jeon*, KAIST; Joon-Young Lee, KAIST; Yudeog Han, ; Seon Joo Kim, Yonsei University; In So Kweon, KAIST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">826</Data></Cell>
    <Cell><Data ss:Type="String">From Emotions to Action Units with Hidden and Semi-Hidden-Task Learning</Data></Cell>
    <Cell><Data ss:Type="String">Limited annotated training data is a challenging problem in Action Unit recognition. In this paper, we investigate how the use of large databases labelled according to the 6 universal facial expressions can increase the generalization ability of Action Unit classifiers. For this purpose, we propose a novel learning framework: Hidden-Task Learning. HTL aims to learn a set of Hidden-Tasks (Action Units) for which samples are not available but, in contrast, training data is easier to obtain from a set of related Visible-Tasks (Facial Expressions). To that end, HTL is able to exploit prior knowledge about the relation between Hidden and Visible-Tasks. In our case, we base this prior knowledge on empirical psychological studies providing statistical correlations between Action Units and universal facial expressions. Additionally, we extend HTL to Semi-Hidden Task Learning (SHTL) assuming that Action Unit training samples are also provided. Performing exhaustive experiments over four different datasets, we show that HTL and SHTL improve the generalization ability of AU classifiers by training them with additional facial expression data. Additionally, we show that SHTL achieves competitive performance compared with state-of-the-art Transductive Learning approaches which face the problem of limited training data by using unlabelled test samples during training.</Data></Cell>
    <Cell><Data ss:Type="String">Adria Ruiz*, Universitat Pompeu Fabra; Joost van de Weijer, &quot;Universitat Autonoma de Barcelona, Spain&quot;; Xavier Binefa, Universitat Pompeu Fabra</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">832</Data></Cell>
    <Cell><Data ss:Type="String">Entropy-based Latent Structured Output Prediction</Data></Cell>
    <Cell><Data ss:Type="String">Recently several generalizations of the popular latent structured SVM framework have been proposed in the literature. Broadly speaking, the generalizations can be divided into two categories: (i) those that predict the output variables while either marginalizing the latent variables or estimating their most likely values; and (ii) those that predict the output variables by minimizing an entropy-based uncertainty measure over the latent space. In order to aid their application to computer vision, we study these generalizations with the aim of identifying their strengths and weaknesses. To this end, we propose a novel prediction criterion that includes as special cases all previous prediction criteria that have been used in the literature. Our framework's prediction criterion minimizes the Aczél and Daróczy entropy of the output. This in turn allows us to design a learning objective that provides a unified framework (UF) for latent structured prediction. Using a simple yet effective optimization algorithm, we provide empirical evidence that supports the use of the minimization of the latent space uncertainty as an accurate prediction criterion. We show that our optimization algorithm is as effective as the more complex approaches that have been previously been employed for latent structured prediction.</Data></Cell>
    <Cell><Data ss:Type="String">Diane Bouchacourt*, CentraleSupelec; Sebastian Nowozin, Microsoft Research Cambridge; M. Pawan Kumar, CentraleSupelec</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">834</Data></Cell>
    <Cell><Data ss:Type="String">Photogeometric Scene Flow for High-Detail Dynamic 3D Reconstruction</Data></Cell>
    <Cell><Data ss:Type="String">Photometric stereo (PS) is an established technique for high-detail reconstruction of 3D geometry and appearance. To correct for surface integration errors, PS is often combined with multiview stereo (MVS). With dynamic objects, PS reconstruction also faces the problem computing optical flow (OF) under rapid changes in illumination, for image alignment. Current PS methods predominantly compute optical flow and MVS as independent stages, each one with its own limitations and errors introduced by early regularization. In contrast, scene flow methods estimate geometry and motion, but lack the fine detail from PS. This paper proposes photogeometric scene flow (PGSF) for high-quality dynamic 3D reconstruction. PGSF performs PS, OF, and MVS simultaneously. It is based on two key observations: (i)~while image alignment improves PS, PS allows for surfaces to be relit to improve alignment; (ii)~PS provides surface gradients that render the smoothness term in MVS unnecessary, leading to truly data-driven, continuous depth estimates. This synergy is demonstrated in the quality of the resulting RGB appearance, 3D geometry, and 3D motion.</Data></Cell>
    <Cell><Data ss:Type="String">Paulo Gotardo*, Disney Research Pittsburgh; Tomas Simon, Carnegie Mellon University; Yaser Sheikh, Carnegie Mellon University; Iain Matthews, Disney Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">835</Data></Cell>
    <Cell><Data ss:Type="String">Facial Trait Judgment and Election Outcome Prediction: Social Dimensions of Face</Data></Cell>
    <Cell><Data ss:Type="String">The human face is a primary medium of human communication and a prominent source of information used to infer various attributes, such as gender, age, emotional state and identity. In this paper, we study how different categories of traits inferred from faces, specifically social dimensions, such as &quot;intelligence,&quot; &quot;honesty,&quot; and &quot;competence&quot; are used to predict the outcome of real-world social events that involve long-term commitments, such as political elections, job hires, and marriage engagements. To this end, we propose a hierarchical model for enduring traits inferred from faces, incorporating high-level perceptions and intermediate-level attributes.     We demonstrate that our model can successfully classify the outcomes of two important political events, only using campaign photographs of politicians' faces. First, it classifies the winners of a series of recent U.S. elections with an accuracy of 70.5% (Governors) and 63.6% (Senators). We also reveal that the different political offices require different types of preferred traits. Second, our model can categorize the political party of politicians, i.e., Democrats vs. Republicans, with an accuracy of 62.6% (male) and 60.1% (female). To the best of our knowledge, our paper is the first to use automated visual trait analysis to predict the outcomes of real-world social events. This approach is more scalable and objective than the prior behavioral studies, and opens for a range of new applications.  </Data></Cell>
    <Cell><Data ss:Type="String">Jungseock Joo*, UCLA; Francis Steen, UCLA; Song-Chun Zhu, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">836</Data></Cell>
    <Cell><Data ss:Type="String">Semantic Guidance of Visual Attention for  Localizing Objects in Scenes</Data></Cell>
    <Cell><Data ss:Type="String">We present a visual attention model guided by semantic knowledge of object categories. The model allows an agent to focus attention on candidate regions and to decide how to proceed for localizing objects correctly. This agent learns to deform a bounding box using simple transformation actions, with the goal of identifying the most specific location of target objects. An important characteristic of our approach is that objects are localized following top-down reasoning to search objects in scenes. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed visual attention model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.</Data></Cell>
    <Cell><Data ss:Type="String">Juan Caicedo*, Konrad Lorenz University; Svetlana Lazebnik, UIUC</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">838</Data></Cell>
    <Cell><Data ss:Type="String">Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions</Data></Cell>
    <Cell><Data ss:Type="String">Hands appear very often in egocentric video, and their appearance and pose give important cues about what people are doing and what they are paying attention to. But existing work in hand detection has made strong assumptions that work well in only simple scenarios, such as with limited interaction with other people or in lab settings. We develop methods to locate and distinguish between hands in egocentric video using strong appearance models with Convolutional Neural Networks, and introduce a simple candidate region generation approach that outperforms existing techniques at a fraction of the computational cost. We show how these high-quality bounding boxes can be used to create accurate pixelwise hand regions, and as an application, we investigate the extent to which hand segmentation alone can distinguish between different activities.  We evaluate these techniques on a new dataset of 48 first-person videos (along with pixel-level ground truth for over 15,000 hand instances) of people interacting in realistic environments.</Data></Cell>
    <Cell><Data ss:Type="String">Sven Bambach*, Indiana University; Stefan Lee, Indiana University; David Crandall, Indiana University; Chen Yu, Indiana University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">843</Data></Cell>
    <Cell><Data ss:Type="String">What makes an object memorable?</Data></Cell>
    <Cell><Data ss:Type="String">Recent studies on image memorability have shed light  on what distinguishes the memorability of different images  and the intrinsic and extrinsic properties that make those  images memorable. However, a clear understanding of the  memorability of specific objects inside an image remains  elusive. In this paper, we provide the first attempt to answer  the question: what exactly about an image is remembered?  We augment both the images and object segmentations  from the PASCAL-S dataset with ground truth memorability  scores and shed light on the various factors and  properties that make an object memorable (or forgettable)  to humans. We analyze various visual factors that may influence  object memorability (e.g. color, visual saliency, and  object categories). We also study the correlation between  object and image memorability and find that image memorability  is greatly affected by the memorability of its most  memorable object. Lastly, we explore the effectiveness of  deep learning and other computational approaches in predicting  object memorability in images. Our efforts offer a  deeper understanding of memorability in general thereby  opening up avenues for a wide variety of applications.</Data></Cell>
    <Cell><Data ss:Type="String">Rachit Dubey*, King Abdullah University of Science and Technology; Joshua  Peterson, University of California, Berk; Aditya Khosla, MIT; Ming-Hsuan Yang, UC Merced, USA; Bernard Ghanem, KAUST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">844</Data></Cell>
    <Cell><Data ss:Type="String">Fast Orthogonal Projection Based on Kronecker Product</Data></Cell>
    <Cell><Data ss:Type="String">We propose a family of structured matrices to speed up orthogonal projections for high-dimensional data, such as those often seen in computer vision and many other applications. In this, a structured matrix is formed by the Kronecker product of a series of smaller orthogonal matrices. This achieves $\mathcal{O}(d \log d)$ computational complexity and $\mathcal{O}(\log d)$ space complexity, a drastic improvement over the standard unstructured projections whose computation and space complexity are both $\mathcal{O}(d^2)$, where $d$ is the feature dimension. We demonstrate the significant advantages of the proposed approach in solving the approximate nearest neighbor (ANN) image search problem with both binary embedding and Cartesian k-means. Experiments show that the proposed approach can achieve similar or better accuracy as the existing state-of-the-art but with significantly less time and memory.</Data></Cell>
    <Cell><Data ss:Type="String">Xu Zhang*, Tsinghua University; Felix Yu, ; Sanjiv Kumar, Google; Shengjin  Wang, ; Shi-Fu Chang, Columbia; Ruiqi Guo, UIUC</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">850</Data></Cell>
    <Cell><Data ss:Type="String">Blur-aware Disparity Estimation from Defocus Stereo Images</Data></Cell>
    <Cell><Data ss:Type="String">Defocus blur usually causes performance degradation in establishing the visual correspondence between stereo images. We propose a blur-aware disparity estimation method that is robust to the mismatch of focus in stereo images. The relative blur resulting from the mismatch of focus between stereo images is approximated as the difference of the square diameters of the blur kernels. Based on the defocus and stereo model, we propose the relative blur versus disparity (RBD) model that characterizes the relative blur as a second-order polynomial function of disparity. Our method alternates between RBD model update and disparity update in each iteration. The RBD model in return refines the disparity estimation by updating the matching cost and aggregation weight to compensate the mismatch of focus. Experiments using both synthesized and real datasets demonstrate the effectiveness of our proposed algorithm.</Data></Cell>
    <Cell><Data ss:Type="String">Ching-Hui Chen*, University of Maryland; Hui Zhou, Nokia Technologies; Timo Ahonen, Nokia Technologies</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">851</Data></Cell>
    <Cell><Data ss:Type="String">Online Object Tracking with Proposal Selection</Data></Cell>
    <Cell><Data ss:Type="String">Tracking-by-detection approaches are some of the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e.g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel proposals estimated from the geometric transformations undergone by the object and building a rich candidate set for predicting the object location. The second one is devising a novel selection strategy using multiple cues, i.e., detection score and edgeness score computed from state-of-the-art object edges and motion boundaries. We extensively evaluate our approach on the visual object tracking 2014 challenge and online tracking benchmark datasets, and show the best performance.</Data></Cell>
    <Cell><Data ss:Type="String">Yang Hua*, Inria, France; Karteek Alahari, Inria; Cordelia Schmid, &quot;INRIA Grenoble, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">865</Data></Cell>
    <Cell><Data ss:Type="String">Global Structure-from-Motion by Similarity Averaging</Data></Cell>
    <Cell><Data ss:Type="String">Global structure-from-motion (SfM) methods solve all cameras simultaneously from all available relative motions. It has better potential in both reconstruction accuracy and computation efficiency than incremental methods. However, global SfM is challenging, mainly because of two reasons. Firstly, translation averaging is difficult, since an essential matrix only tells the direction of relative translation. Secondly, it is also hard to filter out bad essential matrices due to feature matching failures. We propose to compute a sparse depth image at each camera to solve both problems. Depth images help to upgrade an essential matrix to a similarity transformation, which can determine the scale of relative translation. Depth images also make the filtering of essential matrices simple and effective. In this way, translation averaging can be solved robustly in two convex $L_1$ optimization problems, which reach the global optimum rapidly. We demonstrate this method in various examples including sequential data, Internet data, and ambiguous data with repetitive scene structures.</Data></Cell>
    <Cell><Data ss:Type="String">Zhaopeng Cui*, Simon Fraser University; Ping Tan, Simon Fraser University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">866</Data></Cell>
    <Cell><Data ss:Type="String">Scene-Domain Active Part Models for Object Representation</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we are interested in enhancing the expressivity and robustness of part-based models for object representation, in the common scenario where the training data are based on 2D images. To this end, we propose scene-domain active part models (SDAPM), which reconstruct and characterize the 3D geometric statistics between object's parts in 3D scene-domain by using 2D training data in the image-domain alone. And on top of this, we explicitly model and handle occlusions in SDAPM. Together with the developed learning and inference algorithms, such a model provides rich object descriptions, including 2D object and parts localization, 3D landmark shape and camera viewpoint, which offers an effective representation to various image understanding tasks, such as object and parts detection, 3D landmark shape and viewpoint estimation from images. Experiments on the above tasks show that SDAPM outperforms previous part-based models, and thus demonstrates the potential of the proposed technique.</Data></Cell>
    <Cell><Data ss:Type="String">Zhou Ren*, UCLA; Chaohui Wang, Université Paris-Est Marne-la-Vallée; Alan Yuille, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">870</Data></Cell>
    <Cell><Data ss:Type="String">Fast and Accurate Head Pose Estimation via Random Projection Forests</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we consider the problem of estimating the gaze direction of a person from a low-resolution image.    Under these conditions, reliably extracting facial features is very difficult.    We propose a novel head pose estimation algorithm based on compressive sensing.    Head image patches are mapped to a large feature space using the proposed extensive, yet efficient filter bank.    The filter bank is designed to generate sparse responses of color and gradient information, which can be compressed using random projection, and classified by a random forest.    Extensive experiments on challenging datasets show that the proposed algorithm performs favorably against the state-of-the-art methods on head pose estimation in low-resolution images degraded by noise, occlusion, and blurring.</Data></Cell>
    <Cell><Data ss:Type="String">Donghoon Lee*, Seoul National University; Ming-Hsuan Yang, UC Merced, USA; Songhwai Oh, Seoul National University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">871</Data></Cell>
    <Cell><Data ss:Type="String">Joint Optimization of Segmentation and Color Clustering </Data></Cell>
    <Cell><Data ss:Type="String">Binary energy optimization is a popular approach for segmenting a color image into foreground/background regions. To model the appearance of the regions, color, a relatively high dimensional feature, should be handled effectively. A full color histogram is usually too sparse to be reliable. One approach is to explicitly reduce dimensionality by clustering or quantizing the color space. Another popular approach is to fit GMMs for soft implicit clustering of the color space. These approaches work well when  the foreground/background are sufficiently distinct. In cases of more subtle difference in appearance, both approaches may reduce or even eliminate foreground/background distinction. This happens because either color clustering is performed completely independently from the segmentation process, as a preprocessing step (in clustering), or independently for the foreground and independently for the background (in GMM). We propose to make clustering an integral part of segmentation, by including a new clustering term in the energy function. Our energy function with a clustering term favours clusterings that make foreground/background appearance more distinct. Thus our energy function jointly optimizes over color clustering, foreground/background models, and segmentation. Exact optimization is not feasible, therefore we develop an approximate algorithm. We show the advantage of including the color clustering term into the energy function on  camouflage images, as well as standard segmentation datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Ekaterina Lobacheva, Moscow State University; Yuri Boykov*, University of Western Ontario; Olga Veksler, &quot;University of Western Ontario, Canada&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">873</Data></Cell>
    <Cell><Data ss:Type="String">An Adaptive Data Representation for Robust Point-Set Registration and Merging</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a framework for rigid point-set registration and merging using a robust continuous data representation. Our point-set representation is constructed by training a one-class support vector machine with a Gaussian radial basis function kernel and subsequently approximating the output function with a Gaussian mixture model. We leverage the representation's sparse parametrisation and robustness to noise, outliers and occlusions in an efficient registration algorithm, SVMReg, that minimises the L2 distance between our support vector-parametrised Gaussian mixtures. In contrast, existing techniques, such as Iterative Closest Point and Gaussian mixture approaches, manifest a narrower region of convergence and are less robust to occlusions and missing data, as demonstrated in the evaluation on a range of 2D and 3D datasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously and equitably merges aligned mixture models, allowing the framework to be used for reconstruction and mapping.</Data></Cell>
    <Cell><Data ss:Type="String">Dylan Campbell*, Australian National University; Lars Petersson, NICTA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">882</Data></Cell>
    <Cell><Data ss:Type="String">Adaptive Exponential Smoothing for Online Filtering of Pixel Prediction Maps</Data></Cell>
    <Cell><Data ss:Type="String">We propose an efficient online video filtering method, called adaptive exponential filtering (AES) to refine pixel labeling maps. Assuming each pixel is associated with a discriminative prediction score, our proposed AES applies exponentially decreasing weights over time to smooth the labeling score of each pixel, similar to classic exponential smoothing. However, instead of fixing the spatial pixel location to perform temporal filtering, we trace each pixel in the past frames by finding the optimal path that can bring the maximum exponential smoothing score, thus performing adaptive and non-linear filtering. Thanks to the pixel tracing, AES can better address object movement and avoid over-smoothing. To enable real-time filtering, we propose a linear-complexity dynamic programming scheme that can trace all pixels simultaneously. We apply our proposed filtering method to improve both saliency detection maps and scene parsing maps. The comparison with average and exponential filtering, as well as state-of-the-art methods, validate that our proposed AES can effectively refine the pixel labeling map, without using the original video again.</Data></Cell>
    <Cell><Data ss:Type="String">Kang  Dang*, Nanyang Tech; JIONG YANG, Nanyang Tech; Junsong Yuan, Nanyang Technological University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">884</Data></Cell>
    <Cell><Data ss:Type="String">Similarity Gaussian Process Latent Variable Model for Multi-Modal Data Analysis</Data></Cell>
    <Cell><Data ss:Type="String">Data from real applications involve multiple modalities representing content with the same semantics and deliver rich information from complementary aspects. However, relations among heterogeneous modalities are simply treated as observation-to-fit by existing work, and the parameterized cross-modal mapping functions lack flexibility in directly adapting to the content divergence and semantic complicacy of multi-modal data. In this paper, we build our work based on Gaussian process latent variable model (GPLVM) to learn the non-linear non-parametric mapping functions and transform heterogeneous data into a shared latent space. We propose multi-modal Similarity Gaussian Process latent variable model (m-SimGP), which learns the nonlinear mapping functions between the intra-modal similarities and latent representation. We further propose multi-modal regularized similarity GPLVM (m-RSimGP) by encouraging similar/dissimilar points to be similar/dissimilar in the output space. The overall objective functions are solved by simple and scalable gradient decent techniques. The proposed models are robust to content divergence and high-dimensionality in multi-modal representation. They can be applied to various tasks to discover the non-linear correlations and obtain the comparable low-dimensional representation for heterogeneous modalities. On two widely used real-world datasets, we achieve at least 15% improvement over the existing approaches in cross-modal content retrieval task, and about 3% improvement over DS-SBP [9] in cross-modal classification task.</Data></Cell>
    <Cell><Data ss:Type="String">Guoli Song*, UCAS; Shuhui Wang, ; Qingming Huang, Graduate Univ of Chinese Academy of Sciences; Qi Tian, U Texas San Antonio</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">885</Data></Cell>
    <Cell><Data ss:Type="String">Simultaneous Binary Feature Learning and Encoding for Face Recognition</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a simultaneous binary feature learning and encoding(SBFLE) method for face recognition. Different from most existing face recognition methods which use hand-crafted descriptors such as local binary pattern (LBP) and Gabor features for face representation, we propose a unsupervised feature learning approach to learn face feature representation from raw data directly. Unlike existing binary face descriptors such as the LBP and discriminant face descriptor (DFD) methods which use a two-stage feature extraction approach, the proposed SBFLE method jointly learns binary codes for each local patch and the dictionary as the codebook for feature pooling so that discriminative information from raw pixels can be simultaneously learned with a one-stage procedure. Experimental results on four widely used face datasets including LFW, YouTube Face (YTF), FERET and PaSC clearly demonstrates the effectiveness of the proposed method.</Data></Cell>
    <Cell><Data ss:Type="String">Jiwen Lu*, ADSC, Singapore; Venice Erin Liong, ADSC, Singapore; Jie Zhou, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">886</Data></Cell>
    <Cell><Data ss:Type="String">A Unified Multiplicative Framework for Attribute Learning</Data></Cell>
    <Cell><Data ss:Type="String">Attributes are mid-level semantic properties of objects. Recent  research has shown that visual attributes can benefit many  traditional learning problems in computer vision community. However,  attribute learning is still a challenging problem as the attributes  may not always be predictable from input images and their annotation  is costly. In this paper, we propose a unified multiplicative  framework for attribute learning, which tackles the key problems.  Specifically, images and category information are jointly projected  into a shared feature space, where the latent factors are  disentangled and multiplied for attribute prediction. Moreover, our  method can leverage auxiliary data to enhance the predictive ability  of attribute classifiers, reducing the effort of instance-level  attribute annotation to some extent. Experimental results show that  our method achieves superior performance on both instance-level and  category-level attribute prediction. For zero-shot learning based on  attributes, our method significantly improves the state-of-the-art  performance on AwA dataset and achieves comparable performance on CUB  dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Kongming Liang*, ICT,CAS,CHINA; Hong Chang, Chinese Academy of Sciences; Shiguang Shan, Chinese Academy of Sciences; Xilin Chen, &quot;Institute of Computing Technology, Chinese Academy of Sciences&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">889</Data></Cell>
    <Cell><Data ss:Type="String">Learning Ensemble Latent Structured Models in Functional Space by Gradient Boosting</Data></Cell>
    <Cell><Data ss:Type="String">Many visual recognition tasks involve modeling variables which are structurally related. Hidden conditional random fields (HCRFs) are a powerful class of models for encoding structure in weakly supervised training examples. This paper presents HCRF-Boost, a novel and general framework for learning HCRFs in functional space. An algorithm is proposed to learn the potential functions of an HCRF as a combination of abstract nonlinear feature functions, expressed by regression models. Consequently, the resulting latent structured model is not restricted to traditional log-linear potential functions or any explicit parameterization. Further, functional optimization helps to avoid direct interactions with the possibly large parameter space of nonlinear models and improves efficiency. As a result, a complex and flexible ensemble method is achieved for structured prediction which can be efficiently used in a variety of applications. We validate the effectiveness of this method on tasks such as group activity recognition, human action recognition, and multi-instance learning of video events.</Data></Cell>
    <Cell><Data ss:Type="String">Hossein Hajimirsadeghi*, Simon Fraser University; Greg Mori, &quot;Simon Fraser University, Canada&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">891</Data></Cell>
    <Cell><Data ss:Type="String">Contractive Rectifier Networks for Nonlinear Maximum Margin Classification</Data></Cell>
    <Cell><Data ss:Type="String"> To find the optimal nonlinear separating boundary with maximum margin in the input data space, this paper proposes Contractive Rectifier Networks (CRNs), wherein the hidden-layer transformations are restricted to be contract mappings. The contractive constraints ensure that the achieved separating margin is the input space is larger than or equal to the separating margin in the output layer. The training of the proposed CRNs is formulated as a linear support vector machine (SVM) in the output layer, combined with two or more contractive hidden layers. Effective algorithms have been proposed to address the optimization challenges arising from contractive constraints. Experimental results on MNIST, CIFAR-10, CIFAR-100 and MIT-67 datasets demonstrate that the proposed contractive rectifier networks consistently outperform their conventional unconstrained rectifier networks counterparts.  </Data></Cell>
    <Cell><Data ss:Type="String">Senjian An*, University of Western Australia; Munawar Hayat , The Universitry  of Western Australian ; Salman Khan, UWA; Mohammed Bennamoun, UWA; Farid Boussaid, The Universitry  of Western Australian; Ferdous Sohel, UWA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">892</Data></Cell>
    <Cell><Data ss:Type="String">kNN Hashing with Factorized Neighborhood Representation</Data></Cell>
    <Cell><Data ss:Type="String">Hashing is very effective for many tasks in reducing the  processing time and in compressing massive databases. Although  lots of approaches have been developed to learn  data-dependent hash functions in recent years, how to learn  hash functions to yield good performance with acceptable  computational and memory cost is still a challenging problem.  Based on the observation that retrieval precision is  highly related to the kNN classification accuracy, this paper  proposes a novel kNN-based supervised hashing method,  which learns hash functions by directly maximizing the kNN  accuracy of the Hamming-embedded training data. To make  it scalable well to large problem, we propose a factorized  neighborhood representation to parsimoniously model the  neighborhood relationships inherent in training data. Considering  that real-world data are often linearly inseparable,  we further kernelize this basic model to improve its performance.  As a result, the proposed method is able to learn accurate  hashing functions with tolerable computation and storage  cost. Experiments on three benchmarks demonstrate  that our method outperforms the state-of-the-arts.</Data></Cell>
    <Cell><Data ss:Type="String">Kun Ding*, Institute of Automation; Chunlei Huo,  Institute of Automation; Bin Fan, &quot;NLPR, CASIA&quot;; Chunhong PAN, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">893</Data></Cell>
    <Cell><Data ss:Type="String">Deep Learning Face Attributes in the Wild</Data></Cell>
    <Cell><Data ss:Type="String">Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.  (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.  (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.  (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.</Data></Cell>
    <Cell><Data ss:Type="String">Ziwei Liu*, The Chinese University of HK; Ping Luo, The Chinese University of Hong Kong; Xiaogang Wang, The Chinese University of Hong Kong, Hong Kong; Xiaoou Tang, The Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">896</Data></Cell>
    <Cell><Data ss:Type="String">Augmenting Strong Supervision Using Web Data for Fine-grained Categorization</Data></Cell>
    <Cell><Data ss:Type="String">We propose a new method for fine-grained object recognition that employs part-level annotations and deep convolutional neural networks (CNNs) in a unified framework. Although both schemes have been widely used to boost recognition performance, due to the difficulty in acquiring detailed part annotations, strongly supervised fine-grained datasets are usually too small to keep pace with the rapid evolution of CNN architectures. In this paper, we solve this problem by exploiting inexhaustible web data. The proposed method improves classification accuracy in two ways: more discriminative CNN feature representations are generated using a training set augmented by collecting a large number of part patches from weakly supervised web images; and more robust object classifiers are learned using a multi-instance learning algorithm jointly on the strong and weak datasets. Despite its simplicity, the proposed method delivers a remarkable performance improvement on the CUB200-2011 dataset compared to baseline part-based R-CNN methods, and achieves the highest accuracy on this dataset even in the absence of test image annotations.</Data></Cell>
    <Cell><Data ss:Type="String">Zhe  Xu*, UTS;SJTU; Shaoli Huang,  University of Technology, Syd; Ya Zhang, ; Dacheng Tao, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">902</Data></Cell>
    <Cell><Data ss:Type="String">Multi-View Complementary Hash Tables for Nearest Neighbor Search</Data></Cell>
    <Cell><Data ss:Type="String">Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many applications (\eg, visual search, object detection, image matching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views. However, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build multiple complementary hash tables, much less to adaptively integrate information stemming from multiple views. In this paper we first present a novel multi-view complementary hash table method that learns complementarity hash tables from the data with multiple views. For single multi-view table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix, and learn discriminative hash functions in an efficient way. To build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars shared by mis-separated neighbors. Extensive experiments on three large-scale image datasets demonstrate that the proposed method significantly outperforms various naive solutions and state-of-the-art multi-table methods.</Data></Cell>
    <Cell><Data ss:Type="String">Xianglong Liu*, Beihang Uniersity; Lei Huang, Beihang University; Cheng Deng, Xidian University; Jiwen Lu, ADSC, Singapore; Bo Lang, Beihang University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">911</Data></Cell>
    <Cell><Data ss:Type="String">Understanding and Diagnosing Visual Tracking Systems</Data></Cell>
    <Cell><Data ss:Type="String">Several benchmark datasets for visual tracking research have been proposed in recent years.  Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable.  To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor.  We then conduct ablative experiments on each component to study how it affects the overall result.  Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community.  We find that the feature extractor plays the most important role in a tracker.  On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement.  Moreover, the motion model and model updater contain many details that could affect the result.  Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity.  Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers.  We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research.</Data></Cell>
    <Cell><Data ss:Type="String">Naiyan Wang*, HKUST; Jianping Shi, CUHK; Dit-Yan Yeung, Hong Kong University of Science and Technology; Jiaya Jia, Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">913</Data></Cell>
    <Cell><Data ss:Type="String">Describing Videos by Exploiting Temporal Structure</Data></Cell>
    <Cell><Data ss:Type="String">Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.  </Data></Cell>
    <Cell><Data ss:Type="String">Li Yao*, Université de Montréal; Atousa Torabi, ; Kyunghyun Cho, ; Nicolas Ballas, ; Christopher Pal, Ecole Polytechnique de Montreal; Hugo Larochelle, ; Aaron Courville, Univ. Toronto</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">918</Data></Cell>
    <Cell><Data ss:Type="String">BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called ``BoxSup'', produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.</Data></Cell>
    <Cell><Data ss:Type="String">Jifeng Dai*, Microsoft Research Asia; Kaiming He, Microsoft Research Asia; Jian Sun, Microsoft Research China</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">924</Data></Cell>
    <Cell><Data ss:Type="String">Fast Novel Visual Concept Learning from Sentence Descriptions of Images</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on [30] with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts.</Data></Cell>
    <Cell><Data ss:Type="String">Junhua Mao*, UCLA; Xu Wei, Baidu USA LLC; Yi Yang, Baidu IDL; Jiang Wang, Baidu USA LLC; Zhiheng Huang, Baidu USA LLC; Alan Yuille, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">935</Data></Cell>
    <Cell><Data ss:Type="String">Scalable Person Re-identification: A Benchmark</Data></Cell>
    <Cell><Data ss:Type="String">This paper contributes a new high quality dataset for person re-identification, named &quot;Market-1501&quot;. Generally, current datasets: 1) are limited in scale; 2) consist of hand-drawn bounding boxes, which are unavailable under realistic settings; 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated bounding boxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the Deformable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera.    As with a minor contribution, inspired by recent advances in large-scale image search, this paper proposes an unsupervised Bag-of-Words representation. We treat person re-identification as a special task of image search. In our experiment, we show that on VIPeR, CUHK03, and Market-1501 datasets, our method yields very competitive accuracy compared with state-of-the-arts. Moreover, our method is very efficient and achieves a speedup of 2-3 orders of magnitude compared with brute-force matching methods.</Data></Cell>
    <Cell><Data ss:Type="String">Liang Zheng*, Tsinghua University; Shengjin  Wang, ; Jingdong Wang, Microsoft Research; Liyue Shen, Tsinghua University; Jiahao  Bu, Tsinghua University; Qi Tian, U Texas San Antonio</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">937</Data></Cell>
    <Cell><Data ss:Type="String">Simultaneous Deep Transfer Across Domains and Tasks</Data></Cell>
    <Cell><Data ss:Type="String">Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.</Data></Cell>
    <Cell><Data ss:Type="String">Eric Tzeng*, UC Berkeley; Judy Hoffman, ; Trevor Darrell, &quot;UC Berkeley, USA&quot;; Kate Saenko, &quot;University of Massachusetts Lowel, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">946</Data></Cell>
    <Cell><Data ss:Type="String"> Multi-modal Sharable and Specific Feature Learning for RGB-D Object Recognition</Data></Cell>
    <Cell><Data ss:Type="String">Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Anran Wang, Nanyang Technological Univ.; Jianfei Cai*, ; Jiwen Lu, ADSC, Singapore; Tat-Jen Cham, Nanyang Technological University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">959</Data></Cell>
    <Cell><Data ss:Type="String">Low Dimensional Explicit Feature Maps</Data></Cell>
    <Cell><Data ss:Type="String">Approximating non-linear kernels by finite-dimensional feature maps is a popular approach for speeding up training and evaluation of support vector machines or to encode information into efficient match kernels.  We propose a novel method of data independent construction of low dimensional feature maps. The problem is cast as a linear program which jointly considers competing objectives: the quality of the approximation and the dimensionality of the feature map.    For both shift-invariant and homogeneous kernels the proposed method achieves a better approximations at the same dimensionality or comparable approximations at lower dimensionality of the feature map compared with state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Ondrej Chum*, Czech Technical University in Prague</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">962</Data></Cell>
    <Cell><Data ss:Type="String">Highly articulated human detection using a Markov Random Field model of poselets and variational method</Data></Cell>
    <Cell><Data ss:Type="String">Detecting highly articulated objects such as humans is a challenging problem. In this paper, we present a novel human detection method based on the use of poselet, a new notion of part, and Markov Random Field (MRF) for modelling the human body structure under the variation of human poses and viewpoints. The problem of human detection is then formulated as maximum a posteriori (MAP) estimation in the MRF model. Variational mean field method, a robust statistical model, is adopted to approximate the MAP inference. The proposed method was evaluated and compared with existing methods on different test sets including H3D and PASCAL VOC 2007-2009. Experimental results have favourbly shown the robustness of the proposed method in comparison to the state-of-the-art.</Data></Cell>
    <Cell><Data ss:Type="String">Thanh Nguyen*, SUTD; Khoi Tran, University of Science of HCM City, Vietnam; Sai-Kit Yeung, SUTD</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">980</Data></Cell>
    <Cell><Data ss:Type="String">Gipuma: Massively parallel multiview stereopsis</Data></Cell>
    <Cell><Data ss:Type="String">We present a new, massively parallel method for high quality multiview matching.   Our work builds on the Patchmatch idea: starting from randomly generated 3D planes  in scene space, the best-fitting planes are iteratively propagated   and refined to obtain a 3D depth and normal field per  view, such that a robust photo-consistency measure over all  images is maximized. Our main novelties are on the one  hand to formulate Patchmatch in scene space, which makes  it possible to aggregate image similarity across multiple  views and obtain more accurate depth maps. And on the  other hand a modified, diffusion-like propagation scheme  that can be massively parallelized and delivers dense multiview   correspondence over ten 1.9-Megapixel images in  6 seconds, on a consumer-grade GPU. Our method uses  a slanted support window and thus has no fronto-parallel  bias; it is completely local and parallel, such that computation   time scales linearly with image size, and inversely  proportional to the number of parallel threads. Furthermore,   it has low memory footprint (four values per pixel,  independent of the depth range). It therefore scales   exceptionally well and can handle multiple large images at high  depth resolution. Experiments on the DTU and Middlebury  multiview datasets as well as oblique aerial images show  that our method achieves very competitive results with high  accuracy and completeness, across a range of different scenarios.</Data></Cell>
    <Cell><Data ss:Type="String">Silvano Galliani*, ETH; Katrin Lasinger, ETH; Konrad Schindler, &quot;ETH Zurich, Switzerland&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">981</Data></Cell>
    <Cell><Data ss:Type="String">Convolutional networks for real-time 6-DOF camera relocalization</Data></Cell>
    <Cell><Data ss:Type="String">We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress a 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degree accuracy for large scale outdoor scenes and 0.5m and 5 degree accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show the convnet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.</Data></Cell>
    <Cell><Data ss:Type="String">Alex Kendall*, University of Cambridge; Matthew Grimes, University of Cambridge; Roberto Cipolla, University of Cambridge</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">982</Data></Cell>
    <Cell><Data ss:Type="String">Intrinsic decomposition of image sequences from local temporal variations</Data></Cell>
    <Cell><Data ss:Type="String">We present a method for intrinsic image decomposition,  which aims to decompose photographs into reflectance and  illumination layers. Our input is a sequence of images  with different illuminations acquired by a static camera, e.g. an indoor scene with a moving light source or an outdoor timelapse. We leverage the local color variations observed  over time to infer constraints on the reflectance and solve  the ill-posed image decomposition problem. In particular,  we derive an adaptive local energy from the observations  of each local neighborhood over time, and integrate distant  pairwise constraints to enforce coherent decomposition  across all surfaces with consistent illumination changes.  Our method is solely based on multiple observations of the  same scene and does not require user interaction, scene  geometry, or assumptions on the lighting distribution and  color. We demonstrate the validity of our approach and  compare to state-of-the-art methods on a number of synthetic  and real datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Pierre-Yves Laffont*, ETH Zurich; Jean-Charles Bazin, Disney Research Zurich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">985</Data></Cell>
    <Cell><Data ss:Type="String">Multi-Task Learning with Low Rank Attribute Embedding for Person Re-identification</Data></Cell>
    <Cell><Data ss:Type="String">We propose a novel Multi-Task Learning with Low Rank Attribute Embedding (MTL-LORAE) framework for person re-identification. Re-identifications from multiple cameras are regarded as related tasks to exploit shared information to improve re-identification accuracy. Both low level features and semantic/data-driven attributes are utilized. Since attributes are generally correlated, we introduce a low rank attribute embedding into the MTL formulation to embed original binary attributes to a continuous attribute space, where incorrect and incomplete attributes are rectified and recovered to better describe people. The learning objective function consists of a quadratic loss regarding class labels and an attribute embedding error, which is solved by an alternating optimization procedure. Experiments on three person re-identification datasets have demonstrated that MTL-LORAE outperforms existing approaches by a large margin and produces state-of-the-art results.</Data></Cell>
    <Cell><Data ss:Type="String">Chi Su*, Peking University, National Engineering Laboratory for Video Technology; Fan Yang, University of Maryland; Shiliang Zhang, ; Qi Tian, U Texas San Antonio; Larry Davis, &quot;University of Maryland, USA&quot;; Wen Gao, Peking University, National Engineering Laboratory for Video Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">987</Data></Cell>
    <Cell><Data ss:Type="String">Low-Rank Tensor Approximation with Laplacian Scale Mixture Modeling for Multiframe Image Denoising</Data></Cell>
    <Cell><Data ss:Type="String">Patch-based low-rank models have shown effective in exploiting spatial redundancy of natural images especially for the application of image denoising. However, two-dimensional low-rank model can not fully exploit the spatio-temporal correlation in larger data sets such as multispectral images and 3D MRIs. In this work, we propose a novel low-rank tensor approximation framework with Laplacian Scale Mixture (LSM) modeling for multi-frame image denoising. First, similar 3D patches are grouped to form a tensor of $d$-order and high-order Singular Value Decomposition (HOSVD) is applied to the grouped tensor. Then the task of multiframe image denoising is formulated as a Maximum A Posterior (MAP) estimation problem with the LSM prior for tensor coefficients. Both unknown sparse coefficients and hidden LSM parameters can be efficiently estimated by the method of alternating optimization. Specifically, we have derived closed-form solutions for both subproblems. Experimental results on spectral and dynamic MRI images show that the proposed algorithm can better preserve the sharpness of important image structures and outperform several existing state-of-the-art multiframe denoising methods (e.g., BM4D and tensor dictionary learning).</Data></Cell>
    <Cell><Data ss:Type="String">Weisheng Dong*, Xidian University; Guangyu Li, Xidian University; Guangming Shi, ; Xin LI, West Virginia University; Yi Ma, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1007</Data></Cell>
    <Cell><Data ss:Type="String">RIDE: Reversal Invariant Descriptor Enhancement</Data></Cell>
    <Cell><Data ss:Type="String">In many fine-grained object recognition datasets, image orientation (left/right) might vary from sample to sample. Since handcrafted descriptors such as SIFT are not reversal invariant, the stability of image representation based on them is consequently limited. A popular solution is to augment the datasets by adding a left-right reversed copy for each original image. This strategy improves recognition accuracy to some extent, but also brings the price of almost doubled time and memory consumptions.    In this paper, we present RIDE (Reversal Invariant Descriptor Enhancement) for fine-grained object recognition. RIDE is a generalized algorithm which cancels out the impact of image reversal by estimating the orientation of local descriptors, and guarantees to produce the identical representation for an image and its left-right reversed copy. Experimental results reveal the consistent accuracy gain of RIDE with various types of descriptors. We also provide insightful discussions on the working mechanism of RIDE and its generalization to other applications.  </Data></Cell>
    <Cell><Data ss:Type="String">Lingxi Xie*, Tsinghua University; Jingdong Wang, Microsoft Research; Weiyao Lin, Shanghai Jiaotong University; Bo Zhang, Tsinghua University; Qi Tian, U Texas San Antonio</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1011</Data></Cell>
    <Cell><Data ss:Type="String">P-CNN: Pose-based CNN Features for Action Recognition</Data></Cell>
    <Cell><Data ss:Type="String">This work targets human action recognition in video.  While recent methods typically represent actions by statistics of local video features, here we argue for the importance of structural information derived from human poses.  To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition.  The descriptor aggregates motion and appearance information along tracks of human body parts.  We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses.   We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art.</Data></Cell>
    <Cell><Data ss:Type="String">Guilhem Chéron*, Inria; Ivan Laptev, INRIA Paris; Cordelia Schmid, &quot;INRIA Grenoble, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1015</Data></Cell>
    <Cell><Data ss:Type="String">Fully Connected Object Proposals for Video Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">We present a novel approach to video segmentation using multiple object proposals.  The problem is formulated as a minimization of a novel energy function defined over a fully connected graph of object proposals. Our model combines appearance with long-range point tracks, which is key to ensure robustness with respect to fast motion and occlusions over longer video sequences. As opposed to previous approaches based on object proposals, we do not seek the best per-frame object hypotheses to perform the segmentation.  Instead, we combine multiple, potentially imperfect proposals to improve overall segmentation accuracy and ensure robustness to outliers. Overall, the basic algorithm consists of three steps. First we generate a very large number of  object proposals for each video frame using existing techniques. Next we perform an SVM-based pruning step to retain only high quality proposals with sufficiently discriminative power. Finally, we determine the fore- and background classification by solving for the maximum a posteriori of a fully connected conditional random field, defined using our novel energy function. Experimental results on a well established dataset demonstrate that our method compares favorably to several recent state-of-the-art approaches.</Data></Cell>
    <Cell><Data ss:Type="String">Federico Perazzi*, ETHZ - Disney Research Zurich; Oliver Wang, Disney Research Zurich; Alexander Sorkine-Hornung, Disney Research Zurich; Markus Gross, ETHZ - Disney Research Zurich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1016</Data></Cell>
    <Cell><Data ss:Type="String">Learning Parametric Distributions for Image Super-Resolution: Where Patch Matching Meets Sparse Coding</Data></Cell>
    <Cell><Data ss:Type="String">Existing approaches toward Image super-resolution (SR) is often either data-driven (e.g., based on internet-scale matching and web image retrieval) or model-based (e.g., formulated as an Maximizing a Posterior estimation problem). The former is conceptually simple yet heuristic; while the latter is constrained by the fundamental limit of frequency aliasing. In this paper, we propose to develop a hybrid approach toward SR by combining those two lines of ideas. More specifically, the parameters underlying sparse distributions of desirable HR image patches are learned from a pair of LR image and retrieved HR images. Our hybrid approach can be interpreted as the first attempt of reconciling the difference between parametric and nonparametric models for low-level vision tasks. Experimental results show that the proposed hybrid SR method performs much better than existing state-of-the-art methods in terms of both subjective and objective image qualities.</Data></Cell>
    <Cell><Data ss:Type="String">Yongbo Li, Xidian University; Weisheng Dong*, Xidian University; Guangming Shi, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1020</Data></Cell>
    <Cell><Data ss:Type="String">Variational PatchMatch MultiView Reconstruction and Refinement</Data></Cell>
    <Cell><Data ss:Type="String"> In this work we propose a novel approach to the problem of multi-view stereo reconstruction. Building upon the previously proposed PatchMatch stereo and PM-Huber algorithm we introduce an extension to the multi-view scenario that employs an iterative refinement scheme. Our proposed approach uses an extended and robustified volumetric truncated signed distance function representation, which is advantageous for the fusion of refined depth maps and also for raycasting the current reconstruction estimation together with estimated depth normals into arbitrary camera views. We formulate the combined multi-view stereo reconstruction and refinement as a variational optimization problem. The newly introduced plane based smoothing term in the energy formulation is guided by the current reconstruction confidence and the image contents. Further we propose an extension of the PatchMatch scheme with an additional KLT step to avoid unnecessary sampling iterations. Improper camera poses are corrected by a direct image aligment step that performs robust outlier compensation by means of a recently proposed kernel lifting framework. To speed up the optimization of the variational formulation an adapted scheme is used for faster convergence.</Data></Cell>
    <Cell><Data ss:Type="String">Philipp Heise*, Tech. Univ. Muenchen, TUM; Brian Jensen, TU Munich; Sebastian Klose, TU Munich; Alois Knoll, TU Munich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1025</Data></Cell>
    <Cell><Data ss:Type="String">As-Rigid-As-Possible Volumetric Shape-from-Template</Data></Cell>
    <Cell><Data ss:Type="String">The objective of Shape-from-Template (SfT) is to infer  an object’s shape from a single image and a 3D object tem-  plate. Existing methods are called thin-shell SfT as they  represent the object by its outer surface. This may be an  open surface for thin objects such as a piece of paper or a  closed surface for thicker objects such as a ball. We pro-  pose volumetric SfT, which specifically handles objects of  the latter kind. Volumetric SfT uses the object’s full volume  to express the deformation constraints and reconstructs the  object’s surface and interior deformation. This is a chal-  lenging problem because for opaque objects, only a part of  the outer surface is visible in the image. Inspired by mesh-  editing techniques, we use an As-Rigid-As-Possible (ARAP)  deformation model that softly imposes local rigidity. We  formalise ARAP isometric SfT as a constrained variational  optimisation problem which we solve using iterative opti-  misation. We present strategies to find an initial solution  based on thin-shell SfT and volume propagation. Experi-  ments with synthetic and real data show that our method  has a typical maximum relative error of 5% in reconstruct-  ing the deformation of an entire object, including its back  and interior for which no visual data is available.</Data></Cell>
    <Cell><Data ss:Type="String">Shaifali PARASHAR*, ISIT, University of Auvergne; Daniel Pizarro, Universidad de Alcala; Adrien Bartoli, Universite d'Auvergne, France; Toby Collins, Universite d'Auvergne</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1029</Data></Cell>
    <Cell><Data ss:Type="String">Object detection via a multi-region and semantic segmentation-aware  CNN model</Data></Cell>
    <Cell><Data ss:Type="String">We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules we detect object with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of $74.9\%$ and $69.1\%$ correspondingly, surpassing any other published work by a significant margin.</Data></Cell>
    <Cell><Data ss:Type="String">Spyros Gidaris*, Enpf.fr; Nikos Komodakis, &quot;Ecole des Ponts ParisTech, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1033</Data></Cell>
    <Cell><Data ss:Type="String">General Dynamic Scene Reconstruction from Multiple View Video</Data></Cell>
    <Cell><Data ss:Type="String">This paper introduces a general approach to dynamic  scene reconstruction from multiple moving cameras without  prior knowledge or limiting constraints on the scene structure,  appearance, or illumination. Existing techniques for  dynamic scene reconstruction from multiple wide-baseline  camera views primarily focus on accurate reconstruction in  controlled environments, where the cameras are fixed and  calibrated and background is known. These approaches  are not robust for general dynamic scenes captured with  sparse moving cameras. Previous approaches for outdoor  dynamic scene reconstruction assume prior knowledge of  the static background appearance and structure. The primary  contributions of this paper are twofold: an automatic  method for initial coarse dynamic scene segmentation and  reconstruction without prior knowledge of background appearance  or structure; and a general robust approach for  joint segmentation refinement and dense reconstruction of  dynamic scenes from multiple wide-baseline static or moving  cameras. Evaluation is performed on a variety of indoor  and outdoor scenes with cluttered backgrounds and multiple  dynamic non-rigid objects such as people. Comparison  with state-of-the-art approaches demonstrates improved accuracy  in both multiple view segmentation and dense reconstruction.  The proposed approach also eliminates the  requirement for prior knowledge of scene structure and appearance.</Data></Cell>
    <Cell><Data ss:Type="String">Armin Mustafa*, University of Surrey; Jean-Yves Guillemaut, University of Surrey; Hansung Kim, U.Surrey; Adrian Hilton, University of Surrey, UK</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1037</Data></Cell>
    <Cell><Data ss:Type="String">The joint image handbook</Data></Cell>
    <Cell><Data ss:Type="String"> Given $n\ge 2$ perspective cameras, point correspondences form the ``joint image'', effectively a replica of three-dimensional space distributed across its two-dimensional projections. This set can be characterized by multilinear equations over image coordinates, such as epipolar and trifocal constraints. We revisit in this paper the geometric and algebraic properties of the joint image, and address fundamental questions such as how many and which multilinearities are necessary and/or sufficient to determine camera geometry and/or image correspondences. The series of new theoretical results proven in this paper answers these questions in a very general setting and, in turn, is intended to serve as a practical &quot;handbook&quot; reference for useful facts about multilinearities.  </Data></Cell>
    <Cell><Data ss:Type="String">Matthew Trager*, École Normale Supérieure de Pa; Jean Ponce, ENS; Martial Hebert, Carnegie Mellon University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1046</Data></Cell>
    <Cell><Data ss:Type="String">Dense, Direct and Deformable: Non-Rigid 3D Reconstruction from RGB Video</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we tackle the problem of capturing the dense, detailed 3D geometry of generic non-rigid surfaces using a single RGB-only commodity video camera and a direct approach.  While robust and even real-time solutions exist to this problem if the observed scene is static, for non-rigid dense shape capture current systems are typically either restricted to the use of complex multi-camera rigs, take advantage of the additional depth channel available in RGB-D cameras, or are restricted to specific shapes such as faces.     In contrast, our method makes use of a single RGB video camera as input; does not require a known model of the observed object, so it can capture generic shapes; and the depth estimation is dense, per-pixel and direct.  Our energy optimization approach minimizes a photometric error that effectively estimates the temporal correspondences and 3D deformations simultaneously.  We first compute a dense 3D template of the shape of the object, using a short rigid sequence, and subsequently performs online reconstruction of the non-rigid surface as it evolves over time.  In our experimental evaluation we show a range of qualitative results on novel datasets and compare against an existing method that requires multi-frame optical flow.  </Data></Cell>
    <Cell><Data ss:Type="String">Rui Yu, UCL; Chris  Russell, UCL; Neill Campbell, UCL; Lourdes Agapito*, University College London</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1051</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Generation of a Viewpoint Annotated Car Dataset from Videos</Data></Cell>
    <Cell><Data ss:Type="String">Object recognition approaches have recently been extended to yield, aside of the object class output, also viewpoint or pose. Training such approaches typically requires additional viewpoint or keypoint annotation in the training data or, alternatively, synthetic CAD models. In this paper, we present an approach that creates a dataset of images annotated with bounding boxes and viewpoint labels in a fully automated manner from videos. We assume that the scene is static in order to reconstruct 3D surfaces via structure from motion. We automatically detect when the reconstruction fails and normalize for the viewpoint of the 3D models by aligning the reconstructed point clouds. Exemplarily for cars we show that we can expand a large dataset of annotated single images and obtain improved performance when training a viewpoint classifier on this joined dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Nima Sedaghat Alvar*, University of Freiburg; Thomas Brox, &quot;University of Freiburg, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1053</Data></Cell>
    <Cell><Data ss:Type="String">Regressing a 3D Face Shape from a Single Image</Data></Cell>
    <Cell><Data ss:Type="String">In this work we present a method to estimate a 3D face shape from a single image. Our method is based on a cascade regression framework that directly estimates face landmarks locations in 3D. We include the knowledge that a face is a 3D object into the learning pipeline and show how this information decreases localization errors while keeping the computational time low. We predict the actual positions of the landmarks even if they are occluded due to face rotation. To support the ability of our method to reliably reconstruct 3D shapes, we introduce a simple method for head pose estimation using a single image that reaches higher accuracy than the state of the art. Comparison of 3D face landmarks localization with the available state of the art further supports the feasibility of a single-step face shape estimation. The code, trained models and our 3D annotations will be made available to the research community.</Data></Cell>
    <Cell><Data ss:Type="String">Sergey Tulyakov*, University of Trento; Nicu  Sebe, &quot;University of Trento, Italy&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1055</Data></Cell>
    <Cell><Data ss:Type="String">Integrating Dashcam Views through Inter-Video Mapping </Data></Cell>
    <Cell><Data ss:Type="String">In this paper, an inter-video mapping approach is proposed to integrate video footages from two dashcams installed on a preceding and its following vehicle to provide the illusion that the driver of the following vehicle can see-through the preceding one.  The key challenge is to adapt the perspectives of the two videos based on a small number of common features since a large portion of the common region in the video captured by the following vehicle is occluded by the preceding one.  Inspired by the observation that images with the most similar viewpoints yield dense and high-quality matches,  the proposed inter-video mapping estimates spatially-varying motions across two videos utilizing images of very similar contents.  Specifically, we estimate frame-to-frame motions of each two consecutive images and incrementally add new views into a merged representation.  In this way, long-rang motion estimation is achieved, and the observed perspective discrepancy between the two videos can be well approximated our motion estimation.  Once the inter-video mapping is established, the correspondences can be updated incrementally, so the proposed method is suitable for on-line applications.  Our experiments demonstrate the effectiveness of our approach on real-world challenging videos.</Data></Cell>
    <Cell><Data ss:Type="String">Hsin-Yi Chen*, National Taiwan University; Bing-Yu Chen, ; Yi-Ling Chen, ; Wei-Tse Lee, ; Fan Wang, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1057</Data></Cell>
    <Cell><Data ss:Type="String">Visual Tracking with Fully Convolutional Networks</Data></Cell>
    <Cell><Data ss:Type="String">We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives.  A top layer encodes more semantic features and serves as a category detector, while a lower layer  carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark shows that the proposed tacker outperforms the state-of-the-art significantly.</Data></Cell>
    <Cell><Data ss:Type="String">Lijun Wang*, Dalian University of Tech; Wanli Ouyang, Chinese University of Hong Kong; Xiaogang Wang, The Chinese University of Hong Kong, Hong Kong; Huchuan Lu, Dalian University of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1058</Data></Cell>
    <Cell><Data ss:Type="String">Multiple Feature Fusion via Weighted Entropy for Visual Tracking</Data></Cell>
    <Cell><Data ss:Type="String">It is desirable to combine multiple feature descriptors to improve the visual tracking performance because different features can provide complementary information to describe objects of interest. However, how to effectively fuse multiple features remains a challenging problem in visual tracking, especially in a data-driven manner. In this paper, we propose a new data-adaptive visual tracking approach by using multiple feature fusion via weighted entropy. Unlike existing visual trackers which simply concatenate multiple feature vectors together for object representation, we employ the weighted entropy to evaluate the dissimilarity between the object state and the background state, and seek the optimal feature combination by minimizing the weighted entropy, so that more complementary information can be exploited for object representation. Experimental results demonstrate the effectiveness of our approach in tackling various challenges for visual object tracking.</Data></Cell>
    <Cell><Data ss:Type="String">Lin Ma*, Tsinghua university; Jiwen Lu, ADSC, Singapore; Jianjiang Feng, ; Jie Zhou, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1065</Data></Cell>
    <Cell><Data ss:Type="String">Local Subspace Collaborative Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Subspace models have been widely used for appearance based object tracking. Most existing subspace based trackers employ a linear subspace to represent object appearances, which are not accurate enough to model large variations of objects. To address this, this paper presents a local subspace collaborative tracking method for robust visual tracking, where multiple linear and nonlinear subspaces are learned to better model the nonlinear relationship of object appearances. First, we retain a set of key samples and compute a set of local subspaces for each key sample. Then, we construct a hyper sphere to represent the local nonlinear subspace for each key sample. The hyper sphere of one key sample passes the local key samples and also is tangent to the local linear subspace of the specific key sample. In this way, we are able to represent the nonlinear distribution of the key samples and also approximate the local linear subspace near the specific key sample, so that local distributions of the samples can be represented more accurately. Experimental results on challenging video sequences demonstrate the effectiveness of our method.</Data></Cell>
    <Cell><Data ss:Type="String">Lin Ma*, Tsinghua university; Xiaoqin Zhang, ; Weiming Hu, ; Junliang Xing, ; Jiwen Lu, ADSC, Singapore; Jie Zhou, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1071</Data></Cell>
    <Cell><Data ss:Type="String">Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks</Data></Cell>
    <Cell><Data ss:Type="String">Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a  completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks.      In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations  are available and achieve state-of-the-art performance for the Stanford Dog dataset.  We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios. We publish the source code of our method online upon acceptance.</Data></Cell>
    <Cell><Data ss:Type="String">Marcel Simon*, FSU Jena; Erik Rodner, &quot;Computer Vision Group, University of Jena&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1072</Data></Cell>
    <Cell><Data ss:Type="String">FaceDirector: Continuous Control of Facial Performance in Video</Data></Cell>
    <Cell><Data ss:Type="String">We present a method to continuously blend between multiple facial performances of an actor, which can contain different facial expressions or emotional states. As an example, given sad and angry video takes of a scene, our method empowers the movie director to specify arbitrary weighted combinations and smooth transitions between the two takes in post-production. Our contributions include (1) a robust nonlinear audio-visual synchronization technique that exploits complementary properties of audio and visual cues to automatically determine robust, dense spatiotemporal correspondences between takes, and (2) a seamless facial blending approach that provides the director full control to interpolate timing, facial expression, and local appearance, in order to generate novel performances after filming. In contrast to most previous works, our approach operates entirely in image space, avoiding the need of 3D facial reconstruction. We demonstrate that our method can synthesize visually believable performances with applications in emotion transition, performance correction, and timing control.</Data></Cell>
    <Cell><Data ss:Type="String">Charles Malleson, Surrey; Jean-Charles Bazin*, Disney Research Zurich; Oliver Wang, Disney Research Zurich; Thabo Beeler, DIsney Research; Derek Bradley, DIsney Research; Adrian Hilton, University of Surrey, UK; Alexander Sorkine-Hornung, Disney Research Zurich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1077</Data></Cell>
    <Cell><Data ss:Type="String">Hyperpoints and Fine Vocabularies for Large-Scale Location Recognition</Data></Cell>
    <Cell><Data ss:Type="String">Structure-based localization is the task of finding the absolute pose of a given query image w.r.t. a pre-computed 3D model. While this is almost trivial at small scale, special care must be taken as the size of the 3D model grows, because straight-forward descriptor matching becomes ineffective due to the large memory footprint of the model, as well as the strictness of the ratio test in 3D. Recently, several authors have tried to overcome these problems, either by a smart compression of the 3D model or by clever sampling strategies for geometric verification. Here we explore an orthogonal strategy, which uses all the 3D points and standard sampling, but performs feature matching implicitly, by quantization into a fine vocabulary. We show that although this matching is ambiguous and gives rise to 3D hyperpoints when matching each 2D query feature in isolation, a simple voting strategy, which enforces the fact that the selected 3D points shall be co-visible, can reliably find a locally unique 2D-3D point assignment. Experiments on two large-scale datasets demonstrate that our method achieves state-of-the-art performance, while the memory footprint is greatly reduced, since only visual word labels but no 3D point descriptors need to be stored.</Data></Cell>
    <Cell><Data ss:Type="String">Torsten Sattler*, ETH Zurich; Michal Havlena, ETH Zurich; Filip Radenovic, CMP, CTU in Prague; Konrad Schindler, &quot;ETH Zurich, Switzerland&quot;; Marc Pollefeys, ETH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1097</Data></Cell>
    <Cell><Data ss:Type="String">Video Segmentation with Just a Few Strokes</Data></Cell>
    <Cell><Data ss:Type="String">As the use of videos is becoming more popular in computer vision, the need for annotated video datasets increases.  Such datasets are required either as training data or simply as ground truth for benchmark datasets. A particular challenge in video segmentation is due to disocclusions, which hamper frame-to-frame propagation, in conjunction with non-moving objects. We show that a combination of motion from point trajectories, as known from motion segmentation, along with minimal supervision can largely help solve this problem. Moreover, we integrate a new constraint that enforces consistency of the color distribution in successive frames. We quantify user interaction effort with respect to segmentation quality on challenging ego motion videos.  We compare our approach to a diverse set of algorithms in terms of user effort and in terms of performance on common video segmentation benchmarks.</Data></Cell>
    <Cell><Data ss:Type="String">Naveen Shankar Nagaraja*, University of Freiburg; Frank Schmidt, TUM; Thomas Brox, &quot;University of Freiburg, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1101</Data></Cell>
    <Cell><Data ss:Type="String">Learning Spatially Regularized Correlation Filters for Visual Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Robust and accurate visual tracking is one of the most challenging computer vision problems. Due to the inherent lack of training data, a robust approach for constructing a target appearance model is crucial. Recently, discriminatively learned correlation filters (DCF) have been successfully applied to counter this problem for tracking. These methods utilize a periodic assumption of the training samples to efficiently learn the classifier on all patches in the target neighborhood. However, the periodic assumption also introduces unwanted boundary effects, which severely degrade the quality of the tracking model.    We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. A spatial regularization component is used to modulate the correlation filter coefficients in the learning of the model. The proposed SRDCF formulation allows the correlation filters to be learned on a significantly larger set of negative training samples, without corrupting the positive samples. We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efficient online learning of our SRDCF. Experiments are performed on four benchmark datasets: OTB-2013, ALOV++, OTB-2015 and VOT2014. Our approach achieves state-of-the-art results on all four datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and 7.5% respectively, in mean overlap precision, compared to the best existing trackers.</Data></Cell>
    <Cell><Data ss:Type="String">Martin Danelljan*, Linköping University; Gustav Häger, Linköping University; Fahad Khan, &quot;Computer Vision Laboratory, Linkoping University , Sweden&quot;; Michael Felsberg, Link_ping University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1104</Data></Cell>
    <Cell><Data ss:Type="String">Globally Optimal 2D / 3D Registration from Points or Lines Without Correspondences</Data></Cell>
    <Cell><Data ss:Type="String">We present a novel approach to 2D / 3D registration from points or lines without correspondences. While there   exist established solutions in the case where correspondences are known, there are many situations where it is   not possible to reliably extract such correspondences across modalities, thus requiring the use of a   correspondence-free registration algorithm. Existing correspondence-free methods rely on local search strategies   and consequently have no guarantee of finding the optimal solution. In contrast, we present the first globally   optimal approach to 2D / 3D registration without correspondences, achieved by a Branch-and-Bound algorithm.   Furthermore, a deterministic annealing procedure is proposed to speed up the nested branch-and-bound algorithm used.   The theoretical and practical advantages this brings are demonstrated on a range of synthetic and real data where   it is observed that the proposed approach is significantly more robust to high proportions of outliers compared   to existing approaches.</Data></Cell>
    <Cell><Data ss:Type="String">Mark Brown*, University of Surrey; David Windridge, Middlesex University; Jean-Yves Guillemaut, University of Surrey</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1106</Data></Cell>
    <Cell><Data ss:Type="String">Improving Image Restoration with Soft-Rounding</Data></Cell>
    <Cell><Data ss:Type="String">Several important classes of images such as text, barcode and pattern images have the property that pixels can only take a distinct subset of values. This knowledge can benefit the restoration of such images, but it has not been widely considered in current restoration methods. In this work, we describe an effective and efficient approach to incorporate the knowledge of distinct pixel values of the pristine images into the general regularized least squares restoration framework. We introduce a new regularizer that attains zero at the designated pixel values and becomes a quadratic penalty function in the intervals between them. When incorporated into the regularized least squares restoration framework, this regularizer leads to a simple and efficient step that resembles and extends the rounding operation, which we term as soft-rounding. We apply the soft-rounding enhanced solution to the restoration of binary text/barcode images and pattern images with multiple distinct pixel values. Experimental results show that soft-rounding enhanced restoration methods achieve significant improvement in both visual quality and quantitative measures (PSNR and SSIM). Furthermore, we show that this regularizer can also benefit the restoration of general natural images.</Data></Cell>
    <Cell><Data ss:Type="String">Xing Mei*, University at Albany, SUNY; Honggang Qi, University of Chinese Academy of Sciences; Baogang Hu, NLPR; Siwei Lyu, SUNY Albany</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1107</Data></Cell>
    <Cell><Data ss:Type="String">SpeDo: 6 DOF Ego-Motion Sensor Using Speckle Defocus Imaging</Data></Cell>
    <Cell><Data ss:Type="String">Sensors that measure their motion with respect to the surrounding environment (ego-motion sensors) can be broadly classified into two categories. First is inertial sensors such as accelerometers. In order to estimate position and velocity, these sensors integrate the measured acceleration, which often results in accumulation of large errors over time. Second, camera-based approaches such as SLAM that can measure velocity directly, but their performance depends on the surrounding scenes properties. These approaches cannot function reliably if the scene has low frequency textures or small depth variations. We present a novel ego-motion sensor called SpeDo that addresses these fundamental limitations. SpeDo is based on using coherent light sources and cameras with large defocus. Coherent light, on interacting with a scene, creates a high frequency interferometric pattern in the captured images, called speckle. We develop a theoretical model for speckle flow (motion of speckle as a function of sensor motion), and show that it is quasi-invariant to surrounding scenes properties. As a result, SpeDo can measure ego-motion (not derivative of motion) simply by estimating optical flow at a few image locations. We have built a low-cost and compact hardware prototype of SpeDo and demonstrated high precision 6 DOF ego-motion estimation for complex trajectories in scenarios where the scene properties are challenging (e.g., repeating or no texture) as well as unknown.</Data></Cell>
    <Cell><Data ss:Type="String">Kensei Jo*, Columbia University; Mohit Gupta, ; Shree Nayar, &quot;Columbia University, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1109</Data></Cell>
    <Cell><Data ss:Type="String">Detection and Segmentation of 2D Curved Symmetric Structures</Data></Cell>
    <Cell><Data ss:Type="String">Symmetry, as one of the key components of Gestalt theory, provides an important mid-level cue that serves as input to higher visual processes such as segmentation. In this work, we propose a complete approach that links the detection of curved reflection symmetries to produce symmetry-constrained segments of structures/regions in real images with clutter. For curved symmetry detection, we leverage on patch-based symmetric features to train a Structured Random Forest classifier that detects multiscaled curved symmetries in 2D images. Next, using these curved symmetries, we modulate a novel symmetry-constrained foreground-background segmentation by their symmetry scores so that we enforce global symmetrical consistency in the final segmentation. This is achieved by imposing a pairwise symmetry prior that encourages symmetric pixels to have the same labels over a MRF-based representation of the input image edges, and the final segmentation is obtained via graph-cuts. Experimental results over four publicly available datasets containing annotated symmetric structures: 1) SYMMAX-300, 2) BSD-Parts, 3) Weizmann Horse and 4) NY-roads demonstrate the approach's applicability to different environments with state-of-the-art performance.</Data></Cell>
    <Cell><Data ss:Type="String">Ching Teo*, University of Maryland; Cornelia Fermuller, Univ. Maryland USA; Yiannis Aloimonos, UMD</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1113</Data></Cell>
    <Cell><Data ss:Type="String">Rendering of Eyes for Eye-Shape Registration and Gaze Estimation</Data></Cell>
    <Cell><Data ss:Type="String">Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniques to build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model’s controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.</Data></Cell>
    <Cell><Data ss:Type="String">Erroll Wood*, University of Cambridge; Tadas Baltrušaitis, Computer Laboratory, University of Cambridge; Peter Robinson, Computer Laboratory, University of Cambridge; Andreas Bulling, MPI Informatics; Yusuke Sugano, Max Planck Institute for Informatics; Xucong Zhang, MPI-INF</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1114</Data></Cell>
    <Cell><Data ss:Type="String">Beyond RMS: Geometry-Aware Performance Evaluation of Stereo Algorithms</Data></Cell>
    <Cell><Data ss:Type="String">Performance characterization of stereo methods is mandatory to decide which algorithm is useful for which application. Prevalent benchmarks use the root mean squared error (RMS) with respect to ground truth disparity maps to quantify algorithm performance.    We show that the RMS is of limited expressiveness for algorithm selection. Instead, we assess stereo results by harnessing three semantic cues: depth discontinuities, planar surfaces, and fine geometric structures. For each cue, we extract the relevant set of pixels from existing ground truth. We then apply our metric functions to quantify characteristics such as edge fattening and surface smoothness.    We demonstrate that our approach supports practitioners in selecting the most suitable algorithm for their application. Using the new Middlebury dataset, we show that rankings based on our metrics reveal specific algorithm strengths and weaknesses which are not quantified by existing metrics. We finally show how stacked bar charts and radar charts visually support multidimensional performance evaluation.</Data></Cell>
    <Cell><Data ss:Type="String">Katrin Honauer*, HCI, University of Heidelberg; Lena Maier-Hein, DKFZ Heidelberg; Daniel Kondermann, HCI, university of Heidelberg</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1119</Data></Cell>
    <Cell><Data ss:Type="String">Multi-scale Learning for Low-resolution Person Re-identification</Data></Cell>
    <Cell><Data ss:Type="String">In real world person re-identification (re-id), images of people captured at very different resolutions from different locations need be matched. Existing re-id models typically normalise all person images to the same size. However, a low-resolution (LR) image contains much less information about a person, and direct image scaling and simple size normalisation as done in conventional re-id methods cannot compensate for the loss of information. To solve this LR person re-id problem, we propose a novel joint multi-scale learning framework, termed joint multi-scale discriminant component analysis (JUDEA). The key component of this framework is a heterogeneous class mean discrepancy (HCMD) criterion for cross-scale image domain alignment, which is optimised simultaneously with discriminant modelling across multiple scales in the joint learning framework. Our experiments show that the proposed JUDEA framework outperforms existing representative re-id methods as well as other related LR visual matching models applied for the LR re-id person problem.</Data></Cell>
    <Cell><Data ss:Type="String">Xiang Li, SYSU; Wei-Shi Zheng*, Sun Yat-Sen University; Xiaojuan Wang, SYSU; Tao Xiang, Queen Mary University of London; Shaogang Gong, QMUL</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1120</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Learning of Spatiotemporally Coherent Metrics</Data></Cell>
    <Cell><Data ss:Type="String">Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning and metric learning. Using this connection we define &quot;temporal coherence&quot;&#45;-a criterion which can be used to select hyper-parameters automatically. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labeled data. </Data></Cell>
    <Cell><Data ss:Type="String">Ross Goroshin*, New York University; Joan Bruna, New York University; Jonathan Tompson, ; David Eigen, New York University; Yann LeCun, New York University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1125</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Trajectory Clustering via Adaptive Multi-Kernel-based Shrinkage</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes a shrinkage-based framework for unsupervised trajectory clustering.  Facing to the challenges of trajectory clustering, e.g., large variations within a cluster and ambiguities across clusters, we first introduce an adaptive multi-kernel-based estimation process to estimate the `shrunk' positions and speeds of trajectories' points.  This kernel-based estimation effectively leverages both multiple structural information within a trajectory and the local motion patterns across multiple trajectories, such that the discrimination of the shrunk point can be properly increased.  We further introduce a speed-regularized optimization process, which utilizes the estimated speeds to regularize the optimal shrunk points, so as to guarantee the smoothness and the discriminative pattern of the final shrunk trajectory.  Using our approach, the variations among similar trajectories can be reduced while the boundaries between different clusters are enlarged.  Experimental results demonstrate that our approach is superior to the state-of-art approaches on both clustering accuracy and robustness.  Besides, additional experiments further reveal the effectiveness of our approach when applied to trajectory analysis applications such as anomaly detection.</Data></Cell>
    <Cell><Data ss:Type="String">Hongteng Xu*, Georgia Institute of Technology; Yang Zhou, Shanghai Jiao Tong University; Weiyao Lin, Shanghai Jiaotong University; Hongyuan Zha, Georgia Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1133</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders</Data></Cell>
    <Cell><Data ss:Type="String">With the growing popularity of short-form video sharing platforms such as \em{Instagram} and \em{Vine}, there has been an increasing need for techniques that automatically extract highlights from video. Whereas prior works have approached this problem with heuristic rules or supervised learning, we present an unsupervised learning approach that takes advantage of the abundance of user-edited videos on social media websites such as YouTube. Based on the idea that the most significant sub-events within a video class are commonly present among edited videos while less interesting ones appear less frequently, we identify the significant sub-events via a robust recurrent auto-encoder trained on a collection of user-edited videos queried for each particular class of interest. The auto-encoder is trained using a proposed shrinking exponential loss function that makes it robust to noise in the web-crawled training data, and is configured with bidirectional long short term memory (LSTM)~\cite{LSTM:97} cells to better model the temporal structure of highlight segments. Different from supervised techniques, our method can infer highlights using only a set of downloaded edited videos, without also needing their pre-edited counterparts which are rarely available online. Extensive experiments indicate the promise of our proposed solution in this challenging unsupervised setting.</Data></Cell>
    <Cell><Data ss:Type="String">Huan Yang, Shanghai Jiao Tong University; Baoyuan Wang*, Microsoft Research Asia; Stephen  Lin, &quot;Microsoft Research Asia, China&quot;; David Wipf, Microsoft Research Asia; Minyi Guo, Shanghai Jiao Tong University; Baining Guo, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1134</Data></Cell>
    <Cell><Data ss:Type="String">Learning to transfer: transferring latent task structures and its application to person-specific facial action unit detection</Data></Cell>
    <Cell><Data ss:Type="String">In this article we explore the problem of constructing person-specific models for the detection of facial Action Units (AUs), addressing the problem from the point of view of Transfer Learning and Multi-Task Learning. Our starting point is the fact that some expressions, such as smiles, are very easily elicited, annotated, and automatically detected, while others are much harder to elicit and to annotate. We thus consider a novel problem:  all AU models for the target subject are to be learnt using person-specific annotated data for a reference AU (AU12 in our case), and no data or little data regarding the target AU. In order to design such a model, we propose a novel Multi-Task Learning and the associated Transfer Learning framework, in which we consider both relations across subjects and  AUs. That is to say, we consider a tensor structure among the tasks. Our approach hinges on learning the latent relations among tasks using one single reference AU, and then transferring these latent relations to other AUs. We show that we are able to effectively make use of the annotated data for AU12 when learning other person-specific AU models, even in the absence of data for the target task. Finally, we show the excellent performance of our method when small amounts of annotated data for the target tasks are made available.</Data></Cell>
    <Cell><Data ss:Type="String">Timur Almaev, University of Nottingham; Brais Martinez, University of Nottingham; Michel Valstar*, Nottingham University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1143</Data></Cell>
    <Cell><Data ss:Type="String">TRIC-track: Tracking by Regression with Incrementally Learned Cascades</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes a novel approach to part-based tracking by replacing local matching of an appearance model by direct prediction of the displacement between local image patches and parts' location. Targeting primarily highly deformable objects, we propose to use cascaded regression with incremental learning to track generic objects without any prior knowledge of an object's structure or appearance. We exploit the spatial constraints between parts by implicitly learning the shape and deformation parameters of the object in an online fashion. We integrate a multiple temporal scale motion model to initialise our cascaded regression search close to the target and to allow it to cope with occlusions. Experimental results compared with all the state-of-the-art in the 2013 CVPR Visual Tracker Benchmark show that our tracker ranks first on the non-rigid deformation (DEF) dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaomeng Wang, University of Nottingham; Michel Valstar*, Nottingham University; Brais Martinez, University of Nottingham; Muhammad Khan, University of Nottingham; Tony Pridmore, University of Nottingham</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1144</Data></Cell>
    <Cell><Data ss:Type="String">See the Difference: Direct Pre-Image Reconstruction and Pose Estimation by Differentiating HOG</Data></Cell>
    <Cell><Data ss:Type="String">The Histogram of Oriented Gradient (HOG) descriptor has led to many advances in computer vision over the last decade and is still part of many state of the art approaches.   We realize that the associated feature computation is piecewise differentiable and therefore many pipelines which build on HOG can be made differentiable. This lends to advanced introspection as well as opportunities for end-to-end optimization. We present our implementation of $\nabla$ HOG based on the auto-differentiation toolbox Chumpy and show applications to per-image visualization and pose estimation which extends the existing differentiable renderer OpenDR pipeline. Both applications improve on the respective state-of-the-art HOG approaches.</Data></Cell>
    <Cell><Data ss:Type="String">Wei-Chen Chiu*, MPI-ING; Mario Fritz, MPI Informatics, Saarbrucken, Germany</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1145</Data></Cell>
    <Cell><Data ss:Type="String">An Efficient Statistical Method for Image Noise Level Estimation</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we address the problem of estimating noise level from a single image contaminated by additive zero-mean Gaussian noise. We first provide rigorous analysis on the statistical relationship between the noise variance and the eigenvalues of the covariance matrix of patches within an image, which shows that many state-of-the-art noise estimation methods underestimate the noise level of an image. To this end, we derive a new nonparametric algorithm for efficient noise level estimation based on the observation that patches decomposed from a clean image often lie around a low-dimensional subspace. The performance of our method has been guaranteed both theoretically and empirically. Specifically, our method outperforms existing state-of-the-art algorithms on estimating noise level with the least executing time in our experiments. We further demonstrate that the denoising algorithm BM3D algorithm achieves optimal performance using noise variance estimated by our algorithm.</Data></Cell>
    <Cell><Data ss:Type="String">Guangyong Chen*, CUHK; Fengyuan Zhu, CUHK; Pheng ann Heng, CUHK</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1147</Data></Cell>
    <Cell><Data ss:Type="String">A Multiscale Variable-grouping Framework for MRF Energy Minimization</Data></Cell>
    <Cell><Data ss:Type="String">We present a multiscale approach for minimizing the energy associated with Markov Random Fields (MRFs) with energy functions that include arbitrary pairwise potentials. The MRF is represented on a hierarchy of successively coarser scales, where the problem on each scale is itself an MRF with suitably defined potentials. These representations are used to construct an efficient multiscale algorithm that seeks a minimal-energy solution to the original problem. The algorithm is iterative and features a bidirectional crosstalk between fine and coarse representations. We use consistency criteria to guarantee that the energy is nonincreasing throughout the iterative process. The algorithm is evaluated on real-world datasets, achieving competitive performance in relatively short run-times.</Data></Cell>
    <Cell><Data ss:Type="String">Omer Meir*, Weizmann Institute of Science; Meirav Galun, ; Stav Yagev, Weizmann Institute of Science; Ronen Basri, Weizmann Institute of Science; Irad Yavneh, Technion Israel Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1150</Data></Cell>
    <Cell><Data ss:Type="String">Inferring M-Best Diverse Labelings in a Single One</Data></Cell>
    <Cell><Data ss:Type="String">We consider the task of finding $M$-best diverse solutions in a graphical model. In a previous work by Batra et al. an algorithmic approach for finding such solutions was proposed, and its usefulness was shown in numerous applications. Contrary to previous work we propose a novel formulation of the problem in form of a single energy minimization problem in a specially constructed graphical model. We show that the method of Batra et al. can be considered as a greedy approximate algorithm for our model, whereas we introduce an efficient specialized optimization technique for it, based on alpha-expansion. We evaluate our method on two application scenarios, interactive and semantic image segmentation, with binary and multiple labels. In both cases we achieve considerably better error rates than state-of-the art diversity methods. Furthermore, we empirically discover that in the binary label case we were able to reach global optimality for all test instances.</Data></Cell>
    <Cell><Data ss:Type="String">Alexander Kirillov*, TU Dresden; Bogdan Savchynskyy, Dresden University of Technology; Dmitrij Schlesinger, TU Dresden; Dmitry Vetrov, HSE; Carsten Rother, TU Dresden</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1151</Data></Cell>
    <Cell><Data ss:Type="String">Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">  A novel efficient method for extraction of object proposals is introduced. Its ”objectness” function exploits deep spatial pyramid features, a new fast-to-compute HoG-based edge statistic and the EdgeBoxes score. The efficiency is achieved by the use of spatial bins in combination with sparsity-inducing group normalized SVM.    State-of-the-art recall performance is achieved on Pascal VOC07, significantly outperforming methods with comparable speed. Interestingly, when only 100 proposals per image are considered our method attains 78% recall on VOC07. The novel method improves mAP of the RCNN state-of-the-art class-specific detector, increasing it by 10 points when only 50 proposals are utilized in each image. Despite training the system on twenty classes, results on two-hundred class ILSVRC2013 dataset confirm generalization to previously unseen data.</Data></Cell>
    <Cell><Data ss:Type="String">David Novotny*, Center for Machine Perception; Jiri Matas, Czech Technical University in Prague</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1152</Data></Cell>
    <Cell><Data ss:Type="String">Merging the Unmatchable: Stitching Visually Disconnected SfM Models</Data></Cell>
    <Cell><Data ss:Type="String">Recent advances in Structure-from-Motion not only enable the reconstruction of large scale scenes but are also able to detect ambiguous structures caused by repeating elements that might result in incorrect reconstructions. Yet, it is not always possible to fully reconstruct a scene. The images required to merge different sub-models might be missing or it might be impossible to acquire such images in the first place due to occlusions or the structure of the scene. The problem of aligning multiple reconstructions that do not have visual overlap is impossible to solve in general. An important variant of this problem is the case in which individual sides of a building can be reconstructed but not joined due to the missing visual overlap. In this paper, we present a combinatorial approach for solving this variant by automatically stitching multiple sub-models of a building together. Our approach exploits symmetries and semantic information to reason about the possible geometric relations between the individual models. We show that our approach is able to reconstruct complete building models where traditional SfM ends up with disconnected models.</Data></Cell>
    <Cell><Data ss:Type="String">Andrea Cohen*, ETHZ; Torsten Sattler, ETH Zurich; Marc Pollefeys, ETH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1154</Data></Cell>
    <Cell><Data ss:Type="String">Multi-Label Cross-modal Retrieval</Data></Cell>
    <Cell><Data ss:Type="String">In this work, we address the problem of cross-modal retrieval in presence of  multi-label annotations.  In particular, we introduce multi-label Canonical  Correlation Analysis (ml-CCA), an extension of CCA, for learning shared  subspaces taking into account high level semantic information in the form of  multi-label annotations. Unlike CCA, ml-CCA does not rely on explicit pairing  between modalities, instead it uses the multi-label information to establish  correspondences.  This results in a discriminative subspace which is better  suited for cross-modal retrieval tasks.  We also present Fast ml-CCA, a  computationally efficient version of ml-CCA, which is able to handle large  scale datasets.  We show the efficacy of our approach by conducting extensive  cross-modal retrieval experiments on three standard benchmark datasets. The results show  that the proposed approach achieves state of the art retrieval performance on  the three datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Viresh Ranjan*, IIIT, Hyderabad; Nikhil  Rasiwasia, Snapdeal.com; C.V. Jawahar, IIIT Hyderabad</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1159</Data></Cell>
    <Cell><Data ss:Type="String">Probabilistic Label Relation Graphs with Ising Models</Data></Cell>
    <Cell><Data ss:Type="String">We consider classification problems in which the label space has structure. A common example is hierarchical label spaces, corresponding to the case where one label subsumes another (e.g., animal subsumes dog). But labels can also be mutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). To jointly model hierarchy and exclusion relations, the notion of a HEX (hierarchy and exclusion) graph was introduced in [7]. This combined a conditional random field (CRF) with a deep neural network (DNN), resulting in state of the art results when applied to visual object classification problems where the training labels were drawn from different levels of the ImageNet hierarchy (e.g., an image might be labeled with the basic level category &quot;dog'', rather than the more specific label &quot;husky''). In this paper, we extend the HEX model to allow for soft or probabilistic relations between labels, which is useful when there is uncertainty about the relationship between two labels (e.g., an antelope is &quot;sort of'' furry, but not to the same degree as a grizzly bear). We call our new model pHEX, for probabilistic HEX. We show that the pHEX graph can be converted to an Ising model, which allows us to use existing off-the-shelf inference methods (in contrast to the HEX method, which needed specialized inference algorithms). Experimental results show significant improvements in a number of large-scale visual object classification tasks, outperforming the previous HEX model.</Data></Cell>
    <Cell><Data ss:Type="String">Nan Ding*, Google Inc. ; Jia Deng, University of Michigan; Kevin Murphy, Google Inc.; Hartmut Neven, Google Inc.</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1160</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Tube Extraction using Transductive Learning and Dense Trajectories</Data></Cell>
    <Cell><Data ss:Type="String">    We address the problem of automatic extraction of foreground objects from videos. The goal is to provide a method for unsupervised collection of samples  which can be further used for object detection training without any human intervention. We use the well known Selective Search approach to produce an initial still-image based segmentation of the video frames.    This initial set of proposals is pruned and temporally extended using optical flow and transductive learning.     Specifically, we propose to  use Dense Trajectories in order to robustly match and track candidate boxes over different frames. The obtained box tracks are used to collect samples for unsupervised training of track-specific detectors. Finally, the detectors are run on the videos  to extract the final tubes. The combination of appearance-based static ''objectness'' (Selective Search), motion information (Dense Trajectories) and transductive learning (detectors are forced to &quot;overfit&quot; on the unsupervised data used for training) makes the proposed approach extremely robust. We are able to outperform the-state-of-the-art by more than 30% relative improvement on the common UCF Sport benchmark used for tube proposal evaluation. </Data></Cell>
    <Cell><Data ss:Type="String">Mihai Puscas, University of Trento; Enver Sangineto*, University of Trento; Dubravko Culibrk, University of Trento; Nicu  Sebe, &quot;University of Trento, Italy&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1162</Data></Cell>
    <Cell><Data ss:Type="String">Predicting Multiple Structured Visual Interpretations</Data></Cell>
    <Cell><Data ss:Type="String">We present a simple approach for producing a small number of structured visual outputs which have high recall, for a variety of tasks including monocular pose estimation and semantic scene segmentation. Current state-of-the-art approaches learn a single model and modify inference procedures to produce a small number of diverse predictions.  We take the alternate route of modifying the learning procedure to directly optimize for good, high recall sequences of structured-output predictors. Our approach introduces no new parameters, naturally learns diverse predictions and is not tied to any specific structured learning or inference procedure. We leverage recent advances in the contextual submodular maximization literature to learn a sequence of predictors and empirically demonstrate the simplicity and performance of our approach on multiple challenging vision tasks including achieving state-of-the-art results on multiple predictions for monocular pose-estimation and image foreground/background segmentation.  </Data></Cell>
    <Cell><Data ss:Type="String">Debadeepta Dey*, Carnegie Mellon University; Varun Ramakrishna, Carnegie Mellon University; Martial Hebert, Carnegie Mellon University; J. Andrew Bagnell, Carnegie Mellon University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1167</Data></Cell>
    <Cell><Data ss:Type="String">Person Re-Identification with Discriminatively Trained Viewpoint Invariant Dictionaries</Data></Cell>
    <Cell><Data ss:Type="String">This paper introduces a new approach to address the person re-identification problem in cameras with non-overlapping fields of view. Unlike previous approaches that learn Mahalanobis-like distance metrics in some embedding space, we propose to learn  a dictionary that is capable of discriminatively and sparsely encoding features representing different people.    Our approach directly addresses two key challenges in person re-identification: viewpoint variations and discriminability. First, to tackle viewpoint and associated appearance changes, we learn a single dictionary to represent both gallery and probe images in the training phase. We then discriminatively train the dictionary by enforcing explicit constraints on the associated sparse representations of the feature vectors. In the testing phase, we re-identify a probe image by simply determining the gallery image that has the closest sparse representation to that of the probe image in the Euclidean sense.    Extensive performance evaluations on two publicly available multi-shot re-identification datasets demonstrate the advantages of our algorithm over several state-of-the-art dictionary  learning, temporal sequence matching, and spatial appearance and metric learning based techniques.</Data></Cell>
    <Cell><Data ss:Type="String">Srikrishna Karanam*, Rensselaer Polytechnic I.; Yang Li, Rensselaer Polytechnic Institu; Richard Radke, Rensselaer Polytechnic Institute</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1174</Data></Cell>
    <Cell><Data ss:Type="String">Pairwise Conditional Random Forests for Facial Expression Recognition</Data></Cell>
    <Cell><Data ss:Type="String">Facial expression can be seen as the dynamic variation of one’s appearance over time. Successful recognition thus involves finding representations of high-dimensional spatiotemporal patterns that can be generalized to unseen facial morphologies and variations of the expression dynamics. In this paper, we propose to learn Random Forests from heterogeneous derivative features (e.g. facial fiducial point movements or texture variations) upon pairs of images. Those forests are conditioned on the expression label of the first frame to reduce the variability of the ongoing expression transitions. When testing on a specific frame of a video, pairs are created between this frame and the previous ones. Predictions for each previous frame are used to draw trees from Pairwise Conditional Random Forests (PCRF) whose pairwise outputs are averaged over time to produce robust estimates. As such, PCRF appears as a natural extension of Random Forests to learn spatio-temporal patterns, that leads to significant improvements over standard Random Forests as well as state-of-the-art approaches on several facial expression benchmarks.</Data></Cell>
    <Cell><Data ss:Type="String">Arnaud Dapogny*, ISIR-UPMC; Kevin Bailly, UPMC; Severine Dubuisson, UPMC</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1176</Data></Cell>
    <Cell><Data ss:Type="String">3D Fragment Reassembly using Integrated Template Guidance and Fracture-Region Matching</Data></Cell>
    <Cell><Data ss:Type="String">This paper studies the matching of fragmented objects to recompose their original geometry. Effectively solving this matching problem has direct applications in archaeology and forensic investigation in the computer-aided restoration of damaged artifacts and evidence.   We develop a reassembly algorithm to effectively integrate both guidance from a template and from matching of adjacent pieces' fracture-regions.  First, we compute partial matchings between fragments and a template, and pairwise matchings among fragments.   Many potential matches are obtained and then selected/refined in a multi-piece matching stage to maximize global groupwise matching consistency.   This pipeline is effective in composing fragmented objects that are thin-shells and contain small pieces, whose   pairwise matching is usually unreliable and ambiguous and hence their reassembly remains challenging to the existing algorithms. </Data></Cell>
    <Cell><Data ss:Type="String">Kang Zhang, Louisiana State University; Wuyi Yu, Louisiana State University; Mary Manhein, ; Waggenspack Warren, Louisiana State University; Xin Li*, Louisiana State University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1185</Data></Cell>
    <Cell><Data ss:Type="String">Single Image Pop-Up from Discriminatively Learned Parts</Data></Cell>
    <Cell><Data ss:Type="String">We introduce a new approach for estimating a fine  grained 3D shape and continouse pose of an object from  a single image. Given a training set of view exemplars,  we learn and select appearance-based discriminative parts  which are mapped onto the 3D model through a facility lo-  cation optimization. The training set of 3D models is sum-  marized into a sparse set of shapes from which we can gen-  eralize by linear combination. Given a test picture, we de-  tect hypotheses for each part. The main challenge is to se-  lect from these hypotheses and compute the 3D pose and  shape coefficients at the same time. To achieve this, we op-  timize a function that considers simultaneously the appear-  ance matching of the parts as well as the geometric repro-  jection error. We apply the alternating direction method of  multipliers (ADMM) to minimize the resulting convex func-  tion. Our main and novel contribution is the simultaneous  solution for part localization and detailed 3D geometry esti-  mation by maximizing both appearance and geometric com-  patibility with convex relaxation.</Data></Cell>
    <Cell><Data ss:Type="String">Menglong Zhu*, University of Pennsylvania; Xiaowei Zhou, University of Pennsylvania; Kostas Danilidiis, University of Pennsylvania</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1190</Data></Cell>
    <Cell><Data ss:Type="String">Compositional Hierarchical Representation of Shape Manifolds for Classification of Non-manifold Shapes</Data></Cell>
    <Cell><Data ss:Type="String">We address the problem of design and construction of shape models on manifolds considering different invariance properties in compositional hierarchies when data spaces of measurements and shape spaces are not topological manifolds. We resolve the aforementioned problem, called Multi-component shape representation, by first reformulating the relationship between data and shape spaces considering the interaction between Receptive Fields (RFs) and Shape Manifolds (SMs) in a compositional hierarchical shape vocabulary. Then, we suggest a method to model topological structure of the SMs using statistical properties of the data. For this purpose, we design a disjoint union topology using an indexing mechanism for the formation of shape models on SMs in the vocabulary, recursively. We represent topological relationship between shape components using graphs, which are aggregated to construct a hierarchical graph structure of the shape vocabulary. The learned shape vocabulary induces an inference tree which is employed for inference of shape components, and recognition of shapes. To this end, we introduce a framework to implement the indexing mechanisms for the employment of the vocabulary for structural shape classification. The proposed approach is used to construct different shape representations with different invariance properties such as scale, translation and/or rotation invariance. Results on benchmark shape classification outperform state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Mete Ozay*, University of Birmingham; Umit Aktas, University of Birmingham; Jeremy Wyatt, University of Birmingham; Ales Leonardis, &quot;University of Birmingham, UK&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1192</Data></Cell>
    <Cell><Data ss:Type="String">Procedural Editing of Building Point Clouds</Data></Cell>
    <Cell><Data ss:Type="String">Thanks to the recent advances in computational photography and remote sensing, point clouds of buildings are becoming increasingly available, yet their processing poses various challenges. In our work, we tackle the problem of point cloud completion and editing and we approach it via inverse procedural modeling. Contrary to the previous work, our approach operates directly on the point cloud without an intermediate triangulation. Our approach consists of 1) semi-automatic segmentation of the input point cloud with segment comparison and template matching to detect repeating structures, 2) a consensus-based voting schema and a pattern extraction algorithm to discover completed terminal geometry and their patterns of usage, all encoded into a context-free grammar, and 3) an interactive editing tool where the user can create new point clouds by using procedural copy and paste operations, and smart resizing. We demonstrate our approach on editing of building models with up to 1.8M points. In our implementation, preprocessing takes up to several minutes and a single editing  operation needs from one second to one minute depending on model size and the operation type.</Data></Cell>
    <Cell><Data ss:Type="String">Ilke Demir*, Purdue University; Daniel Aliaga, Purdue University; Bedrich Benes, Purdue University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1197</Data></Cell>
    <Cell><Data ss:Type="String">Contour Detection and Characterization for Asynchronous Event Sensors</Data></Cell>
    <Cell><Data ss:Type="String">The bio-inspired, asynchronous event-based dynamic vision sensor records temporal changes in the luminance of the scene at high temporal resolution. Since events are only triggered at significant luminance changes, most events occur at the boundary of objects and their parts. The detection of these contours is an essential step for further interpretation of the scene. This paper presents a method that uses Structured Random Forests to learn the location of contours and their border ownership from event-based features which encode motion, timing, texture, and spatial orientations. The classifier integrates elegantly information continuously by utilizing the classification results previously computed. Finally, the contour detection and boundary assignment are demonstrated in a layer-segmentation of the  scene. Experimental results demonstrate good performance in boundary detection and segmentation.</Data></Cell>
    <Cell><Data ss:Type="String">Francisco Barranco*, University of Maryland; Ching Teo, University of Maryland; Cornelia Fermuller, Univ. Maryland USA; Yiannis Aloimonos, UMD</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1199</Data></Cell>
    <Cell><Data ss:Type="String">Improving ferns ensembles by pruning leaves and quantising posterior probabilities</Data></Cell>
    <Cell><Data ss:Type="String">Ferns ensembles offer an accurate and efficient multiclass non-linear classification, commonly at the expense of consuming a large amount of memory. We introduce a two-fold contribution that produces large reductions in their memory consumption. First, an efficient L0 regularised cost optimisation finds a sparse representation of the leaves in the ensemble by discarding elements with zero contribution to valid responses in the training samples. As a by-product this can produce a prediction accuracy gain that, if required, can be traded for further reductions in memory size and prediction time. Secondly, elements in the leaves are quantised and stored in a memory-friendly sparse data structure. We reported a minimum of 75% memory reduction for different types of classification problems using generative and discriminative ferns ensembles, without increasing prediction time or classification error. For image patch recognition our proposal produced a 90% memory reduction, and improved in several percentage points the prediction accuracy.  </Data></Cell>
    <Cell><Data ss:Type="String">Antonio Rodriguez*, Institute for Transuranium Elements (ITU) - Joint Research Centre (JRC); Vitor Sequeira, Institute for Transuranium Elements (ITU) - Joint Research Centre (JRC)</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1202</Data></Cell>
    <Cell><Data ss:Type="String">Storyline Representation of Egocentric Videos with an Applications to Story-based Search</Data></Cell>
    <Cell><Data ss:Type="String">Egocentric videos are a valuable source of information as a daily log of our lives. However, much content of egocentric videos is irrelevant and boring to re-watch. It is an antagonizing task, for example, to manually search for the moment when your daughter first met Mickey Mouse from hours-long egocentric videos taken at Disneyland. Although many summarization methods have been successfully proposed to create concise representations of videos, in practice, the value of the subshots to users may change according to different aspects of semantic meaning, and thus summaries with fixed criteria may not fully satisfy users’ various search intents. To address this, we propose a storyline representation that epitomizes an egocentric video as a set of jointly inferred, through MRF inference, story elements comprising of actors, locations, supporting objects and events, depicted on a timeline. We construct such a storyline with very limited annotation data (i.e . a list of map locations and weak knowledge of what events may be possible at each location), by bootstrapping the process with data obtained through focused Web image and video searches. Our representation promotes story-based search with queries in the form of AND-OR graphs, which span any subset of story elements and their spatio-temporal composition. We show effectiveness of our approach on a set of unconstrained YouTube egocentric videos of Disneyland.</Data></Cell>
    <Cell><Data ss:Type="String">Bo Xiong*, University of Texas at Austin; Leonid Sigal, &quot;Disney Research, Piitsburgh, USA&quot;; Gunhee Kim, Carnegie Mellon University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1204</Data></Cell>
    <Cell><Data ss:Type="String">Joint Facial Action Unit Detection with Multi-conditional Latent Variable Models</Data></Cell>
    <Cell><Data ss:Type="String">We propose a novel multi-conditional latent variable model  (MC-LVM) for simultaneous facial feature fusion and detection of facial Action Units (AU).  The proposed MC-LVM exploits the structure-discovery capabilities of generative models such as  Gaussian Processes, and the discriminative power of classifiers such as logistic regression. This leads to superior performance of the proposed model compared to existing classifiers for the target task that exploit either the discriminative or generative property, but not both.  The model learning is performed via efficient newly proposed Bayesian learning approach based on Monte Carlo (MC) sampling. Consequently, the proposed model is robust to data overfitting, regardless of both the number of input features and jointly estimated AUs. The proposed approach is validated on posed and spontaneous facial expressions from three publicly available datasets (CK+, Shoulder-pain and DISFA). We show that the proposed MC-LVM outperforms the state-of-the-art methods for the target task on (i) facial feature fusion, and (ii) multiple facial AU detection.</Data></Cell>
    <Cell><Data ss:Type="String">Stefanos Eleftheriadis*, Imperial College London; Ognjen Rudovic, Imperial College London; Maja Pantic, &quot;Imperial College London, UK&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1206</Data></Cell>
    <Cell><Data ss:Type="String">Learning by Playing: Learning Common Sense Via Visual Abstractions</Data></Cell>
    <Cell><Data ss:Type="String">Commonsense is essential for building intelligent machines. While some commonsense knowledge is explicitly stated in human-generated text and can be learnt by mining the web, much of this knowledge is unwritten. After all, it is often unnecessary and even unnatural to write about commonsense facts. While unwritten, this commonsense knowledge is not unseen! Our visual world is full of structure. Can machines learn common sense simply by observing our visual world? Unfortunately, this requires automatic and accurate detection of objects, their attributes, poses, and an understanding of the intricate ways in which objects interact with each other. Recognizing such deep automatic understanding of images is still an unsolved problem. Our key insight is that while visual commonsense is depicted in visual content, it is not the pixels in the visual content that are relevant. What is relevant are the semantic features of the scene, i.e. photorealism is not necessary to learn commonsense. We explore the use of human-generated abstract scenes made from clip art for learning. In particular, we reason about the plausibility of an interaction or relation between a pair of nouns by measuring the similarity of the relation and nouns with other relations and nouns we have seen in abstract scenes. We show that the commonsense knowledge we learn is complementary to what can be learnt from text sources.</Data></Cell>
    <Cell><Data ss:Type="String">Shanmukha Ramakrishn Vedantam*, Virginia Tech; Xiao Lin, Virginia Tech; Tanmay Batra, Virginia Tech; Larry Zitnick, Microsoft Research Redmond, USA; Devi Parikh, Virginia Tech, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1208</Data></Cell>
    <Cell><Data ss:Type="String">Semantically-Aware Aerial Reconstruction from Multi-Modal Data</Data></Cell>
    <Cell><Data ss:Type="String">Humans integrate a variety of sensory and semantic cues to reason about their environment. Here we consider a methodology for integrating multipe sensors along with semantic information for the purpose of scene representation. We propose a probabilistic generative model for inferring  semantically consistent aerial reconstructions from multimodal data within a consistent mathematical model. The approach, termed Semantically-Aware Aerial Reconstruction (SAAR), exploits not only inferred scene geometry and appearance to obtain a semantic annotation of the data but  extends previously proposed methods by imposing structure on the prior over geometry, appearance and semantic labels. The leads to more accurate reconstructions and the ability to fill in missing contextual labels via joint sensor and semantic information. We first introduce a new a new  multi-modal synthetic dataset in order to demonstrate the utility of the approach and to provide quantitative performance analysis. Additionally, we apply the model to realworld  data and exploit OpenStreetMap data as a source of semantic observations. We demonstrate qualitative as well as quantitative improvements in large-scale reconstructions of urban scenes from the combination of LiDAR, aerial photography and OpenStreetMap data. Furthermore, we  demonstrate the ability to leverage semantic data to fill in for missing sensed data in urban scenes, leading to more interpretable reconstructions.</Data></Cell>
    <Cell><Data ss:Type="String">Randi Cabezas*, MIT; Julian Straub, MIT; John Fisher III, MIT</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1210</Data></Cell>
    <Cell><Data ss:Type="String">Predicting Good Features for Image Geo-Localization Using Per-Bundle VLAD</Data></Cell>
    <Cell><Data ss:Type="String">We address the problem of recognizing a place depicted in a query image by using a large database of geo-tagged images at a city-scale. In particular, we discover features that are useful for recognizing a place in a data-driven manner, and use this knowledge to predict useful features in a query image prior to the geo-localization process. This allows us to achieve better performance while reducing the number of features. Also, for both learning to predict features and retrieving geo-tagged images from the database, we propose per-bundle vector of locally aggregated descriptors (PBVLAD), where each maximally stable region is described by a vector of locally aggregated descriptors (VLAD) on multiple scale-invariant features detected within the region. Experimental results show the proposed approach achieves a significant improvement over other baseline methods.</Data></Cell>
    <Cell><Data ss:Type="String">Hyo Jin Kim*, UNC Chapel Hill; Enrique Dunn, UNC Chapel Hill; Jan-Michael Frahm, UNC Chapel Hill</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1211</Data></Cell>
    <Cell><Data ss:Type="String">Sequence to Sequence  Video to Text</Data></Cell>
    <Cell><Data ss:Type="String">Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).</Data></Cell>
    <Cell><Data ss:Type="String">Subhashini Venugopalan*, University of Texas at Austin; Marcus Rohrbach, UC Berkeley; Raymond Mooney, ; Trevor Darrell, &quot;UC Berkeley, USA&quot;; Kate Saenko, &quot;University of Massachusetts Lowel, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1212</Data></Cell>
    <Cell><Data ss:Type="String">Appearance Mosaics from Internet Photo-Collections</Data></Cell>
    <Cell><Data ss:Type="String">We propose a framework for the automatic creation of  appearance mosaics of a given scene. We achieve this  by leveraging the appearance variation captured in Internet  photo-collections. In order to depict and characterize  the appearance spectrum of a scene, our method relies on  building discrete representations of the image appearance  space through connectivity graphs defined over a pairwise  image distance function. The smooth appearance transitions  are found as the shortest path in the similarity graph  among images, and robust image alignment is achieved by  leveraging scene semantics, multi-view geometry, and image  warping techniques. The attained results present an insightful  and compact visualization of the scene appearance  variability captured in crowd-sourced imagery.</Data></Cell>
    <Cell><Data ss:Type="String">Dinghuang Ji*, Unc; Enrique Dunn, UNC Chapel Hill; Jan-Michael Frahm, UNC Chapel Hill</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1231</Data></Cell>
    <Cell><Data ss:Type="String">Learning informative edge maps for indoor scene layout prediction</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we introduce new edge-based features for the task of recovering the 3D layout of an indoor scene from a single image. Indoor scenes have certain edges that are very informative about the spatial layout of the room. Specifically, these are the edges formed by the intersection of different room faces (two walls, wall and ceiling, wall and floor). Unlike previous approaches that rely on area-based features like geometric context and orientation maps, our method directly targets the detection of these informative edges. We learn to predict `informative-edge' probability maps using two recently proposed methods that exploit local and global context, respectively: structured edge detection forests, and a fully convolutional network for pixelwise labeling. We show that the fully convolutional network is quite successful at predicting the informative edges even when they lack contrast or are occluded, and the accuracy can be further improved by training the network to jointly predict the edges and the geometric context. Incorporating features computed from the `informative-edge' maps, we learn a maximum-margin structured classifier that achieves state-of-the-art performance on layout prediction.</Data></Cell>
    <Cell><Data ss:Type="String">Arun Mallya*, UIUC; Svetlana Lazebnik, UIUC</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1237</Data></Cell>
    <Cell><Data ss:Type="String">Recurrent Network Models for Kinematic Tracking</Data></Cell>
    <Cell><Data ss:Type="String">We propose the Encoder-Recurrent-Decoder (ERD)  model for recognition and prediction of human body pose in  videos and motion capture. The ERD model is a type of re-  current neural network that incorporates nonlinear encoder  and decoder networks before and after the recurrent lay-  ers. We simultaneously learn the representation suitable for  recognition and prediction, as well as its dynamics, We test  instantiations of ERD architectures in the tasks of motion  capture (mocap) generation, body pose labeling and body  pose prediction in videos. Our model handles mocap train-  ing data across multiple subjects and activity domains, and  synthesizes novel motions with smooth cross-action transi-  tions. For human pose tracking, ERD outperforms a per  frame body part detector by resolving left-right body part  confusions. For video pose prediction, ERD effectively fore-  casts body joint displacements across a temporal horizon of  400ms and outperforms a first order motion model based on  optical flow. ERDs extend previous RNNs in the literature  to jointly learn representations and their dynamics. Our ex-  periments show such representation learning is crucial for  both labeling and prediction in the spatio-temporal visual  domain. We find this is a distinguishing feature of the 2D  spatio-temporal visual domain in comparison to 1D text,  speech or handwriting, where straightforward hard coded  representations have shown excellent results when directly  combined with RNNs, without any sophisticated represen-  tation learning.</Data></Cell>
    <Cell><Data ss:Type="String">Katerina Fragkiadaki*, UC Berkeley; Panna Felsen, ; Sergey Levine, ; Jitendra Malik, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1251</Data></Cell>
    <Cell><Data ss:Type="String">Contour Flow:  Middle-Level Motion Estimation by Combining Motion Segmentation and Contour Alignment</Data></Cell>
    <Cell><Data ss:Type="String">Our goal is to estimate contour flow (the contour pairs with consistent point correspondence) from inconsistent contours extracted independently in two video frames. Inspired by the impressive progress of optical flow, we formulate the contour flow estimation locally as a motion segmentation problem where motion patterns grouped from optical flow field are exploited for local correspondence measurement. To solve local ambiguities, contour flow estimation is further formulated globally as a contour alignment problem. We propose a novel two-staged strategy to obtain global consistent point correspondence under various contour transitions such as splitting, merging and branching. The goal of the first stage is to obtain possible accurate contour-to-contour alignments, and the second stage aims to make a consistent fusion of many partially conflicted alignments. Such a strategy can properly balance the accuracy and the consistency, which enables a middle-level motion representation to be constructed by just concatenating frame-by-frame contour flow estimation. Experiments prove the effectiveness of our method.</Data></Cell>
    <Cell><Data ss:Type="String">Huijun Di*, Beijing Institute of Technolog; Qingxuan Shi, Beijing Institute of Technology; Feng Lv, Beijing Institute of Technology; Ming Qin, Beijing Institute of Technology; Yao Lu, Beijing Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1264</Data></Cell>
    <Cell><Data ss:Type="String">Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs</Data></Cell>
    <Cell><Data ss:Type="String">State-of-the-art image-set matching techniques typically implicitly model each image-set with a Gaussian distribution.  Here, we propose to go beyond these representations and model image-sets as probability distribution functions (PDFs) using kernel density estimators. To compare and match image-sets, we exploit Csiszar f-divergences, which bear strong connections to the geodesic distance defined on the space of PDFs, i.e., the statistical manifold. Furthermore, we introduce valid positive definite kernels on the statistical manifolds, which let us make use of more powerful classification schemes to match image-sets.   Finally, we introduce a supervised dimensionality reduction technique that learns a latent space where f-divergences reflect the class labels of the data. Our experiments on diverse problems, such as video-based face recognition and dynamic texture classification, evidence the benefits of our approach over the state-of-the-art image-set matching methods.</Data></Cell>
    <Cell><Data ss:Type="String">Mehrtash Harandi*, Australian National University; Mathieu Salzmann, NICTA; Mahsa Baktashmotlagh, QUT</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1268</Data></Cell>
    <Cell><Data ss:Type="String">Actionness-assisted Recognition of Actions</Data></Cell>
    <Cell><Data ss:Type="String">We elicit from a fundamental definition of action low-level attributes that can reveal agency and intentionality. These descriptors are mainly trajectory-based, measuring sudden changes, temporal synchrony, and repetitiveness. The actionness map can be used to localize actions in a way that is generic across action and agent types. Furthermore, it also groups interacting regions into a useful unit of analysis, which is crucial for recognition of actions involving interactions. We then implement an actionness-driven pooling scheme to improve action recognition performance. Experimental results on three datasets show the advantages of our method on both action detection and action recognition comparing with other state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Ye Luo, ; Loong-Fah Cheong*, NUS; An Tran, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1275</Data></Cell>
    <Cell><Data ss:Type="String">Leveraging Datasets with Varying Annotations for Face Alignment via Deep Regression Network</Data></Cell>
    <Cell><Data ss:Type="String">Facial landmark detection, as a vital topic in computer vision, has been studied for many decades and lots of datasets have been collected for evaluation. These datasets usually have different annotations, e.g., 68-landmark markup for LFPW dataset, while 74-landmark markup for GTAV dataset. Intuitively, it is meaningful to fuse all the datasets to predict a union of all types of landmarks from multiple datasets (i.e., transfer the annotations of each dataset to all other datasets), but this problem is nontrivial due to the distribution discrepancy between datasets and incomplete annotations of all types for each dataset. In this work, we propose a deep regression network coupled with sparse shape regression (DRN-SSR) to predict the union of all types of landmarks by leveraging datasets with varying annotations, each dataset with one type of annotation. Specifically, the deep regression network intends to predict the union of all landmarks, and the sparse shape regression attempts to approximate those undefined landmarks on each dataset so as to guide the learning of the deep regression network for face alignment. Extensive experiments on two challenging datasets, IBUG and GLF, demonstrate that our method can effectively leverage the multiple datasets with different annotations to predict the union of all types of landmarks.</Data></Cell>
    <Cell><Data ss:Type="String">Jie Zhang*, ICT,CAS; Meina Kan, Chinese Academy of Sciences; Shiguang Shan, Chinese Academy of Sciences; Xilin Chen, &quot;Institute of Computing Technology, Chinese Academy of Sciences&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1276</Data></Cell>
    <Cell><Data ss:Type="String">A SPATIO-TEMPORAL APPEARANCE REPRESENTATION FOR VIDEO-BASED PEDESTRIAN RE-IDENTIFICATION</Data></Cell>
    <Cell><Data ss:Type="String">Pedestrian re-identification is a difficult problem due to the large variations in a person's appearance caused by different poses and viewpoints, illumination changes, and occlusions.  Spatial alignment is a common strategy achieved by treating the appearance of different body parts independently.  However, a body part can also appear differently during different phases of an action.  In this paper we consider the temporal alignment problem, in addition to the spatial one, and propose a new approach that takes the video of a walking person as input and builds a spatio-temporal appearance representation for pedestrian re-identification.  Particularly, given a video sequence we exploit the periodicity exhibited by a walking person to generate a spatio-temporal body-action model, which consists of a series of body-action units corresponding to certain action primitives of certain body parts.  Fisher vectors are learned and extracted from individual body-action units and concatenated into the final representation of the walking person.  Unlike previous spatio-temporal features that only take into account local dynamic appearance information, our representation aligns the spatio-temporal appearance of a pedestrian globally.  Extensive experiments on public datasets show the effectiveness of our approach compared with the state of the art.</Data></Cell>
    <Cell><Data ss:Type="String">Kan Liu, Shandong University; Bingpeng Ma, University of Chinese Academy of Sciences; Wei Zhang, Shandong University; Rui Huang*, NEC Labs China</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1280</Data></Cell>
    <Cell><Data ss:Type="String">Task-Driven Feature Pooling for Image Classification</Data></Cell>
    <Cell><Data ss:Type="String">Feature pooling is an important strategy to achieve high performance in image classification. However, most pooling methods are unsupervised and heuristic. In this paper, we propose a novel  task-driven pooling (TDP) model to directly learn the pooled representation from data in a discriminative manner. Different from the traditional methods (e.g., average and max pooling), TDP is an implicit pooling method which elegantly integrates the learning of representations into the given classification task. The optimization of TDP can equalize the similarities between the descriptors and the learned representation, and maximize the classification accuracy. TDP can be combined with the traditional BoW models (coding vectors) or the recent state-of-the-art CNN models (feature maps) to achieve a much better pooled representation. Furthermore, a self-training mechanism is used to generate the TDP representation for a new test image. A multi-task extension of TDP is also proposed to further improve the performance. Experiments on three databases (Flower-17, Indoor-67 and Caltech-101) well validate the effectiveness of our models.</Data></Cell>
    <Cell><Data ss:Type="String">Guo-Sen Xie*, NLPR,CASIA ; Xu-Yao Zhang, NLPR CASIA; Xiangbo Shu, Nanjing University of Science and Technology ; Shuicheng Yan, National University of Singapore; Cheng-Lin Liu,  National Laboratory of Pattern Recognition, Institute of Automation</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1285</Data></Cell>
    <Cell><Data ss:Type="String">Class-Specific Image Deblurring</Data></Cell>
    <Cell><Data ss:Type="String">In image deblurring, a fundamental problem is that the  blur kernel suppresses a number of spatial frequencies that  are difficult to recover reliably. In this paper, we explore  the potential of a class-specific image prior for recovering  spatial frequencies suppressed by the blurring process.  Specifically, we devise a prior based on the class-specific  subspace of image intensity responses to band-pass filters.  We learn that the aggregation of these subspaces across all  frequency bands serves as a good class-specific prior for  the restoration of suppressed frequencies that cannot be recovered with generic image priors. In an extensive validation,  our method, equipped with the above prior, yields  greater image quality than many state-of-the-art methods  by up to 5 dB in terms of image PSNR, across various image  categories including portraits, cars, cats, pedestrians  and household objects.</Data></Cell>
    <Cell><Data ss:Type="String">Saeed Anwar, ANU-NICTA; Cong Phuoc Huynh, National ICT Australia; Fatih Porikli*, ANU / NICTA, Australia</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1287</Data></Cell>
    <Cell><Data ss:Type="String">Two Birds, One Stone: Jointly Learning Binary Code for Large-scale Face Image Retrieval and Attributes Prediction</Data></Cell>
    <Cell><Data ss:Type="String">We address the challenging large-scale content-based face image retrieval problem, intended as searching images based on the presence of specific subject, given one face image of him/her. To this end, one natural demand is a supervised discriminative binary code learning method. While the learned codes might be sufficiently discriminating, people often have a further expectation that it would be wonderful if some semantic message (e.g., visual attributes) can be read from the human-incomprehensible binary codes. For this purpose, we propose to jointly encode a number of facial attributes into the binary code by incorporating the code-attribute correlation into the code learning process, and thus derive a novel binary code learning framework with three crucial constraints, i.e., identity discriminability, code-attribute consistency, and code prediction stability. In this way, the learned binary codes can be used to not only conduct fine-grained face image retrieval, but also predict the presence (absence) of facial attributes, which is the very innovation of this work, just like killing two birds with one stone. To evaluate the effectiveness of the proposed method, extensive experiments are conducted on a new purified large-scale web celebrity database, i.e., CFW 60K, with abundant manual annotation of identity and facial attributes, and experimental results exhibit the superiority of our method over state-of-the-art.</Data></Cell>
    <Cell><Data ss:Type="String">Yan Li*, ICT, CAS; Ruiping Wang, Institute of Computing Technology, Chinese Academy of Sciences; Haomiao Liu, ICT; Huajie Jiang, ICT; Shiguang Shan, Chinese Academy of Sciences; Xilin Chen, &quot;Institute of Computing Technology, Chinese Academy of Sciences&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1288</Data></Cell>
    <Cell><Data ss:Type="String">Cutting Edge: Soft Correspondences in Multimodal Scene Parsing</Data></Cell>
    <Cell><Data ss:Type="String">Exploiting multiple modalities for semantic scene parsing has been shown to improve accuracy over the single-modality scenario. Existing methods, however, assume that corresponding regions in two modalities have the same label. In this paper, we address the problem of data misalignment and label inconsistencies, e.g., due to moving objects, in semantic labeling, which violate the assumption of existing techniques. To this end, we formulate multimodal semantic labeling as inference in a CRF, and introduce latent nodes to explicitly model inconsistencies between two domains. These latent nodes can not only leverage information from both domains to improve their labeling, but also cut the edges between inconsistent regions. To eliminate the need for hand tuning the parameters of our model, we propose to learn intra-domain and inter-domain potential functions from training data. We demonstrate the benefits of our approach on two publicly available datasets containing 2D imagery and 3D point clouds. Thanks to our latent nodes and our learning strategy, our method outperforms the state-of-the-art in both cases.  </Data></Cell>
    <Cell><Data ss:Type="String">Sarah Taghavi Namin*, ANU/NICTA; Mohammad Najafi, ANU/NICTA; Mathieu Salzmann, NICTA; Lars Petersson, NICTA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1294</Data></Cell>
    <Cell><Data ss:Type="String">Domain Generalization for Object Recognition with Multi-task Autoencoders</Data></Cell>
    <Cell><Data ss:Type="String">The problem of domain generalization is to take knowledge acquired from a number of related domains, where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition.    The algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects.   Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier.     We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization.</Data></Cell>
    <Cell><Data ss:Type="String">Muhammad Ghifary*, VUW; W. Bastiaan Kleijn, Victoria University of Wellington; Mengjie Zhang, Victoria University of Wellington; David Balduzzi, Victoria University of Wellington</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1295</Data></Cell>
    <Cell><Data ss:Type="String">COUNT Forest: CO-voting Uncertain Number of Targets using Random Forest for Crowd Density Estimation</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a patch-based approach for crowd density estimation in public scenes. We formulate the problem of estimating density in a structured learning framework applied to random decision forests. Our approach learns the mapping between patch features and relative locations of all objects inside each patch, which contribute to generate the patch density map through Gaussian kernel density estimation. We build the forest in a coarse-to-fine manner with two split node layers, and further propose a crowdedness prior and an effective forest reduction method to improve the estimation accuracy and speed. Moreover, we introduce a semi-automatic training method to learn the estimator for a specific scene. We achieved state-of-the-art results on the public Mall dataset and UCSD dataset, and also proposed two potential applications in traffic counts and scene understanding with promising results.</Data></Cell>
    <Cell><Data ss:Type="String">Viet Pham*, Toshiba Corporation; Osamu Yamaguchi, Toshiba Corporation; Tatsuo Kozakaya, Toshiba Corporation; Ryuzo Okada, Toshiba Corporation</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1298</Data></Cell>
    <Cell><Data ss:Type="String">Guaranteed Outlier Removal for Rotation Search</Data></Cell>
    <Cell><Data ss:Type="String">Rotation search has become a core routine for solving many computer vision problems. The aim is to rotationally align two input point sets with correspondences. Recently, there is significant interest in developing globally optimal rotation search algorithms. A notable weakness of global algorithms, however, is their relatively high computational cost, especially on large problem sizes and data with a high proportion of outliers. In this paper, we propose a novel outlier removal technique for rotation search. Our method guarantees that any correspondence it discards as an outlier does not exist in the inlier set of the globally optimal rotation for the original data. Based on simple geometric operations, our algorithm is deterministic and fast. Used as a preprocessor to prune a large portion of the outliers from the input data, our method enables substantial speed-up of rotation search algorithms without compromising global optimality. We demonstrate the efficacy of our method in various synthetic and real data experiments.  </Data></Cell>
    <Cell><Data ss:Type="String">Alvaro Parra*, The University of Adelaide; Tat-Jun Chin, University of Adelaide</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1301</Data></Cell>
    <Cell><Data ss:Type="String">One Shot Learning via Compositions of Meaningful Patches</Data></Cell>
    <Cell><Data ss:Type="String">The task of discriminating one object from another is almost trivial for a human being. However, this task is computationally taxing for most modern machine learning methods; whereas, we perform this task at ease given very few examples for learning. It has been proposed that the quick grasp of concept may come from the shared knowledge between the new example and examples previously learned. We believe that the key to one-shot learning is the sharing of common parts as each part holds immense amounts of information on how a visual concept is constructed. We propose an unsupervised method for learning a compact dictionary of image patches representing meaningful components of an objects. Using those patches as features, we build a compositional model that outperforms a number of popular algorithms on a one-shot learning task. We demonstrate the effectiveness of this approach on hand-written digits and show that this model generalizes to multiple datasets.  </Data></Cell>
    <Cell><Data ss:Type="String">Alex Wong*, UCLA; Alan Yuille, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1306</Data></Cell>
    <Cell><Data ss:Type="String">High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and its Applications to High-Level Vision</Data></Cell>
    <Cell><Data ss:Type="String">Most of the current boundary detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method can be viewed as a &quot;High-for-Low&quot; approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run.    Additionally, we show that due to the semantic nature of our boundaries we can use them to aid a number of high-level vision tasks. We demonstrate that using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can view this process as a &quot;Low-for-High'&quot; scheme, where low-level boundaries aid high-level vision tasks.     Thus, our contributions include a boundary detection system that is accurate, efficient, generalizes well to multiple datasets, and is also shown to improve existing state-of-the-art high-level vision methods on three distinct tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Gedas Bertasius*, University of Pennsylvania; Jianbo  Shi, &quot;University of Pennsylvania, USA&quot;; Lorenzo Torresani, &quot;Dartmouth College, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1307</Data></Cell>
    <Cell><Data ss:Type="String">Discrete Tabu Search for Graph Matching</Data></Cell>
    <Cell><Data ss:Type="String">Graph matching is a fundamental problem in computer vision. In this paper, we propose a novel graph matching algorithm based on tabu search. The proposed method solves graph matching problem by casting it into an equivalent weighted maximum clique problem of the corresponding association graph, and then uses tabu search technique for the optimization. The distinct feature of tabu search optimization is that it utilizes the history of search to make more strategic decisions while looking for the optimal solution, thus effectively escaping local optima and in practice achieving superior results. The proposed method, unlike the existing algorithms, enables direct optimization in the original discrete space while encouraging rather than artificially enforcing hard one-to-one constraint, thus resulting in better solution. The experiments illustrate the robustness of the algorithm in a variety of settings, presenting the state-of-the-art results.</Data></Cell>
    <Cell><Data ss:Type="String">Kamil Adamczewski, Seoul National University; Yumin Suh, ; Kyoung Mu Lee*, Seoul National University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1312</Data></Cell>
    <Cell><Data ss:Type="String">Hyperspectral Compressive Sensing Using Manifold-Structured Sparsity Prior</Data></Cell>
    <Cell><Data ss:Type="String">To reconstruct hyperspectral image (HSI) accurately from a few noisy compressive measurements, we present a novel manifold-structured sparsity prior based hyperspectral compressive sensing (HCS) method in this study. A matrix based hierarchical prior is first proposed to represent the spectral structured sparisty and spatial unknown manifold structure of HSI simultaneously. Then, a latent variable Bayes model is proposed to incorporate signal recovery, sparsity prior learning and noise estimation into a unified framework for optimization. The learned prior can fully represent the inherent 3D structure of HSI by considering the correlation among the coefficients of HSI. Additionally, through regulating its shape based on the estimated noise level, the learned prior shows strong robustness to the unknown noise in HCS. With this learned prior, the proposed method improves the reconstruction accuracy significantly. Experiments on four real hyperspectral datasets show that the proposed method outperforms 6 state-of-the-art methods on the reconstruction accuracy of HSI.</Data></Cell>
    <Cell><Data ss:Type="String">Lei Zhang*, School of Computer Science, Northwestern Polytechnical University; Wei Wei, School of Computer Science, Northwestern Polytechnical University; Yanning  Zhang, Northwestern Polytechnical University; Qinfeng Shi, The University of Adelaide; Chunhua Shen, University of Adelaide; Fei Li, School of Computer Science, Northwestern Polytechnical University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1315</Data></Cell>
    <Cell><Data ss:Type="String">Efficient Square Localization for Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">This paper enables the sliding window method to run efficiently in object detection/recognition.  The key contribution is the compact square object localization, which relaxes the sliding window from testing windows of different combinations of aspect ratios. Square object localization is category scalable. By using a binary search strategy, the number of scales to test is further reduced to empirically only $O(\log(\min\{H, W\}))$ rounds of sliding CNNs are run, where $H$ and $W$ are respectively the image height and width.  In the training phase, square CNN models  and object co-presence priors are learned. In the testing phase,  sliding CNN models are applied which produces a set of response maps that can be effectively filtered by the learned co-presence prior to output the final bounding boxes for localizing an object. We performed extensive experimental evaluation on the VOC 2007 and 2012 datasets, focusing our evaluation on demonstrating how our localization can effectively improve the final detection result.</Data></Cell>
    <Cell><Data ss:Type="String">Cewu Lu*, HKUST; Yongyi Lu, HKUST; Hao Chen, The Chinese University of HK; ChiKeung Tang, &quot;The Hong Kong University of  Science and Technology, Hong Kong&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1326</Data></Cell>
    <Cell><Data ss:Type="String">An Accurate Iris Segmentation Framework under Relaxed Imaging Constraints using Total Variation Model</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.</Data></Cell>
    <Cell><Data ss:Type="String">Zijing Zhao*, The Hong Kong Polytechnic Univ; Kumar Ajay, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1331</Data></Cell>
    <Cell><Data ss:Type="String">Learning to Boost Filamentary Structure Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">The challenging problem of filamentary structure segmentation has a broad range of applications in biological and medical fields. A critical issue remains on how to identify and reclaim the small filamentary fragments from backgrounds: The small fragments are of diverse shapes and appearances, while the background are usually cluttered and ambiguous. Focusing on this issue, this paper proposes a two-step learning-based approach in order to boost the performance of an existing segmenter: We start with a partial segmentation where the filamentary structure obtained so far are of high confidence based on an existing segmenter. Step one of our approach involves a data-driven latent tree model to identify the filamentary fragments. This model is learned during training, where a large number of distinct local figure/background separation scenarios are established and geometrically organized into a tree structure.  Step two spatially connects the isolated fragments back to the current partial segmentation via their individual completion fields. Our approach is rather generic and can be easily augmented to a wide range of existing supervised/unsupervised segmenters to produce an improved result. This has been empirically verified on three specific filamentary structure segmentation tasks: retinal blood vessel segmentation as well as 2D and 3D neuronal segmentations, where noticeable improvement has been shown over the original state-of-the-arts.</Data></Cell>
    <Cell><Data ss:Type="String">Lin Gu*, BII, A*STAR; LI Cheng, BII, A*STAR</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1332</Data></Cell>
    <Cell><Data ss:Type="String">Box Aggregation for Proposal Decimation: Last Mile of Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">Regions-with-convolutional-neural-network (RCNN) is now a commonly employed object detection pipeline. Its main steps, proposal generation and convolutional neural network (CNN) feature extraction, have been intensively investigated. We focus on the last step of the system to aggregate thousands of scored box proposals into final object predictions, which we call proposal decimation. We show this step can be enhanced with a novel box aggregation function by considering statistical properties of proposals with respect to ground truth objects. Our method is with extremely light-weight computation, while it yields an improvement of 3.7% in mAP on PASCAL VOC 2007 test. We explain why it works using many statistics in this paper.</Data></Cell>
    <Cell><Data ss:Type="String">Shu Liu*, Chinese University of Hong Kon; Cewu Lu, HKUST; Jiaya Jia, Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1335</Data></Cell>
    <Cell><Data ss:Type="String">Deep Convolutional Feature Point Descriptors</Data></Cell>
    <Cell><Data ss:Type="String">Deep learning has become the dominant paradigm for image-level recognition. Yet, many computer vision applications still rely on hand-crafted features, such as SIFT. In this paper we explore strategies to extract discriminant representations from image patches with Convolutional Neural Networks (CNN). We propose a framework for learning local descriptors with a Siamese architecture of two identical networks. Since this architecture uses pairs rather than single image patches to train, there exist a large number of positive samples and an exponential number of negative samples. We demonstrate that we can obtain powerful models exploring this space with the combination of a stochastic sampling approach of the training set, and an aggressive mining strategy biased towards those patches that are harder to classify. We thoroughly evaluate multiple architectures, configurations and hyper-parameters over different datasets. Our models are fully convolutional, can be computed densely in an efficient manner, and are amenable to modern GPUs. They provide consistent performance gains with respect to the state of the art on various tasks, and generalize well against scaling and rotation, perspective transformations, non-rigid deformations and illumination changes.</Data></Cell>
    <Cell><Data ss:Type="String">Edgar Simo-Serra*, Waseda University; Eduard Trulls, ; Luis Ferraz, ; Iasonas Kokkinos, Ecole Central de Paris; Pascal  Fua, &quot;EPFL, Switzerland&quot;; Francesc Moreno-Noguer, Institut de Robotica i Informatica Industrial (UPC/CSIC)</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1344</Data></Cell>
    <Cell><Data ss:Type="String">Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks</Data></Cell>
    <Cell><Data ss:Type="String">While feedforward deep convolutional neural networks (CNNs) have been a great success in computer vision, it is important to remember that the human visual contex contains generally more feedback connections than foward connections. In this paper, we will briefly introduce the background of feedbacks in the human visual cortex, which motivates us to develop a computational feedback mechanism in the deep neural networks. The proposed networks perform inference from image features in a bottom-up manner as traditional convolutional networks; while during feedback loops it sets up high-level semantic labels as the âgoalâ to infer the activation status of hidden layer neurons. The feedback networks help us better visualize and understand on how deep neural networks work as well as capture visual attention on expected objects, even in the images with cluttered background and multiple objects.</Data></Cell>
    <Cell><Data ss:Type="String">Chunshui Cao, ; Xianming Liu, UIUC; Jiang Wang, Baidu USA LLC; Yinan Yu, Baidu IDL; Wei Xu, Baidu IDL; Yi Yang*, Baidu IDL</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1358</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Domain Adaptation with Imbalanced Cross-Domain Data</Data></Cell>
    <Cell><Data ss:Type="String">We address a challenging unsupervised domain adaptation problem with imbalanced cross-domain data. For standard unsupervised domain adaptation, one typically obtains labeled data in the source domain and only observes unlabeled data in the target domain. However, most existing works do not consider the scenarios in which either the label numbers across domains are different, or the data in the source and/or target domains might be collected from multiple datasets. To address the aforementioned settings of imbalanced cross-domain data, we propose Closest Common Space Learning (CCSL) for associating such data with the capability of preserving label and structural information within and across domains. Experiments on multiple cross-domain visual classification tasks confirm that our method performs favorably against state-of-the-art approaches, especially when imbalanced cross-domain data are presented.</Data></Cell>
    <Cell><Data ss:Type="String">Tzu Ming Harry Hsu, National Taiwan University; Cheng-An Hou, Academia Sinica; Wei Yu Chen, National Taiwan University; Yi-Ren Yeh, Chinese Culture Unoversity; Yu-Chiang Frank Wang*, Academia Sinica</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1360</Data></Cell>
    <Cell><Data ss:Type="String">Variational Depth Superresolution using Example-Based Edge Representations</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we propose a novel method for depth image superresolution which combines recent advances in example based upsampling with variational superresolution based on a known blur kernel. Most traditional depth superresolution approaches try to use additional high resolution intensity images as guidance for superresolution. In our method we learn a dictionary of edge priors from an external database of high and low resolution examples. In a novel variational sparse coding approach this dictionary is used to infer strong edge priors. Additionally to the traditional sparse coding constraints the difference in the overlap of neighboring edge patches is minimized in our optimization. These edge priors are used in a novel variational superresolution as anisotropic guidance of the higher order regularization. Both the sparse coding and the variational superresolution of the depth are solved based on a primal-dual formulation. In an exhaustive numerical and visual evaluation we show that our method clearly outperforms existing approaches on multiple real and synthetic datasets.</Data></Cell>
    <Cell><Data ss:Type="String">David Ferstl*, Graz University of Technology; Gernot Riegler, Graz University of Technology; Matthias Rüther, Graz Univerity of Technology; Horst Bischof, Graz University of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1366</Data></Cell>
    <Cell><Data ss:Type="String">FASTex: Efficient Unconstrained Scene Text Detector</Data></Cell>
    <Cell><Data ss:Type="String">Observing that text in virtually any script is formed of strokes, we propose a novel easy-to-implement stroke detector which is significantly faster and produces significantly less false detections than the detectors commonly used in scene text localization.  First, stroke-specific keypoints are efficiently detected. Text fragments are subsequently extracted by local thresholding guided by keypoint properties. Classification based on effectively calculated features eliminates non-text segmentations.    The stroke-specific keypoints produce 2 times less segmentations and still detects 25% more characters than the commonly exploited MSER detector and the process is 4 times faster. After a novel efficient classification step, the number of segmentations is reduced to 7 times less than the standard method and is still almost 3 times faster.    All stages of the proposed pipeline are scale- and rotation-    invariant and support a wide variety of scripts (Latin, Hebrew, Chinese, etc.) and fonts. When the proposed detector is plugged into a scene text localization and recognition pipeline, a state-of-the-art text localization accuracy is maintained  whilst the processing time is significantly reduced. </Data></Cell>
    <Cell><Data ss:Type="String">Michal Busta, Czech Technical University; Lukas Neumann*, Czech Technical University; Jiri Matas, Czech Technical University in Prague</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1373</Data></Cell>
    <Cell><Data ss:Type="String">Conditioned Regression Models for Non-Blind Single Image Super-Resolution</Data></Cell>
    <Cell><Data ss:Type="String">Single image super-resolution is an important task in the field of computer vision and finds many practical applications. Current state-of-the-art methods typically rely on machine learning algorithms to infer a mapping from low- to high-resolution images. These methods use a single fixed blur kernel during training and, consequently, assume the exact same kernel underlying the image formation process for \emph{all} test images. However, this setting is not realistic for practical applications, because the blur is typically different for each test image. In this paper, we loosen this restrictive constraint and propose conditioned regression models (including convolutional neural networks and random forests) that can effectively exploit the additional kernel information during both, training and inference. This allows for training a single model, while previous methods need to be re-trained for every blur kernel individually to achieve good results, which we demonstrate in our evaluations. Moreover, our experiments also show that the proposed conditioned regression models (i) can effectively handle scenarios where the blur kernel is different for each image and (ii) outperform related approaches trained for only a single kernel. </Data></Cell>
    <Cell><Data ss:Type="String">Gernot Riegler, Graz University of Technology; Samuel  Schulter*, Graz University of Technology; Matthias Rüther, Graz Univerity of Technology; Horst Bischof, Graz University of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1379</Data></Cell>
    <Cell><Data ss:Type="String">A Gaussian Process Latent Variable Model for BRDF Inference</Data></Cell>
    <Cell><Data ss:Type="String">The problem of estimating a full BRDF from partial observations has already been studied using either parametric or non-parametric approaches. The goal in each case is to best match this sparse set of input measurements. In this paper we address the problem of inferring higher order reflectance information starting from the minimal input of a single BRDF slice. We begin from the prototypical case of a homogeneous sphere, lit by a head-on light source, which only holds information about less than $0.001\%$ of the whole BRDF domain. We propose a novel method to infer the higher dimensional properties of the material's BRDF, based on the statistical distribution of known material characteristics observed in real-life samples. We evaluated our method based on a large set of experiments generated from real-world BRDFs and newly measured materials. Although inferring higher dimensional BRDFs from such modest training is not a trivial problem, our method performs better than state-of-the-art parametric, semi-parametric and non-parametric approaches. Finally, we discuss interesting applications on material re-lighting, and flash-based photography.</Data></Cell>
    <Cell><Data ss:Type="String">Stamatios Georgoulis*, KU Leuven; Vincent Vanweddingen, KULeuven; Marc Proesmans, ; Luc Van Gool, KTH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1383</Data></Cell>
    <Cell><Data ss:Type="String">Convolutional Sparse Coding for Image Super-resolution</Data></Cell>
    <Cell><Data ss:Type="String">   Sparse coding (SC) plays an important role in versatile computer vision applications such as image super-resolution (SR). Most of the previous SC based SR methods partition the image into overlapped patches, and process each patch separately. These methods, however, ignore the consistency of pixels in overlapped patches, which is a strong constraint for image reconstruction. In this paper, we propose a convolutional sparse coding (CSC) based SR (CSC-SR) method to address the consistency issue. Our CSC-SR involves three groups of parameters to be learned: (i) a set of filters to decompose the low resolution (LR) image into LR sparse feature maps; (ii) a  mapping function to predict the high resolution (HR) feature maps from the LR ones; and (iii) a set of filters to reconstruct the HR images from the predicted HR feature maps via simple convolution operations. By working directly on the whole image, the proposed CSC-SR algorithm does not need to divide the image into overlapped patches, and can exploit the image global correlation to produce more robust reconstruction of image local structures. Experimental results clearly validate the advantages of CSC over patch based SC in SR application. Compared with state-of-the-art SR methods, the proposed CSC-SR method achieves highly competitive PSNR results, while demonstrating better edge and texture preservation performance.</Data></Cell>
    <Cell><Data ss:Type="String">Shuhang Gu*, Hong Kong PolyU; Lei Zhang, The Hong Kong Polytechnic University; Wangmeng Zuo, Harbin Institute of Technology; Xiangchu Feng, Xidian University; Deyu Meng, Xi'an Jiaotong University; Qi Xie, Xi'an Jiaotong University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1391</Data></Cell>
    <Cell><Data ss:Type="String">Peeking Template Matching for Depth Extension</Data></Cell>
    <Cell><Data ss:Type="String">We propose a method that extends a given depth image into regions in 3D that are not visible from the point of view of the camera. The algorithm detects repeated 3D structures in the visible scene and suggests a set of 3D extension hypotheses, which are then combined together through a global 3D MRF discrete optimization. The recovered global 3D surface is consistent with both the input depth map and the hypotheses.    A key component of this work is a novel 3D template matcher that is used to detect repeated 3D structure in the scene and to suggest the hypotheses. A unique property of this matcher is that it can handle depth uncertainty. This is crucial because the matcher is required to ``peek around the corner'', as it operates at the boundaries of the visible 3D scene where depth information is missing. The proposed matcher is fast and is guaranteed to find an approximation to the globally optimal solution.    We demonstrate on real-world data that our algorithm is capable of completing a full 3D scene from a single depth image and can synthesize a full depth map from a novel viewpoint of the scene. In addition, we report results on an extensive synthetic set of 3D shapes, which allows us to evaluate the method both qualitatively and quantitatively.</Data></Cell>
    <Cell><Data ss:Type="String">Simon Korman*, Tel Aviv University; Eyal Ofek, Microsoft Corporation; Shai Avidan, &quot;Tel-Aviv University, Israel&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1403</Data></Cell>
    <Cell><Data ss:Type="String">Deformable 3D Fusion: From Partial Dynamic 3D Observations to Complete 4D Models</Data></Cell>
    <Cell><Data ss:Type="String">Capturing the 3D motion of dynamic, non-rigid objects has attracted significant attention in computer vision. Existing methods typically require either complete 3D volumetric observations, or a shape template. In this paper, we introduce a template-less 4D reconstruction method that incrementally fuses highly-incomplete 3D observations of a deforming object, and generates a complete, temporally-coherent shape representation of the object. To this end, we design an online algorithm that alternatively registers new observations to the current model estimate and updates the model. We demonstrate the effectiveness of our approach at reconstructing non-rigidly moving objects from highly-incomplete measurements on both sequences of partial 3D point clouds and Kinect videos. </Data></Cell>
    <Cell><Data ss:Type="String">Weipeng Xu*, BIT/NICTA; Mathieu Salzmann, NICTA; Yongtian Wang, Beijing Institute of Technolog; Yue Liu, Beijing Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1416</Data></Cell>
    <Cell><Data ss:Type="String">Multi-scale recognition with DAG-CNNs</Data></Cell>
    <Cell><Data ss:Type="String">We explore multi-scale convolutional neural nets (CNNs) for image classification. Contemporary approaches extract features from a single output layer. By extracting features from multiple layers, one can simultaneously reason about high, mid, and low-level features during classification. The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to learn a set of multi-scale features that can be effectively shared between coarse and fine-grained classification tasks. While fine-tuning such models helps performance, we show that even ``off-the-self'' multi-scale features perform quite well. We present extensive analysis and demonstrate state-of-the-art classification performance on three standard scene benchmarks (SUN397, MIT67, and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets, our results reduce the lowest previously-reported error by 23.9 and 9.5, respectively.</Data></Cell>
    <Cell><Data ss:Type="String">Songfan Yang*, Sichuan University; Deva Ramanan, UC Irvine</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1417</Data></Cell>
    <Cell><Data ss:Type="String">Non-Parametric Structure-Based Calibration of Radially Symmetric Cameras</Data></Cell>
    <Cell><Data ss:Type="String">We propose a two-step method for estimating the intrinsic and extrinsic calibration of any radially symmetric camera, including non-central systems. The first step consists of estimating the camera pose, given a Structure from Motion (SfM) model, up to the translation along the optical axis. As a second step, we obtain the calibration by finding the translation of the camera center using an ordering constraint. The method makes use of the 1D radial camera model, which allows us to effectively handle any radially symmetric camera, including non-central ones. Using this ordering constraint, we show that the we are able to calibrate several different (central and non-central) Wide Field of View (WFOV) cameras, including fisheye, hyper-catadioptric and   spherical catadioptric cameras, using a single image or jointly solving for several views. </Data></Cell>
    <Cell><Data ss:Type="String">Federico Camposeco Paulsen*, ETH Zürich; Torsten Sattler, ETH Zurich; Marc Pollefeys, ETH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1419</Data></Cell>
    <Cell><Data ss:Type="String">Discriminative Pose-Free Descriptors for Face and Object Matching</Data></Cell>
    <Cell><Data ss:Type="String">Pose invariant matching is a very important and challenging problem with various applications like recognizing faces in uncontrolled scenarios, matching objects taken from different view points, etc. In this paper, we propose a discriminative pose-free descriptor (DPFD) which can be used to match faces/objects across pose variations. Training examples at very few representative poses are used to generate virtual intermediate pose subspaces. An image or image region is then represented by a feature set obtained by projecting it on all these subspaces and a discriminative transform is applied on this feature set to make it suitable for classification tasks. Finally, this discriminative feature set is represented by a single feature vector, termed as DPFD. The DPFD of images taken from different viewpoints can be directly compared for matching. Extensive experiments on recognizing faces across pose, pose and resolution on the Multi-PIE and Surveillance Cameras Face  datasets and comparisons with state-of-the-art approaches show the effectiveness of the proposed approach. Experiments on matching general objects across viewpoints show the generalizability of the proposed approach beyond faces.</Data></Cell>
    <Cell><Data ss:Type="String">Soubhik  Sanyal, Indian Institute of Science, Bangalore, India; Sivaram  Mudunuri, Indian Institute of Science, Bangalore, India; Soma Biswas*, Indian Institute of Science</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1424</Data></Cell>
    <Cell><Data ss:Type="String">Bi-shifting Auto-Encoder for Unsupervised Domain Adaptation</Data></Cell>
    <Cell><Data ss:Type="String">In many real-world applications, the domain of model learning (referred as source domain) is usually inconsistent with or even different from the domain of testing (referred as target domain), which makes the learnt model degenerate in the target domain, \textit{i.e.}, the test domain. To alleviate the discrepancy between source and target domains, we propose a domain adaptation method, named as Bi-shifting Auto-Encoder network (BAE). The proposed BAE attempts to shift source domain samples to target domain, and also shift the target domain samples to source domain. The non-linear transformation of BAE ensures the feasibility of shifting between domains, and distribution consistency between the shifted domain and the desirable domain is constrained by sparse reconstruction between them. As a result, the shifted source domain is supervised and follows similar distribution as target domain. Therefore, any supervised method can be applied on the shifted source domain to train a classifier for classification in target domain. The proposed method is evaluated on three domain adaptation scenarios of face recognition, \textit{i.e.}, domain adaptation across view angle, ethnicity, and imaging sensor, and the promising results demonstrate that our proposed BAE can shift samples between domains and thus effectively deal with the domain discrepancy.  </Data></Cell>
    <Cell><Data ss:Type="String">Meina Kan*, Chinese Academy of Sciences; Shiguang Shan, Chinese Academy of Sciences; Xilin Chen, &quot;Institute of Computing Technology, Chinese Academy of Sciences&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1427</Data></Cell>
    <Cell><Data ss:Type="String">Relaxed Multiple-Instance SVM with Application to Object Discovery</Data></Cell>
    <Cell><Data ss:Type="String">Multiple-instance learning (MIL) has served as an important tool for a wide range of vision applications, for instance, image classification, object detection, and visual tracking. In this paper, we propose a novel method to solve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM). We treat the positiveness of instance as a continuous variable, use Noisy-OR model to enforce the MIL constraints, and optimize them jointly in a unified framework. The optimization problem can be efficiently solved using stochastic gradient decent. The extensive experiments demonstrate that RMI-SVM consistently achieves superior performance on various benchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision task, common object discovery. The state-of-the arts results of object discovery on PASCAL VOC datasets further confirm the advantages of the proposed method.</Data></Cell>
    <Cell><Data ss:Type="String">Xinggang Wang*, Huazhong University of Science and Technology; Zhuotun Zhu, Huazhong University of Science and Technology; Cong Yao, Huazhong University of Science and Technology; Xiang Bai, Huazhong University of Science and Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1441</Data></Cell>
    <Cell><Data ss:Type="String">Active oneshot scan  for wide depth range using a light-field projector based on coded aperture</Data></Cell>
    <Cell><Data ss:Type="String">Since optical characteristics of a projector and a camera are the same,   i.e., a central projection model, advantages and disadvantages of them   are also alike. Among several problems on central projection model,   narrow depth of field and necessity of wide base line are crucial. In   the paper, we solve the problems by introducing a light field projector,   which is not a central projection model. The proposed light field   projector consists of normal video projector attached with a coded   aperture with a slit pattern in front of the lens and a slit pattern for   projection. Since such system inevitably creates spatially non-uniform   and depth-variant distortion, which helps to increase the accuracy and   robustness of the system, matching are not easy. For solution, image   based technique is adopted.   Although an image based technique usually requires a large database and heavy   computational cost, they are efficiently solved by our hierarchical   matching approach and a feature based search technique. In the   experiments, it is confirmed that our method can accurately recover the   shape with curved and textured objects for wide depth range.  </Data></Cell>
    <Cell><Data ss:Type="String">Hiroshi Kawasaki*, Kagoshima University; Yuki Horita, ; Yuki Shiba, ; Satoshi Ono, ; Ryo Furukawa, Hiroshima City University; Shinsaku Hiura, Hiroshima City University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1449</Data></Cell>
    <Cell><Data ss:Type="String">Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Video segmentation is a stepping stone to understanding video context.  Video segmentation enables one to represent a video by decomposing it into coherent regions which comprise whole or parts of objects.  However, the challenge originates from the fact that most of the video segmentation algorithms are based on unsupervised learning due to expensive cost of pixelwise video annotation and intra-class variability within similar unconstrained video classes.  We propose a Markov Random Field model for unconstrained video segmentation that relies on tight integration of multiple cues: vertices are defined from contour based superpixels, unary potentials from temporal smooth label likelihood and pairwise potentials from global structure of a video.  Multi-cue structure is a breakthrough to extracting coherent object regions for unconstrained videos in absence of supervision.  Our experiments on VSB100 dataset show that the proposed model significantly outperforms competing state-of-the-art algorithms.  Qualitative analysis illustrates that video segmentation result of the proposed model is consistent with human perception of objects.</Data></Cell>
    <Cell><Data ss:Type="String">Saehoon Yi*, Rutgers University; Vladimir Pavlovic, Rutgers University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1456</Data></Cell>
    <Cell><Data ss:Type="String">Regressive Tree Structured Model for Facial Landmark Localization</Data></Cell>
    <Cell><Data ss:Type="String">Although the Tree Structured Model (TSM) is proven effective for solving face detection, pose estimation and landmark localization in an unified model, its sluggish run time makes it unfavorable in practical applications, especially when dealing with cases of multiple faces. We propose the Regressive Tree Structure Model (RTSM) to improve the run-time speed and localization accuracy. The RTSM is composed of two component TSMs, the coarse TSM (c-TSM) and the refined TSM (r-TSM), and a Bilateral Support Vector Regressor (BSVR). The c-TSM is built on the low-resolution octaves of samples so that it provides coarse but fast face detection. The r-TSM is built on the mid-resolution octaves so that it can locate the landmarks on the face candidates given by the c-TSM and improve precision. The r-TSM based landmarks are used in the forward BSVR as references to locate the dense set of landmarks, which are then used in the backward BSVR to relocate the landmarks with large localization errors. The forward and backward regression goes on iteratively until convergence. The performance of the RTSM is validated on three benchmark databases, the Multi-PIE, LFPW and AFW, and compared with the latest TSM to demonstrate its efficacy.</Data></Cell>
    <Cell><Data ss:Type="String">Gee-Sern Hsu*, Nat. Taiwan Univ. Sci Tech.; Kai-Hsiang  Chang, Nat. Taiwan Univ. Sci. &amp; Tech.; Shih-Chieh Huang, Nat. Taiwan Univ. Sci. &amp; Tech.</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1460</Data></Cell>
    <Cell><Data ss:Type="String">Person Recognition in Personal Photo Collections</Data></Cell>
    <Cell><Data ss:Type="String">Recognising persons in everyday photos presents major challenges (occluded faces, different clothing, locations, etc.) for machine vision. We propose a convnet based person recognition system on which we provide an in-depth analysis of informativeness of different body cues, impact of training data, and the common failure modes of the system. In addition, we discuss the limitations of existing benchmarks and propose more challenging ones. Our method is simple and is built on open source and open data, yet it improves the state of the art results on a large dataset of social media photos (PIPA).</Data></Cell>
    <Cell><Data ss:Type="String">Seong Joon Oh*, MPI-INF; Rodrigo Benenson, MPI Informatics; Mario Fritz, MPI Informatics, Saarbrucken, Germany; Bernt Schiele, MPI Informatics Germany</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1472</Data></Cell>
    <Cell><Data ss:Type="String">Weakly-supervised Structured Output Learning with Flexible and Latent Graphs using High-order Loss Functions</Data></Cell>
    <Cell><Data ss:Type="String">We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimizes a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumors, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumor tissue supplied by a microvessel).  The proposed methodologies take as input multimodal microscopy images of a tumor, and estimate the number and proportion of MCSU classes.  This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labeled with the MCSU class and image location.  The training process uses the manual weak annotation available, consisting of the number of MCSU classes per training image, where the training objective is the minimization of a high-order loss function based on the norm of the error between the manual and estimated annotations.  One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a novel deep convolutional neural network (DCNN) model.  Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumors, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display significant correlation values regarding the number and proportion of MCSU classes compared to the manual annotations. </Data></Cell>
    <Cell><Data ss:Type="String">Gustavo Carneiro*, University of Adelaide; tingying Peng, Technical University of Munich; Christine Bayer, Technical University of Munich; Nassir  Navab, Johns Hopkins University and TU Munich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1479</Data></Cell>
    <Cell><Data ss:Type="String">DeepBoxes: Hunting Objects by Cascading Deep Convolutional Layers</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we evaluate the quality of the activation layers of a convolutional neural network for the generation of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and  show that the final layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall.  Based on this observation we designed a method for proposing objects that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the-art detection performance.</Data></Cell>
    <Cell><Data ss:Type="String">Amir Ghodrati*, KULeuven; Marco Pedersoli, Leuven University; Tinne Tuytelaars, KU Leuven; Ali Diba, KULeuven; Luc Van Gool, KTH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1487</Data></Cell>
    <Cell><Data ss:Type="String">Beyond Tree Structure Models: A New Occlusion Aware Graphical Model for Human Pose Estimation</Data></Cell>
    <Cell><Data ss:Type="String">Occlusion is a main challenge for human pose estimation, which is largely ignored in popular tree structure models. The tree structure model is simple and convenient for exact inference, but short in modeling the occlusion coherence especially in the case of self-occlusion. We propose an occlusion aware graphical model which is able to model both self-occlusion and occlusion by the other objects simultaneously. The proposed model structure can encode the interactions between human body parts and objects, and hence enable it to learn occlusion coherence from data discriminatively.  We evaluate our model on several public benchmarks for human pose estimation including challenging subsets featuring significant occlusion. The experimental results show that our method obtains comparable accuracy with the state-of-the-arts, and achieves promising performance in 2D human pose estimation with occlusion.  </Data></Cell>
    <Cell><Data ss:Type="String">Lianrui Fu*, Institute of Automation, Chine; Junge Zhang, ; Kaiqi Huang, National Laboratory of Pattern Recognition</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1489</Data></Cell>
    <Cell><Data ss:Type="String">Realtime edge-based visual odometry for a monocular camera</Data></Cell>
    <Cell><Data ss:Type="String">In this work we present a novel algorithm for realtime visual odometry for a monocular camera. The main idea is to develop an approach between classical feature-based visual odometry systems and modern direct dense/semi-dense methods, trying to benefit from the best attributes of both. Similar to feature-based systems, we extract information from the images, instead of working with raw image intensities as direct methods. In particular, the information extracted are the edges present in the image, while the rest of the algorithm is designed to take advantage of the structural information provided when pixels are treated as edges. Edge extraction is a higly parallelizable operation with a lower computational cost than regular feature extraction and matching. The edge depth information extracted is dense enough to allow acceptable surface fitting and obstacle avoidance, similar to modern semi-dense methods. This is a valuable attribute that feature-based odometry lacks. Experimental results show that the proposed method has less drift than state of the art feature-based and direct methods, and is a simpler algorithm that runs at realtime and can be parallelized. Finally, we have also developed an inertial aided version that successfully stabilizes an unmanned air vehicle in complex indoor environments using only a frontal camera, while running the complete solution in the embedded hardware on board the vehicle.</Data></Cell>
    <Cell><Data ss:Type="String">Juan Tarrio*, CNEA; Sol Pedre, CNEA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1505</Data></Cell>
    <Cell><Data ss:Type="String">Shell PCA: statistical shape modelling in shell space</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we describe how to perform Principal Com-  ponents Analysis in “shell space”. Thin shells are a phys-  ical model for surfaces with non-zero thickness whose de-  formation dissipates elastic energy. Thin shells, or their  discrete counterparts, can be considered to reside in a shell  space in which the notion of distance is given by the elastic  energy required to deform one shape into another. It is in  this setting that we show how to perform statistical anal-  ysis of a set of shapes (meshes in dense correspondence),  providing a hybrid between physical and statistical shape  modelling. The resulting models are better able to capture  non-linear deformations, for example resulting from articu-  lated motion, even when training data is very sparse com-  pared to the dimensionality of the observation space.</Data></Cell>
    <Cell><Data ss:Type="String">Chao Zhang*, University of York; William Smith, University of York; Behrend Heeren, ; Martin Rumpf, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1509</Data></Cell>
    <Cell><Data ss:Type="String">Secrets of Matrix Factorization: Approximations, Numerics and Manifold Optimization</Data></Cell>
    <Cell><Data ss:Type="String">Matrix factorization (or low-rank matrix completion) with missing data is a key computation in many computer vision and machine learning tasks, and is also related to a broader class of nonlinear optimization problems such as bundle adjustment.  The problem has received much attention recently, with renewed interest in variable-projection approaches, yielding dramatic improvements in reliability and speed.   However, on a wide class of problems, no one approach dominates, and because the various approaches have been derived in a multitude of different ways, it has been difficult to unify them.   This paper provides a unified derivation of a number of recent approaches, not just unifying notation, but drawing close connections between the algorithms, so that similarities and differences are easily observed.   This allows a number of generic improvements applicable to all members of the family to be isolated, yielding a unified algorithm that outperforms our re-implementation of existing algorithms, which could already surpass original authors' publicly available codes.  An important contribution of our work is a new performance metric for such systems that supersedes the current focus on &quot;success rate&quot;, which could  readily be maximized given enough compute.  A non-goal is to present a &quot;new&quot; algorithm, although our derivations are certainly novel, and the practical improvement on the state of the art is substantial.</Data></Cell>
    <Cell><Data ss:Type="String">Je Hyeong Hong*, University of Cambridge; Andrew Fitzgibbon, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1512</Data></Cell>
    <Cell><Data ss:Type="String">Semantic Segmentation With Object Clique Potential</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose an object clique potential for semantic segmentation. Our object clique potential addresses the misclassified object-part issues arising in solutions based on fully-connected networks. Our object clique set, compared to that yielded from segment-proposal-based approaches, is with a significantly small size, making our method consume notably less computation. Regarding system design and model formation, our object clique potential can be regarded as a functionally complement to local-appearance-based CRF models and works in synergy with these effective approaches for further performance improvement. Extensive experiments verify our method.</Data></Cell>
    <Cell><Data ss:Type="String">QI Xiaojuan*, CUHK; Shu Liu, Chinese University of Hong Kon; Jianping Shi, CUHK; Renjie Liao, CUHK; Jiaya Jia, Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1521</Data></Cell>
    <Cell><Data ss:Type="String">Multi-Frame Super-Resolution via Draft-Ensemble Learning</Data></Cell>
    <Cell><Data ss:Type="String">We propose a new direction for fast multi-frame super-resolution (MFSR) via SR draft ensemble that is defined as the set of high-resolution patch candidates before final image doconvolution. Our method contains two main components of SR draft generation and their optimal reconstruction. The first component is to renovate traditional feedforward reconstruction pipeline and greatly enhance its ability to compute different super-resolution results considering large motion variation and possible errors arising in this process. Then we combine SR drafts through the nonlinear process in a deep convolutional neural network (CNN). We analyze why this framework is proposed and explain its unique advantages compared to previous iterative methods to update different modules in passes. Many experimental results are shown on both synthetic and real natural video sequences in this paper and supplementary material.</Data></Cell>
    <Cell><Data ss:Type="String">Renjie Liao*, CUHK; Xin Tao, CUHK; Ziyang Ma, Institute of Software, CAS; Ruiyu Li, CUHK; Jiaya Jia, Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1524</Data></Cell>
    <Cell><Data ss:Type="String">Pan-sharpening with a Hyper-Laplacian Penalty</Data></Cell>
    <Cell><Data ss:Type="String">Pan-sharpening is the task of fusing spectral information in low resolution multispectral images with spatial information in corresponding high resolution panchromatic image. In such approaches, there is a trade-off between spectral and spatial quality, as well as computational efficiency. We present a method for pan-sharpening in which a sparsity-promoting objective function preserves both spatial and spectral content, and is efficient to optimize. Our objective incorporates the L1/2-norm in a way that can leverage recent computationally efficient methods, and L1 for which the alternating direction method of multipliers can be used. Additionally, our objective penalizes image gradients to enforce high resolution fidelity, and exploits the Fourier domain for further computational efficiency. Visual quality metrics demonstrate that our proposed objective function can achieve higher spatial and spectral resolution than several previous well-known methods with competitive computational efficiency.  </Data></Cell>
    <Cell><Data ss:Type="String">Yiyong Jiang, Xiamen University; Xinghao Ding*, Xiamen University; Delu Zeng, Xiamen University; Yue Huang, Xiamen University; John Paisley, Columbia University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1526</Data></Cell>
    <Cell><Data ss:Type="String">Structured Learning for Generating Region Proposals with Mid-level Cues</Data></Cell>
    <Cell><Data ss:Type="String">The object categorization community's migration from object detection to large-scale object categorization has seen a shift from sliding window approaches to bottom-up region segmentation, with the resulting {\it region proposals} offering discriminating shape and appearance features through an attempt to explicitly segment the objects in a scene from their background.   One powerful class of region proposal techniques is based on parametric energy minimization (PEM) via parametric maxflow.  In this paper, we incorporate PEM into a novel structured learning framework that learns how to combine a set of mid-level grouping cues to yield a small set of region proposals with high recall.  We diversify our energy function with location-based and color-based seeds, and show that our structured learning framework is not only compatible with diversification, but also boosts its recall for the same number of proposals.  Our novel approach casts perceptual grouping and cue combination in a structured learning framework which yields baseline improvements on VOC'2012. </Data></Cell>
    <Cell><Data ss:Type="String">Tom Lee*, University of Toronto; Sanja Fidler, University of Toronto; Sven  Dickinson, &quot;University of Toronto, Canada&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1527</Data></Cell>
    <Cell><Data ss:Type="String">FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation</Data></Cell>
    <Cell><Data ss:Type="String">One of the most popular approaches to multi-target tracking is tracking-by-detection. Current  min-cost flow algorithms which solve the data association problem optimally have three main drawbacks: they are computationally expensive, they assume that the whole video is given as a batch, and they scale badly in memory and computation with the length of the video sequence. In this paper, we address each of these issues, resulting in a computationally and memory-bounded solution. First, we introduce a dynamic version of the successive shortest-path algorithm  which solves the data association problem optimally while reusing computation, resulting in considerably faster inference than standard solvers. Second, we address the optimal solution to the data association problem when dealing with an incoming stream of data (i.e., online setting). Finally, we present our main contribution which is an approximate online solution with bounded memory and computation which is capable of handling videos of arbitrarily length while performing tracking in real time. We demonstrate the effectiveness of our algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art performance, while being significantly faster than existing solvers. </Data></Cell>
    <Cell><Data ss:Type="String">Philip Lenz, KIT; Andreas Geiger*, MPI Intelligent Systems; Raquel Urtasun, University of Toronto</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1544</Data></Cell>
    <Cell><Data ss:Type="String">Relaxing from Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging</Data></Cell>
    <Cell><Data ss:Type="String">The development of deep learning has empowered machines with comparable capability of recognizing limited image categories to human beings. However, most existing approaches heavily rely on human-curated training data, which hinders the scalability to large and unlabeled vocabularies in image tagging. In this paper, we propose a weakly-supervised deep learning model which can be trained from the readily available Web images to relax the dependence on human labors and scale up to arbitrary tags(categories). Specifically, we embed the feature map of the last deep layer into a new affinity representation that intrinsically explores the similarities among the deep features of training samples. Based on the assumption that features of true samples in a category tend to be similar and noises tend to be variant, we minimize the discrepancy between the affinity representation and its low-rank approximation. The discrepancy is further transformed into the objective function to  give relevance feedback to back propagation. Since the labels are not reliable in Web scenario, this feedback is to give those ``few and different'' noisy samples low-level authorities in training. Extensive experiments show that by leveraging the Web training images, the proposed model is able to outperform the state-of-the-art methods in various image categorization tasks. Besides, we achieve a performance gain of $14.0\%$ in term of Precision@20 in image tagging with the largest vocabulary set of about 63,000 tags from the WordNet, against the typical deep learning model trained on the ImageNet 1,000 vocabulary set.</Data></Cell>
    <Cell><Data ss:Type="String">Jianlong Fu*, NLPR, CASIA; Yue Wu, ; Tao Mei, Microsoft Research Asia; Jinqiao Wang, National Laboratory of Pattern Recognition, Institute of Automation, CAS; Hanqing Lu, National Lab. of Pattern Recognition  CASIA; Yong Rui, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1549</Data></Cell>
    <Cell><Data ss:Type="String">Learning to Divide and Conquer for Online Multi-Target Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Online Multiple Target Tracking (MTT) is often addressed within the tracking-by-detection paradigm. Detections are previously extracted independently in each frame and then objects trajectories are built by maximizing specifically designed coherence functions. Nevertheless, ambiguities arise in presence of occlusions or detection errors.   In this paper we claim that the ambiguities in tracking could be solved by a selective use of the features, by working with more reliable features if possible and exploiting a deeper representation of the target only if necessary. To this end, we propose an online divide and conquer approach that partitions the assignment problem in local subproblems and solves them by selectively choosing and combining the best features. The complete framework is cast as a structural learning task that unifies these phases and learns tracker parameters from examples.  Experiments on two different datasets highlights a significant improvement of tracking performances (MOTA +10%) over the state of the art.</Data></Cell>
    <Cell><Data ss:Type="String">Francesco Solera*, University of Modena; Simone Calderara, University of Modena; Rita Cucchiara, University of Modena</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1551</Data></Cell>
    <Cell><Data ss:Type="String">Visual Phrases for Exemplar Face Detection</Data></Cell>
    <Cell><Data ss:Type="String">Recently, exemplar based approaches have been successfully applied for face detection in the wild. Contrary to traditional approaches that model face variations from a large and diverse set of training examples, exemplar-based approaches use a collection of discriminatively trained exemplars for detection. In this paradigm, each exemplar casts a vote using retrieval framework and Hough voting, which are then combined to locate the faces in the target image. The advantage of this scheme is that by having a large database that covers all possible variations, faces in challenging conditions can be detected without having to learn explicit models for different variations.    Current schemes, however, make an assumption of independence between the visual words due to which their relations are ignored. They also ignore the spatial consistency of the visual words. Consequently, every exemplar word contributes equally during voting regardless of its location. In this paper, we propose a novel approach that incorporates higher order information in the voting process. We discover visual phrases that contain semantically related visual words and exploit them for detection along with the visual words. For spatial consistency of visual words, we estimate their spatial distribution over the entire database and then weight their occurrence in exemplars. This ensures that a visual word in an exemplar makes more contribution only if it occurs at its semantic location, thereby suppressing the noise significantly. We perform extensive experiments on standard FDDB, AFW and G-album datasets that show significant improvements over previous exemplar approaches.</Data></Cell>
    <Cell><Data ss:Type="String">Vijay Kumar Reddy*, IIIT Hyderabad; Anoop Namboodiri, IIIT Hyderabad; C.V. Jawahar, IIIT Hyderabad</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1554</Data></Cell>
    <Cell><Data ss:Type="String">Enhancing World Maps by Parsing Aerial Images</Data></Cell>
    <Cell><Data ss:Type="String">In recent years, contextual models that exploit maps have been shown to be very effective for many recognition and localization tasks.   In this paper we propose to exploit aerial images in order to enhance  freely available world maps. Towards this goal, we make use of OpenStreetMap and formulate the problem as the one of inference in a Markov random field parameterized in terms of  the location of the road-segment centerlines as well as  their width.   This parameterization enables very efficient inference and  returns only topologically correct roads. In particular, we can segment all OSM roads in the  whole world in a single day using a small cluster of 10 computers. Importantly, our approach generalizes very well; it can be trained using a single  aerial image and produce very accurate results in any location across the globe.    We demonstrate the effectiveness of our approach outperforming the state-of-the-art in two new benchmarks that we collect. We then show how our enhanced maps are beneficial for semantic segmentation of ground images. </Data></Cell>
    <Cell><Data ss:Type="String">Gellert Mattyus*, German Aerospace Center; Shenlong Wang, University of Toronto; Sanja Fidler, University of Toronto; Raquel Urtasun, University of Toronto</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1566</Data></Cell>
    <Cell><Data ss:Type="String">Exploiting Object Similarity in Large-Scale 3D Reconstruction</Data></Cell>
    <Cell><Data ss:Type="String">Despite recent progress, reconstructing outdoor scenes in 3D from movable platforms remains a highly difficult endeavor. Challenges include low frame rates, occlusions, large distortions and difficult lighting conditions. In this paper, we leverage the fact that the larger the reconstructed area, the more likely objects of similar type and shape will occur in the scene. This is particularly true for outdoor scenes where buildings and vehicles often suffer from missing texture or reflections, but share similarity in 3D shape. We take advantage of this shape similarity by locating objects using detectors and jointly reconstructing them while learning a volumetric model of their shape. This allows us to reduce noise while completing missing surfaces as objects of similar shape benefit from all observations for the respective category. We evaluate our approach with respect to LIDAR ground truth on a novel challenging suburban dataset and show its advantages over the state-of-the-art. Code and data will be made available upon publication.</Data></Cell>
    <Cell><Data ss:Type="String">Chen Zhou, Peking University; Fatma Güney, MPI Tübingen; Yizhou Wang, Peking University; Andreas Geiger*, MPI Intelligent Systems</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1582</Data></Cell>
    <Cell><Data ss:Type="String">Probabilistic Appearance Models for Segmentation and Classification</Data></Cell>
    <Cell><Data ss:Type="String">Statistical shape and appearance models are often based on the accurate identification of one-to-one correspondences in a training data set. At the same time, the determination of these corresponding landmarks is the most challenging part of such methods. Hufnagel \etal developed an alternative method using correspondence probabilities for a statistical shape model.   We propose the use of probabilistic correspondences for statistical appearance models by incorporating appearance information into the framework. A point-based representation is employed representing the image by a set of vectors assembling position and appearances. Using probabilistic correspondences between these multi-dimensional feature vectors eliminates the need for extensive preprocessing to find corresponding landmarks and reduces the dependence of the generated model on the landmark positions. Then, a maximum a-posteriori approach is used to derive a single global optimization criterion with respect to model parameters and observation dependent parameters, that directly affects shape and appearance information of the considered structures. Model generation and fitting can be expressed by optimizing the same criterion.   The developed framework describes the modeling process in a concise and flexible mathematical way and allows for additional constraints as topological regularity in the modeling process. Furthermore, it eliminates the demand for costly correspondence determination.   We apply the model for segmentation and landmark identification in hand X-ray images, where segmentation information is modeled as further features in the vectorial image representation. The results demonstrate the feasibility of the model to reconstruct contours and landmarks for unseen test images. Furthermore, we apply the model for tissue classification, where a model is generated for healthy brain tissue using 2D MRI slices. Applying the model to images of stroke patients the probabilistic correspondences are used to classify between healthy and pathological structures. The results demonstrate the ability of the probabilistic model to recognize healthy and pathological tissue automatically.</Data></Cell>
    <Cell><Data ss:Type="String">Julia Krüger*, Universität zu Lübeck; Jan Ehrhardt, University of Lübeck; Heinz Handels, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1585</Data></Cell>
    <Cell><Data ss:Type="String">HotOrNot: What Images Tell Us About Temperature</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we explore interactions between the appearance of an outdoor scene and the ambient temperature.   By studying the statistical correlations between image sequences from outdoor cameras and temperature measurements we identify two interesting interactions.   First, semantically meaningful regions such as foliage and reflective oriented surfaces are often highly indicative of the temperature. Second, small camera motions are correlated with the temperature in some scenes.   We propose simple scene-specific temperature prediction algorithms which can be used to turn a camera into a crude temperature sensor. We find that for this task, simple features such as local pixel intensities outperform sophisticated, global features such as from a semantically-trained convolutional neural network.  </Data></Cell>
    <Cell><Data ss:Type="String">Daniel Glasner*, Harvard University; Pascal  Fua, &quot;EPFL, Switzerland&quot;; Todd Zickler, Harvard; Lihi Zelnik-Manor, Technion</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1595</Data></Cell>
    <Cell><Data ss:Type="String">Geometry-aware Deep Transform</Data></Cell>
    <Cell><Data ss:Type="String">Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled  training samples to learn a huge number of parameters in a network; therefore,  understanding  the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is  the case for many applications.  In this paper, we  propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria.   We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for  both synthetic and real-world data. We further support the proposed framework with a formal $(K,\epsilon)$-robustness analysis.</Data></Cell>
    <Cell><Data ss:Type="String">Jiaji Huang*, Duke University; Qiang Qiu, Duke University; Robert Calderbank, Duke University; Guillermo  Sapiro, &quot;Duke University, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1598</Data></Cell>
    <Cell><Data ss:Type="String">Video Restoration against Yin-Yang Phasing</Data></Cell>
    <Cell><Data ss:Type="String">A common video degradation problem, which is largely untreated in literature, is what we call Yin-Yang Phasing (YYP).  YYP is characterized by involuntary, dramatic flip-flop in the intensity and possibly chromaticity of an object as the video plays.  Such temporal artifacts occur under ill illumination conditions and are triggered by object or/and camera motions, which mislead the settings of camera's auto-exposure and white point.  In this paper, we investigate the problem and propose a video restoration technique to suppress YYP artifacts and retain temporal consistency of objects appearance via inter-frame, spatially-adaptive, optimal tone mapping. The video quality can be further improved by a novel image enhancer designed in Weber's perception principle and by exploiting the second-order statistics of the scene.  Experimental results are encouraging, pointing to an effective, practical solution for a common but surprisingly understudied problem.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaolin Wu*, McMaster University; Zhenhao Li, McMaster University; Xiaowei Deng, McMaster University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1610</Data></Cell>
    <Cell><Data ss:Type="String">Rolling Shutter Super-Resolution</Data></Cell>
    <Cell><Data ss:Type="String">Classical multi-image super-resolution (SR) algorithms, designed for CCD cameras, assume that the motion among the images is global. But CMOS sensors that have increasingly started to replace their more expensive CCD counterparts in many applications do not respect this assumption if there is a motion of the camera relative to the scene during the exposure duration of an image because of the row-wise acquisition mechanism. In this paper, we study the hitherto unexplored topic of multi-image SR in CMOS cameras. We initially develop an SR observation model that accounts for the row-wise distortions called the ``rolling shutter'' (RS) effect observed in images captured using non-stationary CMOS cameras. We then propose a unified RS-SR framework to obtain an RS-free high-resolution image (and the row-wise motion) from distorted low-resolution images. We demonstrate the efficacy of the proposed scheme using synthetic data as well as real images captured using a hand-held CMOS camera. Quantitative and qualitative assessments reveal that our method significantly advances the state-of-the-art.</Data></Cell>
    <Cell><Data ss:Type="String">Abhijith Punnappurath*, IIT Madras; Vijay Rengarajan, IIT Madras; A N Rajagopalan, &quot;Indian Institute of Technology, Madras&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1618</Data></Cell>
    <Cell><Data ss:Type="String">Im2Calories: towards an automated mobile vision food diary</Data></Cell>
    <Cell><Data ss:Type="String">We tackle the challenging problem of estimating the nutritional contents of a meal from a single image.  We break the problem down into 8 key steps:  (1) determining if the image contains food or not (binary classification);  (2) classifying the overall type of meal (multi-class classification);  (3) predicting all the food items present (multi-label classification);  (4) semantic segmentation of all the food items;  (5) estimating the count of countable food items;  (6) estimating the pose of the camera;  (7) estimating the volume of the non-countable food items;  (8) finally, estimating overall nutritional content of the meal.  We create datasets and metrics for each of these 8 steps,   extending the existing Food101 classification dataset  \cite{Bossard2014}.    Next, we introduce our method for solving these tasks,  which relies on deep convolutional neural networks (CNNs).  Finally, we present the results of our system on each of these challenging  problems. We show that we outperform simpler baselines.  In particular,  on problem 2, which corresponds to the existing Food101 classification benchmark, we achieve an accuracy of 78.95\%,  which significantly outperforms the earlier method of  \cite{Bossard2014}, which achieved 50.76\%.</Data></Cell>
    <Cell><Data ss:Type="String">Alex Gorban, ; Nick Johnston, ; Anoop Korattikara, ; George Papandreou, Google Inc.; Vivek Rathod, ; Kevin Murphy*, Google Inc.; Sergio Guadarrama, UC of Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1633</Data></Cell>
    <Cell><Data ss:Type="String">You Are Here: Mimicking the Human Thinking Process in Reading Floor-Plans</Data></Cell>
    <Cell><Data ss:Type="String">A human can easily find his or her way in an unfamiliar building, by walking around and reading the floor-plan. We try to mimic and automate this human thinking process. More precisely, we introduce a new and useful task of locating an user in the floor-plan, by using only a camera and a floor-plan without any other prior information. We address the problem with a novel matching-localization algorithm that is inspired by human logic. We demonstrate through experiments that our method outperforms state-of-the-art floor-plan-based localization methods by a large margin, while also being highly efficient for real-time applications.</Data></Cell>
    <Cell><Data ss:Type="String">Hang Chu*, Cornell University; Dong Ki Kim, Cornell University; Tsuhan Chen, Cornell University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1637</Data></Cell>
    <Cell><Data ss:Type="String">Automatic Concept Discovery from Parallel Text and Visual Corpora</Data></Cell>
    <Cell><Data ss:Type="String"> Humans connect language and vision to perceive the world. How to build a similar connection for computers? One possible way is via visual concepts, which are text terms that relate to visually discriminative entities. We propose an automatic visual concept discovery algorithm using parallel text and visual corpora; it filters text terms based on the visual discriminative power of the associated images, and groups them into concepts using visual and semantic similarities. We illustrate the applications of the discovered concepts using bidirectional image and sentence retrieval task and image tagging task, and show that the discovered concepts not only outperform several large sets of manually selected concepts significantly, but also achieves the state-of-the-art performance in the retrieval task.</Data></Cell>
    <Cell><Data ss:Type="String">Chen Sun*, USC; Chuang Gan, Tsinghua University; Ram Nevatia, University of Southern California</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1642</Data></Cell>
    <Cell><Data ss:Type="String">Simpler non-parametric methods provide as good or better results to multiple-instance learning. </Data></Cell>
    <Cell><Data ss:Type="String"> Multiple-instance learning (MIL) is a unique learning problem in which training data labels are available only for collections of objects (called bags) instead of individual objects (called instances). A plethora of approaches have been developed to solve this problem in the past years. Popular methods include the diverse density, MILIS and DD-SVM. While having been widely used, these methods, particularly those in computer vision have attempted fairly sophisticated solutions to solve certain unique and particular configurations of the MIL space.        In this paper, we analyze the MIL feature space using modified versions of traditional non-parametric techniques like the Parzen window and k-nearest-neighbor, and develop a learning approach employing distances to k-nearest neighbours of a point in the feature space. We show that these methods work as well, if not better than most recently published methods on benchmark datasets. We compare and contrast our analysis with the well-established diverse-density approach and its variants in recent literature, using benchmark datasets including the Musk, Andrews' and Corel datasets, along with a diabetic retinopathy pathology diagnosis dataset. Experimental results demonstrate that, while enjoying an intuitive interpretation and supporting fast learning, these method have the potential of delivering improved performance even for complex data arising from real-world applications.</Data></Cell>
    <Cell><Data ss:Type="String">Ragav Venkatesan*, Arizona State University; Parag Chandakkar, Arizona State University; Baoxin Li, Arizona State University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1650</Data></Cell>
    <Cell><Data ss:Type="String">Minimizing Human Effort in Interactive Tracking by Incremental Learning of Model Parameters</Data></Cell>
    <Cell><Data ss:Type="String">We address the problem of minimizing human effort in interactive tracking by learning instance specific model parameters. Determining the optimal model parameters for each instance is a critical problem in tracking. We demonstrate that by using the optimal instance specific model parameters we can achieve high precision tracking results with significantly less effort. We leverage the sequential nature of interactive tracking to formulate an efficient method for learning model parameters through a maximum margin framework. By using our method we are able to save ~50% of human effort to achieve high precision tracking results on two datasets: the VIRAT dataset and the Infant-Mother Interaction dataset that we introduce.</Data></Cell>
    <Cell><Data ss:Type="String">Arridhana Ciptadi*, Georgia Tech; James Rehg, Georgia Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1656</Data></Cell>
    <Cell><Data ss:Type="String">Robust Statistical Face Frontalization</Data></Cell>
    <Cell><Data ss:Type="String">Recently, it has been shown that excellent results can be achieved in both face landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D faces data. In this paper, we propose a novel method for joint frontal view reconstruction and landmark localization using a small set of \textbf{frontal images only}. By observing that the frontal facial image is the one with the minimum rank from all different poses, an appropriate model which is able to jointly recover the frontalized version of the face as well as the facial landmarks is devised. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix $\ell_1$ norm, is solved. The proposed method is assessed in frontal face reconstruction, face landmark localization, pose-invariant face recognition, and face verification in unconstrained conditions by conducting experiments on $8$ databases. The experimental results demonstrate the effectiveness of the proposed method.</Data></Cell>
    <Cell><Data ss:Type="String">Christos Sagonas*, Imperial College London; Yannis Panagakis, ; Stefanos Zafeiriou, Imperial College London; Maja Pantic, &quot;Imperial College London, UK&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1667</Data></Cell>
    <Cell><Data ss:Type="String">MAP Disparity Estimation using Hidden Markov Trees</Data></Cell>
    <Cell><Data ss:Type="String">A new method is introduced for stereo matching that operates on minimum spanning trees (MSTs) generated from the images. Disparity maps are represented as a collection of hidden states on MSTs, and each MST is modeled as a hidden Markov tree. An efficient recursive message-passing scheme designed to operate on hidden Markov trees, known as the upward-downward algorithm, is used to compute the maximum a posteriori (MAP) disparity estimate at each pixel. The messages processed by the upward-downward algorithm involve two types of probabilities: the probability of a pixel having a particular disparity given a set of per-pixel matching costs, and the probability of a disparity transition between a pair of connected pixels given their similarity. The distributions of these probabilities are modeled from a collection of images with ground truth disparities. Performance evaluation using the Middlebury stereo benchmark version 3 demonstrates that the proposed method ranks second and third in terms of overall accuracy when evaluated on the training and test image sets, respectively.</Data></Cell>
    <Cell><Data ss:Type="String">Eric Psota*, University of Nebraska-Lincoln; Jedrzej Kowalczuk, University of Nebraska-Lincoln; Mateusz Mittek, University of Nebraska-Lincoln; Lance Pérez, University of Nebraska</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1672</Data></Cell>
    <Cell><Data ss:Type="String">Model-Based Tracking at 300Hz using Raw Time-of-Flight Observations</Data></Cell>
    <Cell><Data ss:Type="String">Consumer depth cameras have dramatically improved  our ability to track rigid, articulated, and deformable 3D  objects in real-time. However, depth cameras have a limited  temporal resolution (frame-rate) that restricts the accuracy  and robustness of tracking, especially for fast or  unpredictable motion. In this paper, we show how to perform  model-based object tracking at an order of magnitude  higher frame-rate through simple modifications to an  off-the-shelf depth camera. We focus on phase-based timeof-  flight (ToF) sensing, which reconstructs each low framerate  depth image from a set of short exposure ‘raw’ infrared  captures. These raw captures are taken in quick succession  near the beginning of each depth frame, and differ in  the modulation of their active illumination. We make three  contributions. First, we detail how to perform model-based  tracking against these raw captures. Second, we show that  by reprogramming the camera to space the raw captures  uniformly in time, we obtain a 10x higher frame-rate, and  thereby improve the ability to track fast-moving objects.</Data></Cell>
    <Cell><Data ss:Type="String">Jan Stühmer*, Technische Universität München; Sebastian Nowozin, Microsoft Research Cambridge; Andrew Fitzgibbon, Microsoft Research; Richard Szeliski, &quot;Microsoft Research Redmond, USA&quot;; Travis Perry, Microsoft; Sunil Acharya, ; Daniel Cremers, TUM; Jamie Shotton, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1677</Data></Cell>
    <Cell><Data ss:Type="String">LEWIS: Latent Embeddings for Word Images and their Semantics</Data></Cell>
    <Cell><Data ss:Type="String">The goal of this work is to bring semantics into the tasks of text recognition and retrieval in natural images.  Although text recognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the                                                                                      In this paper, we ask the following question: can we predict semantic concepts directly from a word image, without explicitly trying to transcribe the word image or its characters at any point?  For this goal we propose a convolutional neural network (CNN) with a weighted ranking loss objective that ensures that the concepts relevant to the query image are ranked ahead of those that are not relevant.  This can also be interpreted as learning a Euclidean space where word images and concepts are jointly embedded.  This model is learned in an end-to-end manner, from image pixels to semantic concepts, using a dataset of synthetically generated word images and concepts mined from a lexical database (WordNet).  Our results show that, despite the complexity of the task, word images and concepts can indeed be associated with a high degree of accuracy.  </Data></Cell>
    <Cell><Data ss:Type="String">Albert Gordo*, Xerox XRCE; Jon Almazan, XRCE; Naila Murray, ; Florent  Perronin, &quot;Xerox, France&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1680</Data></Cell>
    <Cell><Data ss:Type="String">Learning Binary Codes for Maximum Inner Product Search</Data></Cell>
    <Cell><Data ss:Type="String">Binary coding or hashing techniques are recognized to accomplish efficient near neighbor search, and have thus attracted broad interests in the recent vision and learning studies. However, such studies have rarely been dedicated to Maximum Inner Product Search (MIPS), which plays a critical role in various vision applications. In this paper, we investigate learning binary codes to exclusively handle the MIPS problem. Inspired by the latest advance in asymmetric hashing schemes, we propose an asymmetric binary code learning framework based on inner product fitting. Specifically, two sets of coding functions are learned such that the inner products between their generated binary codes can reveal the inner products between original data vectors. Although conceptually simple, the associated optimization is very challenging due to the highly nonsmooth nature of the objective that involves sign functions. We tackle the nonsmooth optimization in an alternating manner, by which a single coding function is solved with the other fixed. Through introducing auxiliary discrete variables to replace the sign functions, the optimization procedure is made efficient. As a simplified version of the proposed binary code learning framework, we propose another objective which maximizes the correlations between the inner products of the produced binary codes and raw data vectors. In both objectives, the binary codes and coding functions are simultaneously learned without continuous relaxations, which is the key to achieving high-quality binary codes.  We evaluate the proposed method, dubbed Asymmetric Inner-product Binary Coding (AIBC), relying on the two objectives on several large-scale image datasets. Both of them are superior to the state-of-the-art binary coding and hashing methods in performing MIPS tasks. </Data></Cell>
    <Cell><Data ss:Type="String">Fumin Shen*, UESTC; Shaoting Zhang, Rutgers University; Heng Tao Shen, The University of Queensland; Wei Liu, IBM</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1681</Data></Cell>
    <Cell><Data ss:Type="String">Learning Large-Scale Automatic Image Colorization</Data></Cell>
    <Cell><Data ss:Type="String">We describe an automated method for image colorization that learns to colorize from examples.  Our method exploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaussian random field.  The coefficients of the objective function are conditioned on image features, using a random forest.  The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image.  Images are then colorized by minimizing this objective function.    We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function.  We demonstrate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.</Data></Cell>
    <Cell><Data ss:Type="String">Aditya Deshpande*, University of Illinois at Urba; Jason Rock, University of Illinois at Urbana Champaign; David Forsyth, University of Illinois at Urbana-Champaign, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1688</Data></Cell>
    <Cell><Data ss:Type="String">Robust 3D Object Detection and Tracking from Minimal Image Information</Data></Cell>
    <Cell><Data ss:Type="String">We present a  method that estimates in  real-time the 3D pose of  a known object  under practical conditions. Our method relies  only on grayscale images as depth  cameras fail  on metallic objects;  it can  handle poorly textured  objects, and  cluttered, changing environments.   The pose it predicts  degrades gracefully in  presence   of  large   occlusions.   As   a   result,  by   contrast  with   the  state-of-the-art,  our  method  is  suitable  for  practical  Augmented  Reality  applications even in  industrial environments.  Our key idea is  to first detect  parts of the target object, and to estimate the 3D pose of each part in the form  of a few control  points. The control points from each visible  part can then be  easily combined  together to estimate  the 3D pose  for the complete  object. We  demonstrate our method in several  challenging scenarios, including an Augmented  Reality application in the CERN Atlas particle detector.</Data></Cell>
    <Cell><Data ss:Type="String">Alberto Crivellaro*, EPFL; Mahdi Rad, ICG Graz; Yannick Verdie, EPFL; Kwang Yi, EPFL; Pascal  Fua, &quot;EPFL, Switzerland&quot;; Vincent Lepetit, TU Graz</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1689</Data></Cell>
    <Cell><Data ss:Type="String">Motion Trajectory Segmentation via Minimum Cost Multicuts</Data></Cell>
    <Cell><Data ss:Type="String">For the segmentation of moving objects in videos, the analysis of long-term point trajectories has been very popular recently. In this paper, we formulate the segmentation of a video sequence based on point trajectories as a minimum cost multicut problem. Unlike the commonly used spectral clustering formulation, the minimum cost multicut formulation gives natural rise to optimize not only for a cluster assignment but also for the number of clusters while allowing for varying cluster sizes. In this setup, we provide a method to create a long-term point trajectory graph with attractive and repulsive binary terms and outperform state-of-the-art methods based on spectral clustering on the FBMS-59 dataset and on the motion subtask of the VSB100 dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Margret Keuper*, University of Freiburg; Bjoern Andres, MPI Informatics; Thomas Brox, &quot;University of Freiburg, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1692</Data></Cell>
    <Cell><Data ss:Type="String">Building Dynamic Cloud Maps from the Ground Up</Data></Cell>
    <Cell><Data ss:Type="String">Satellite imagery of cloud cover is extremely important for understanding and predicting weather. In this paper, we demonstrate how this imagery can be constructed from the ground up without requiring expensive geo-stationary satellites. This is accomplished through a novel approach to approximate continental-scale cloud maps using only ground-level imagery from publicly available webcams. We collected a year's worth of satellite data and simultaneously captured geo-located, outdoor webcam images from 4388 sparsely distributed cameras across the continental USA. The satellite data is used to train a hierarchical dynamic texture model of cloud motion, and also to train 4388 regression models (one for each camera) to relate ground-level webcam data to the satellite data at the camera's location. To develop this novel application of large-scale computer vision to meteorology and remote sensing, we derive a smoothed, hierarchically regularized dynamic texture model and a natural approach to drive its system dynamics to remain consistent with measurements from the geo-located webcams. We evaluate our hierarchical model against a standard dynamic textures implementation and show that it is better able to incorporate sparse webcam measurements resulting in more accurate cloud maps. Finally, we demonstrate that our model can be successfully applied to other natural image sequences from the DynTex database, suggesting a broader applicability of our method.  </Data></Cell>
    <Cell><Data ss:Type="String">Calvin Murdock*, Carnegie Mellon University; Nathan Jacobs, University of Kentucky; Robert Pless, Washington University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1694</Data></Cell>
    <Cell><Data ss:Type="String">Multi-view convolutional neural networks for 3D shape recognition</Data></Cell>
    <Cell><Data ss:Type="String">A longstanding question in computer vision concerns the representation of 3D objects for shape recognition: should 3D objects be represented with  shape descriptors operating on their native 3D format, such as their voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors?  We address this question in the context of learning to recognize 3D objects from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the objects' rendered views independently of each other. Starting from such a network, we show that a 3D object can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. The same architecture can be applied to accurately recognize human hand-drawn sketches of objects. Recognition rates further increase when multiple views of the objects are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D object into a single and compact shape descriptor offering even better recognition performance. We conclude that a collection of 2D views can be highly informative for object categorization and is amenable to emerging CNN architectures and their derivatives.</Data></Cell>
    <Cell><Data ss:Type="String">Hang Su*, University of Massachusetts, Amherst; Subhransu Maji, University of Massachusetts Amherst; Erik Miller, ; Evangelos Kalogerakis, UMass Amherst</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1697</Data></Cell>
    <Cell><Data ss:Type="String">Wide Baseline Stereo Matching with Convex Bounded-Distortion Constraints</Data></Cell>
    <Cell><Data ss:Type="String">Finding correspondences in wide baseline setups is a challenging problem. Existing approaches have focused largely on developing better feature descriptors for correspondence and on accurate recovery of epipolar line constraints. This paper focuses on the challenging problem of finding correspondences once approximate epipolar constraints are given. We introduce a novel method that integrates a deformation model. Specifically, we formulate the problem as finding the largest number of corresponding points related by a bounded distortion map that obeys the given epipolar constraints. We show that, while the set of bounded distortion maps is not convex, the subset of maps that obey the epipolar line constraints is convex, allowing us to introduce an efficient algorithm for matching. We further utilize a robust cost function for matching and employ majorization-minimization for its optimization. Our experiments indicate that our method finds significantly more accurate maps than existing approaches.  </Data></Cell>
    <Cell><Data ss:Type="String">Meirav Galun*, ; Tal Amir, Weizmann Inst. of Science ; Tal Hassner, Open University; Ronen Basri, Weizmann Institute of Science; Yaron Lipman, Weizmann Inst. of Science </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1702</Data></Cell>
    <Cell><Data ss:Type="String">ML-MG: Multi-label Learning with Missing Labels Using a Mixed Graph</Data></Cell>
    <Cell><Data ss:Type="String">This work focuses on the problem of multi-label learning with missing labels (MLML), which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels (i.e. some of their labels are missing). To handle missing labels, we propose a unified model of label dependencies by constructing a mixed graph, which jointly incorporates (i) instance-level similarity and class co-occurrence as undirected edges and (ii) semantic label hierarchy as directed edges. Unlike most MLML methods, We formulate this learning problem transductively as a \emph{convex} quadratic matrix optimization problem that encourages training label consistency and encodes both types of label dependencies (i.e. undirected and directed edges) using quadratic terms and hard linear constraints. The alternating direction method of multipliers (ADMM) can be used to exactly and efficiently solve this problem. To evaluate our proposed method, we consider two popular applications (image and video annotation), where the label hierarchy can be derived from Wordnet. Experimental results show that our method leads to a significant improvement in performance and robustness to missing labels over the state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Baoyuan Wu*, KAUST; Siwei Lyu, SUNY Albany; Bernard Ghanem, KAUST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1704</Data></Cell>
    <Cell><Data ss:Type="String">Spatial Semantic Regularisation for Large Scale Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">Large scale object detection with thousands of classes introduces the problem of many contradicting false positive detections which have to be suppressed. Class-independent non-maximum suppression has traditionally been used for this step, but it does not scale well as the number of classes grows. Traditional non-maximum suppression does not consider label- and instance-level relationships nor allow exploitation of the spatial layout of detection proposals. We propose a new multi-class spatial regularisation method based on adaptive affinity propagation clustering which simultaneously optimizes across all categories and all proposed locations in the image to improve both location and categorization of selected detection proposals. Constraints are shared across the labels through the semantic WordNet hierarchy. Our approach proves to be especially useful in large-scale settings with thousands of classes, where spatial and semantic interactions are very frequent and only weakly supervised detectors can be built due to a lack of bounding box annotation. Detection experiments are conducted on the ImageNet and COCO dataset, and in settings with thousands of detected categories. Our method provides a significant precision improvement by reducing false positives while simultaneously improving the recall.</Data></Cell>
    <Cell><Data ss:Type="String">Damian Mrowca*, UC Berkeley; Marcus Rohrbach, UC Berkeley; Judy Hoffman, ; Ronghang Hu, Tsinghua University; Kate Saenko, &quot;University of Massachusetts Lowel, USA&quot;; Trevor Darrell, &quot;UC Berkeley, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1706</Data></Cell>
    <Cell><Data ss:Type="String">A Randomized Ensemble Approach to Industrial CT Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Tuning the models and parameters of common segmentation approaches is challenging especially in the presence of noise and artifacts. Ensemble-based techniques attempt to compensate by randomly varying models and/or parameters to create a diverse set of hypotheses, which are subsequently ranked to arrive at the best solution. However, these methods have been restricted to cases where the underlying models are well-established, e.g. natural images. In practice, it is difficult to determine a suitable base-model and the amount of randomization required. Furthermore, for multi-object scenes no single hypothesis may perform well for all objects, reducing the overall quality of the results.  This paper presents a new ensemble-based segmentation framework for industrial CT images demonstrating that comparatively simple models and randomization strategies can significantly improve the result over existing techniques. Furthermore, we introduce a per-object based ranking, followed by a consensus inference that can outperform even the best case scenario of existing hypothesis ranking approaches. We demonstrate the effectiveness of our approach using a set of noise and artifact rich CT images from baggage security and show that it significantly outperforms existing solutions in this area.</Data></Cell>
    <Cell><Data ss:Type="String">Hyojin Kim*, Lawrence Livermore National Lab; Jayaraman Jayaraman Thiagarajan, Lawrence Livermore National Laboratory; Peer-Timo Bremer, Lawrence Livermore National Laboratory</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1717</Data></Cell>
    <Cell><Data ss:Type="String">Action Localization in Videos through Context Walk</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents an efficient approach for localizing actions in videos using contextual relations learned from the training data. In contrast to sliding window approaches, the proposed approach evaluates classifier at a limited number of locations using ``context walk'' on a graph constructed on the testing video.    To reduce the complexity of the problem and due to their ability to preserve the boundaries, we begin by over-segmenting the videos into supervoxels. Context relations are learned from training videos which capture displacements of all the supervoxels to those lying within the ground truth. Then, given a testing video, we proceed with a randomly selected supervoxel and use the context information acquired during training to predict the location of action. The walk moves to a new supervoxel and the process is repeated for a few time steps. This generates a conditional distribution over the supervoxels and gives probability of an action for all the supervoxels. A Conditional Random Field is used to find contiguous segments of actions in the videos, whose confidences are then obtained using a Support Vector Machine. We validated the proposed approach on several action localization datasets and show that context can be extremely useful for this task resulting in significantly fewer evaluations of the classifier.</Data></Cell>
    <Cell><Data ss:Type="String">Khurram Soomro*, University of Central Florida; Haroon Idrees, UCF; Mubarak Shah, &quot;University of Central Florida, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1728</Data></Cell>
    <Cell><Data ss:Type="String">Zero-Shot Learning via Semantic Similarity Embedding</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (e.g. attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.</Data></Cell>
    <Cell><Data ss:Type="String">Ziming Zhang*, Boston University; Venkatesh Saligrama, Boston University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1738</Data></Cell>
    <Cell><Data ss:Type="String">Bayesian Model Adaptation for Crowd Counts</Data></Cell>
    <Cell><Data ss:Type="String">The problem of transfer learning is considered in the domain of crowd counting. A solution based on Bayesian model adaptation of Gaussian processes is proposed. This is shown to produce intuitive model updates, which are tractable, and lead to an adapted model (predictive distribution) that accounts for all information in both training and adaptation data. The new adaptation procedure achieves significant gains over previous approaches, based on multi-task learning, while requiring much less computation to deploy. This makes it particularly suited for the problem of expanding the capacity of crowd counting camera networks. A large video dataset for the evaluation of adaptation approaches to crowd counting is also introduced. This contains a number of adaptation tasks, involving information transfer across video collected by 1) a single camera under different scene conditions (different times of the day) and 2) video collected from different cameras. Evaluation of the proposed model adaptation procedure in this dataset shows good performance in realistic operating conditions.</Data></Cell>
    <Cell><Data ss:Type="String">Bo Liu*, UCSD; Nuno Vasconcelos, UC San Diego, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1759</Data></Cell>
    <Cell><Data ss:Type="String">Monocular Object Instance Segmentation and Depth Ordering with CNNs</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we tackle the problem of instance level segmentation and depth ordering from a single monocular image. Towards this goal, we take advantage of convolutional neural nets and train them to directly predict instance level segmentations where the instance ID encodes   depth ordering from large image patches. Importantly, to train the deep network we only employ weak labels in the form of 3D bounding boxes. To provide a coherent single explanation of an image we develop a Markov random field which takes as input the predictions of convolutional nets applied at overlapping patches of different resolutions as well as the output of a connected component algorithm and predicts very accurate instance level segmentation and depth ordering. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show very good performance on both tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Ziyu Zhang*, University of Toronto; Alexander Schwing, University of Toronto; Raquel Urtasun, University of Toronto; Sanja Fidler, University of Toronto</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1768</Data></Cell>
    <Cell><Data ss:Type="String">Compression Artifacts Reduction by Deep Convolutional Network</Data></Cell>
    <Cell><Data ss:Type="String">Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar “easy to hard” idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use cases (i.e. Twitter). In addition, we show that our method can be applied as preprocessing to facilitate other low-level vision routines when they take compressed images as input.</Data></Cell>
    <Cell><Data ss:Type="String">Chao Dong, ; Yubin Deng, The Chinese University of Hong Kong; Chen-Change Loy*, the Chinese University of Hong Kong; Xiaoou Tang, The Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1772</Data></Cell>
    <Cell><Data ss:Type="String">Amodal Completion and Size Constancy in Natural  Scenes</Data></Cell>
    <Cell><Data ss:Type="String">We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completion to infer veridical sizes in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scaling ambiguities and we demonstrate qualitative results on challenging real-world scenes.</Data></Cell>
    <Cell><Data ss:Type="String">Abhishek Kar*, UC Berkeley; Shubham Tulsiani, UC Berkeley; Joao Carreira, UC Berkeley; Jitendra Malik, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1773</Data></Cell>
    <Cell><Data ss:Type="String">RGB-W: When Vision Meets Wireless</Data></Cell>
    <Cell><Data ss:Type="String">Inspired by the recent success of RGB-D cameras, we propose the enrichment of RGB data with an additional ``quasi-free&quot; modality, namely, the wireless signal (e.g., wifi or Bluetooth) emitted by individuals' cell phones, referred to as RGB-W. The received signal strength acts as a rough proxy for depth and a reliable cue on their identity. Although the measured signals are highly noisy (more than 2m average localization error), we demonstrate that the combination of visual and wireless data significantly improves the localization accuracy.     We introduce a novel image-driven representation of wireless data which embeds all received signals onto a single image. We then indicate the ability of this additional data to (i) locate persons within a sparsity-driven framework and to (ii) track individuals with a new confidence measure on the data association problem. Our solution outperforms existing localization methods by a significant margin. It can be applied to the millions of currently installed RGB cameras to better analyze human behavior and offer the next generation of high-accuracy location-based services. </Data></Cell>
    <Cell><Data ss:Type="String">Alexandre Alahi*, Stanford University; Albert Haque, Stanford; Fei-Fei Li, Stanford University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1784</Data></Cell>
    <Cell><Data ss:Type="String">Semi-Supervised Normalized Cuts for Image Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Since its introduction as a powerful graph-based method for image segmentation, the Normalized Cuts algorithm has been generalized to incorporate expert knowledge about how certain pixels or regions should be grouped, or how the resulting segmentation should be biased to be correlated with priors. In this paper, we generalize Normalized Cuts to not only handle must-link constraints on how certain pixels should be grouped, but also cannot-link constraints on how other pixels should be separated into different groups. As opposed to previous work that enables hard cannot-link constraints, our formulation allows both sets of constraints to be handled in a soft manner, enabling the user to tune the degree to which the constraints are satisfied. Furthermore, an approximate spectral solution to our constrained problem exists without requiring explicit construction of a large, dense matrix; hence, computation time is comparable to that of unconstrained Normalized Cuts.  </Data></Cell>
    <Cell><Data ss:Type="String">Selene Chew, Rochester Institute of Technology; Nathan Cahill*, Rochester Institute of Technol</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1799</Data></Cell>
    <Cell><Data ss:Type="String">PIEFA: Personalized Incremental and Ensemble Face Alignment in the Wild</Data></Cell>
    <Cell><Data ss:Type="String">Face alignment, especially on real-time or large-scale sequential images, is a challenging task with broad applications. Both generic and joint alignment approaches have been proposed with varying degrees of success. However, many generic methods are heavily sensitive to initializations and usually rely on offline-trained static models, which limit their performance on sequential images with extensive variations. On the other hand, joint methods are restricted to offline applications, since they require all frames to conduct batch alignment. To address these limitations, we propose to exploit incremental learning for personalized ensemble alignment. We sample multiple initial shapes to achieve image congealing within one frame, which enables us to incrementally conduct ensemble alignment by group-sparse regularized rank minimization. At the same time, personalized modeling is obtained by subspace adaptation under the same incremental framework, while correction strategy is used to alleviate model drifting. Experimental results on multiple controlled and in-the-wild databases demonstrate the superior performance of our approach compared with state-of-the-arts in terms of fitting accuracy and efficiency.</Data></Cell>
    <Cell><Data ss:Type="String">Xi Peng*, Rutgers University; Yu Yang, ; Shaoting Zhang, Rutgers University; Dimitris Metaxas, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1803</Data></Cell>
    <Cell><Data ss:Type="String">Understanding Everyday Hands in Action from RGB-D Images</Data></Cell>
    <Cell><Data ss:Type="String">We analyze functional object manipulations, focusing on fine-grained hand-object interactions. To do so, we make use of a recently developed fine-grained taxonomy covering everyday interactions including grasps as well as other interactions including touching and pulling. We introduce a large dataset of 12000 RGB-D images covering 71 everyday grasps in natural interactions. Our dataset is different from past work (usually addressed from a robotics perspective) in terms of its scale, diversity, and combination of RGB and depth data. From a computer-vision perspective, our dataset allows for exploration of contact and force prediction (crucial concepts in functional grasp analysis) from perceptual cues. We present extensive experimental results with state-of-the-art baselines, illustrating the role of segmentation, object context, and 3D-understanding in functional grasp analysis.</Data></Cell>
    <Cell><Data ss:Type="String">Gregory Rogez*, UC Irvine; James Supancic III, UC Irvine; Deva Ramanan, UC Irvine</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1809</Data></Cell>
    <Cell><Data ss:Type="String">Multimodal Convolutional Neural Networks for Matching Image and Sentence</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and sentence. The matching CNN composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results on benchmark databases of bidirectional image and sentence retrieval demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. Specifically, our proposed m-CNNs for bidirectional image and sentence retrieval on Fickr8K and Flick30K databases significantly outperform the state-of-the-art approaches.</Data></Cell>
    <Cell><Data ss:Type="String">Lin Ma*, Huawei Noah's Ark Lab; Zhengdong Lu, ; Lifeng Shang, ; Hang Li, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1813</Data></Cell>
    <Cell><Data ss:Type="String">Per-Sample Kernel Adaptation for Visual Recognition and Grouping</Data></Cell>
    <Cell><Data ss:Type="String">Object, action, or scene representations that are corrupted by noise significantly impair the performance of visual recognition. Typically, partial occlusion, clutter, or excessive articulation affects only a subset of all feature dimensions and, most importantly, different dimensions are corrupted in different samples. Nevertheless, the common approach to this problem in feature selection and kernel methods is to down-weight or eliminate entire training samples or the same dimensions of all samples. Thus, valuable signal is lost, resulting in suboptimal classification.    Our goal is, therefore, to adjust the contribution of individual feature dimensions when comparing any two samples and computing their similarity. Consequently, per-sample selection of informative dimensions is directly integrated into kernel computation. The interrelated problems of learning the parameters of a kernel classifier and determining the informative components of each sample are then addressed in a joint objective function. The approach can be integrated into the learning stage of any kernel-based visual recognition problem and it does not affect the computational performance in the retrieval phase. Experiments on diverse challenges of action recognition in videos and indoor scene classification show the general applicability of the approach and its ability to improve learning of visual representations.</Data></Cell>
    <Cell><Data ss:Type="String">Borislav Antic*, Heidelberg University; Bjorn Ommer, Heidelberg</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1826</Data></Cell>
    <Cell><Data ss:Type="String">Structural Kernel Learning for Large Scale Multiclass Object Co-Detection</Data></Cell>
    <Cell><Data ss:Type="String">Exploiting contextual relationships across images has recently proven key to improve object detection. The resulting object co-detection algorithms, however, fail to exploit the correlations between multiple classes and, for scalability reasons are limited to modeling object instance similarity with relatively low-dimensional hand-crafted features. Here, we address the problem of multiclass object co-detection for large scale datasets. To this end, we formulate co-detection as the joint multiclass labeling of object candidates obtained in a class-independent manner. To exploit the correlations between objects, we build a fully-connected CRF on the candidates, which explicitly incorporates both geometric layout relations across object classes and similarity relations across multiple images. We then introduce a structural boosting algorithm that lets us exploits rich, high-dimensional deep network features to learn object similarity within our fully-connected CRF. Our experiments on PASCAL VOC 2007 and 2012 evidences the benefits of our approach over object detection with RCNN, single-image CRF methods and state-of-the-art co-detection algorithms.</Data></Cell>
    <Cell><Data ss:Type="String">Zeeshan Hayder*, ANU/NICTA; Xuming He, NICTA; Mathieu Salzmann, NICTA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1828</Data></Cell>
    <Cell><Data ss:Type="String">An NMF perspective on Binary Hashing</Data></Cell>
    <Cell><Data ss:Type="String">The pervasiveness of massive data repositories  has led to much interest in efficient methods for  indexing, search, and retrieval. For image data,  a rapidly developing body of work for these applications shows impressive performance with methods that broadly fall under the umbrella term of Binary Hashing. Given a distance matrix, a binary hashing algorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the original distances. The formulation is non-convex— so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective that is numerically more tractable. In this paper, we first derive an Augmented Lagrangian approach to optimize the standard binary Hashing objective (i.e.,maintain fidelity with a given distance matrix). With appropriate step sizes, we find that this scheme already yields results that match or substantially outperform state of the art methods on most benchmarks used in the literature. Then, to allow the model to scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm — whose parallelization properties are exploited to obtain a fast GPU based implementation. We give a probabilistic analysis of our initialization scheme and present a range of experiments to show that the method is simple to implement and competes favorably with available methods (both for optimization  and generalization).</Data></Cell>
    <Cell><Data ss:Type="String">Lopamudra Mukherjee*, University of Wisc Whitewater; Sathya Ravi, University of Wisconsin Madiso; Vamsi Ithapu, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1831</Data></Cell>
    <Cell><Data ss:Type="String">Interactive Visual Hull Refinement for Specular and Transparent Object Surface Reconstruction</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we present a method of using standard multi-view images for 3D surface reconstruction of non-Lambertian objects. We extend the original visual hull concept to incorporate 3D cues presented by internal occluding contours, i.e., occluding contours that are inside the object’s silhouettes. We discovered that these internal contours, which are results of convex parts on an object’s surface, can lead to a tighter fit than the original visual hull. We formulated a new visual hull refinement scheme – Locally Convex Carving that can completely reconstruct concavity caused by two or more intersecting convex surfaces.  In addition we develop a novel approach for contour tracking given labeled contours in sparse key frames. It is designed specifically for highly specular or transparent objects, for which assumptions made in traditional contour detection/tracking methods, such as highest gradient and stationary texture edges, are no longer valid. It is formulated as an energy minimization function where several novel terms are developed to increase robustness.  Based on the two core algorithms, we have developed an interactive system for 3D modeling. We have validated our system, both quantitatively and qualitatively, with four datasets of different object materials. Results show that we are able to generate visually pleasing models for very challenging cases.</Data></Cell>
    <Cell><Data ss:Type="String">Xinxin Zuo*, Northwestern Polytechnical University; University of Kentucky; Chao Du, University of Kentucky; Sen Wang, Northwestern Polytechnical University; Jiangbin Zheng, Northwestern Polytechnical University; Ruigang Yang, University of Kentucky</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1839</Data></Cell>
    <Cell><Data ss:Type="String">Learning Visual Clothing Style with Heterogeneous Dyads from Co-occurrence</Data></Cell>
    <Cell><Data ss:Type="String">With the rapid proliferation of smart mobile devices, users now take millions of photos every day. These include large numbers of clothing and accessory images. We would like to answer questions like `What outfit goes well with this pair of shoes?' To answer these types of questions, one has to go beyond learning visual similarity and learn a visual notion of compatibility across categories. In this paper, we propose a novel learning framework to help answer these types of questions. The main idea of this framework is to learn a feature transformation from images of items into a latent space that expresses compatibility. For the feature transformation, we use a Siamese Convolutional Neural Network (CNN) architecture, where training examples are pairs of items that are either compatible or incompatible.  We model compatibility based on co-occurrence in large-scale user behavior data; in particular co-purchase data from Amazon.com. To learn cross-category fit, we introduce a strategic method to sample training data, where pairs of items are heterogeneous dyads, i.e., the two elements of a pair belong to different high-level categories. While this approach is applicable to a wide variety of settings, we focus on the representative problem of learning compatible clothing style. Our results indicate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that go well together.</Data></Cell>
    <Cell><Data ss:Type="String">Andreas Veit*, Cornell University; Balazs Kovacs, Cornell University; Sean Bell, Cornell University; Julian McAuley, University of California, San Diego; Kavita Bala, Cornell University; Serge Belongie, Cornell</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1841</Data></Cell>
    <Cell><Data ss:Type="String">Multiple-hypotheses affine region estimation by anisotropic LoG filter</Data></Cell>
    <Cell><Data ss:Type="String">We propose a method for estimating multiple-hypotheses affine region from a keypoint using an anisotropic Laplacian-of-Gaussian (LoG) filter.  Although conventional affine region detectors, such as Hessian/Harris-Affine, iteratively find a particular affine region that well fit to a given image patch, such iterative search strongly depends on an initial point as a result of incorrect matching.  To avoid this problem, it is natural to allow multiple detection from a single keypoint. Our finding is that responses of all the possible anisotropic LoG filters can be efficiently computed by factorizing them in a similar manner to Spectral SIFT. More specifically, we derived following two points: (1) a large number of LoG filters that are densely sampled in parameter space are reconstructed by weighted combination of a limited number of representative filters called by eigenfilters, using singular value decomposition (SVD) in advance. (2)The reconstructed filter responses on the sampled parameters can be interpolated to continuous representation by series of proper functions, which results in efficient multiple extrema search in the continuous space.  Experiments revealed that our method provided the higher repeatability than the conventional methods.</Data></Cell>
    <Cell><Data ss:Type="String">Takahiro Hasegawa*, Chubu University; Mitsuru Ambai, Denso IT Laboratory; Gou Koutaki, Kumamoto University; Kohta Ishikawa, Denso IT Laboratory; Yuji Yamauchi, Chubu university; Takayoshi Yamashita, Chubu University; Hironobu Fujiyoshi, Chubu University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1847</Data></Cell>
    <Cell><Data ss:Type="String">Learning where to position parts in 3D</Data></Cell>
    <Cell><Data ss:Type="String">A common issue in deformable object detection is find-  ing a good way to position the parts. This issue is even  more outspoken when considering detection and pose es-  timation for 3D objects, where parts should be placed in  a three-dimensional space. Some methods extract the  3D shape of the object from 3D CAD models. This lim-  its their applicability to categories for which such models  are available. Others represent the object with a pre-  defined and simple shape (e.g. a cuboid). This extends the  applicability of the model, but in many cases the pre-defined  shape is too simple to properly represent the object in 3D.  In this paper we propose a new method for the detection  and pose estimation of 3D objects, that does not use any  3D CAD model or other 3D information. Starting from a  simple and general 3D shape, we learn in a weakly super-  vised manner the 3D part locations that best fit the training  data. As this method builds on a iterative estimation of the  part locations, we introduce several speedups to make the  method fast enough for practical experiments. We evaluate  our model for the detection and pose estimation of faces and  cars. Our method obtains results comparable with the state  of the art, it is faster than most of the other approaches and  does not need any additional 3D information.</Data></Cell>
    <Cell><Data ss:Type="String">Marco Pedersoli*, Leuven University; Tinne Tuytelaars, KU Leuven</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1852</Data></Cell>
    <Cell><Data ss:Type="String">Globally Linear Approximation to Nonlinear Learning for Visual Tracking</Data></Cell>
    <Cell><Data ss:Type="String">Due to appearance changes caused by occlusion, deformation, varied illumination etc., classifiers for visual tracking are nonlinear essentially. Starting from the theory of globally linear approximation to nonlinear function, we introduce a principled method to jointly learn a nonlinear classifier and a dictionary for visual tracking in a semi-supervised sparse coding manner. This forms an obvious distinction from most sparse coding based discriminative tracking algorithms which usually take two-stage learning strategy, i.e., first learning dictionary in an unsupervised way then followed by classifier learning. The treatment of separating dictionary learning from classifier learning may not be able to  produce both descriptive and discriminative dictionary for visual tracking. By contrast, our proposed method can produce a dictionary  that not only fully reflects the true data space manifold structure, but also contains discriminative capacity. An optimization method is presented to obtain optimal dictionary, sparse codes, and classifier in an iterative way. Experiments on challenging video sequences have shown good tracking performance of our method when compared with other state-of-the-art algorithms.</Data></Cell>
    <Cell><Data ss:Type="String">Bo Ma*, Beijing Institute of Tech.; Hongwei Hu, Beijing Institute of Tech.; Yuping Zhang, Beijing Institute of Tech.; Jianbing Shen, Beijing Institute of Technolog; Fatih Porikli, ANU / NICTA, Australia</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1871</Data></Cell>
    <Cell><Data ss:Type="String">Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models</Data></Cell>
    <Cell><Data ss:Type="String">The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 248k coreference chains linking mentions of the same entities in images, as well as 284k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for bidirectional image-sentence retrieval and text-to-image coreference. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging. Our dataset, code, and baseline results will be made publicly available.</Data></Cell>
    <Cell><Data ss:Type="String">Bryan Plummer*, UIUC; Liwei Wang, UIUC; Chris Cervantes, ; Juan Caicedo, Konrad Lorenz University; Julia Hockenmaier, ; Svetlana Lazebnik, UIUC</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1873</Data></Cell>
    <Cell><Data ss:Type="String">Context Aware Online Adaptation of Activity Recognition Models</Data></Cell>
    <Cell><Data ss:Type="String">Activity recognition in video has recently benefited from the use of the context e.g., inter-relationships among the activities and objects. However, these approaches require data to be labeled and entirely available at the outset. In contrast, we formulate a continuous learning framework for context aware activity recognition from unlabeled video data which has two distinct advantages over most existing methods. First,  we propose a novel active learning technique which not only exploits the informativeness of the individual activity instances but also utilizes their contextual information during the query selection process; this leads to significant reduction in expensive manual annotation effort. Second, the learned models can be adapted online as more data is available. We formulate a conditional random field (CRF) model that encodes the context and devise an information theoretic approach that utilizes entropy and mutual information of the nodes to compute the set of most informative query instances, which need to be labeled by a human. These labels are combined with graphical inference techniques for incrementally updating the model as new videos come in. Experiments on four challenging datasets demonstrate that our framework achieves superior performance with significantly less amount of manual labeling.</Data></Cell>
    <Cell><Data ss:Type="String">MAHMUDUL HASAN*, UC Riverside; Amit Roy-Chowdhury, UC Riverside</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1884</Data></Cell>
    <Cell><Data ss:Type="String">Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we address three different computer vision tasks using a single  basic architecture:  depth prediction, surface normal estimation, and semantic  labeling.  We use a multiscale convolutional network that is able to adapt  easily to each task using only small modifications, regressing from the input  image to the output map directly.  Our method progressively refines predictions  using a sequence of scales, and captures many image details without any  superpixels or low-level segmentation.  We achieve state-of-the-art performance  on benchmarks for all three tasks.</Data></Cell>
    <Cell><Data ss:Type="String">David Eigen*, New York University; Rob Fergus, New York University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1887</Data></Cell>
    <Cell><Data ss:Type="String">Self-Occlusions and Disocclusions in Causal Video Object Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">We propose a method to detect disocclusion in video sequences of three-dimensional scenes and to partition the unoccluded regions into objects, defined by coherent deformation corresponding to surfaces in the scene. Our method infers deformation fields that are piecewise smooth by construction without the need for an explicit regularizer and the associated choice of weight. It then partitions the disoccluded region and groups its components with objects by leveraging on the complementarity of motion and appearance cues: Where appearance changes within an object, motion can usually be reliably inferred and used for grouping. Where appearance is close to constant, it can be used for grouping directly. We integrate both cues within an energy minimization framework, formalize prior assumptions explicit into the energy, and propose a numerical scheme to partition a long video sequence into objects.</Data></Cell>
    <Cell><Data ss:Type="String">Yanchao Yang*, UCLA; Ganesh Sundaramoorthi, KAUST; Stefano Soatto, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1892</Data></Cell>
    <Cell><Data ss:Type="String">Action Recognition by Hierarchical Mid-level Action Elements</Data></Cell>
    <Cell><Data ss:Type="String">Realistic videos of human actions exhibit rich spatiotemporal  structures at multiple levels of granularity: an action  can always be decomposed into multiple finer-grained elements  in both space and time. To capture this intuition, we  propose to represent videos by a hierarchy of mid-level action  elements (MAEs), where each MAE corresponds to an  action-related spatiotemporal segment in the video. We introduce  an unsupervised method to generate this representation  from videos. Our method is capable of distinguishing  action-related segments from background segments and  representing actions at multiple spatiotemporal resolutions.  Given a set of spatiotemporal segments generated from  the training data, we introduce a discriminative clustering  algorithm that automatically discovers MAEs at multiple  levels of granularity. We develop structured models  that capture a rich set of spatial, temporal and hierarchical  relations among the segments, where the action label and  multiple levels of MAE labels are jointly inferred. The proposed  model achieves state-of-the-art performance in multiple  action recognition benchmarks. Moreover, we demonstrate  the effectiveness of our model in real-world applications  such as action recognition in large-scale untrimmed  videos and action parsing.</Data></Cell>
    <Cell><Data ss:Type="String">Yuke Zhu, Stanford University; Amir Zamir, Stanford University; Silvio Savarese, Stanford University, USA; Tian Lan*, Amazon</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1894</Data></Cell>
    <Cell><Data ss:Type="String">Selecting Relevant Web Trained Concepts for Automated Event Retrieval</Data></Cell>
    <Cell><Data ss:Type="String">Complex event retrieval is a challenging research problem, especially when no training videos are available. An alternative to collecting training videos is to train a large semantic concept bank a priori. Given a text description of an event, event retrieval is performed by selecting concepts linguistically related to the event description and fusing the concept responses on unseen videos. However, defining an exhaustive concept lexicon and pre-training it requires vast computational resources. Therefore, recent approaches automate concept discovery and training by leveraging large amounts of weakly annotated web data. Compact visually salient concepts are automatically obtained by the use of concept pairs or, more generally, n-grams. However, not all visually salient n-grams are necessarily useful for an event query&#45;-some combinations of concepts may be visually compact but irrelevant&#45;-and this drastically affects performance. We propose an event retrieval algorithm that constructs pairs of automatically discovered concepts and then prunes those concepts that are unlikely to be helpful for retrieval. Pruning depends both on the query and on the specific video instance being evaluated. Our approach also addresses calibration and domain adaptation issues that arise when applying concept detectors to unseen videos. We demonstrate large improvements over other vision based systems on the TRECVID MED 13 dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Bharat Singh*, University of Maryland, Colleg; Xintong Han, Univeristy of Maryland; Zhe Wu, University of Maryland, College Park; Vlad Morariu, University of Maryland; Larry Davis, &quot;University of Maryland, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1896</Data></Cell>
    <Cell><Data ss:Type="String">Action Detection by Implicit Intentional Motion Clustering</Data></Cell>
    <Cell><Data ss:Type="String">Explicitly using human detection and pose estimation has found limited success in action recognition problems.  This may be due to the complexity in the articulated motion human exhibit.  Yet, we know that action requires an actor and intention.  This paper hence seeks to understand the patiotemporal properties of intentional movement and how to capture such intentional movement without relying on challenging human detection and tracking.  We conduct a   quantitative analysis of intentional movement, and our findings motivate a new approach for implicit intentional movement extraction that is based on spatiotemporal trajectory clustering by leveraging the properties of intentional movement.  The intentional movement clusters are then used as action proposals for detection.  Our results on three action detection benchmarks indicate the   relevance of focusing on intentional movement for action detection; our method significantly outperforms the state of the art on the challenging MSR-II multi-action video benchmark.</Data></Cell>
    <Cell><Data ss:Type="String">Wei Chen*, Suny at Buffalo; Jason Corso, University of Michigan, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1901</Data></Cell>
    <Cell><Data ss:Type="String">Beyond Covariance: Feature Representation with Nonlinear Kernel matrices</Data></Cell>
    <Cell><Data ss:Type="String">Covariance matrix has recently received increasing attention in computer vision by leveraging Riemannian geometry of symmetric positive-definite (SPD) matrices. Originally proposed as a region descriptor, it has now been used as a generic representation in various recognition tasks. However, covariance matrix has shortcomings due to its sensitivity to the number of samples, limited capability in modelling complicated feature relationship, and a fixed form of representation. This paper argues that more appropriate SPD-matrix-based representations shall be explored to achieve better recognition performance. It proposes an open framework to use the kernel matrix over feature dimensions as a generic representation and discusses its properties and advantages. The proposed framework significantly elevates covariance representation to the unlimited opportunities provided by this new representation. Experimental study shows that this representation consistently outperforms its covariance counterpart on different vision tasks. In particular, it achieves significant improvement on skeleton-based human action recognition, demonstrating the state-of-the-art performance over both the covariance and the existing non-covariance representations.</Data></Cell>
    <Cell><Data ss:Type="String">Lei Wang*, Wollongong University; Jianjia Zhang, university of wollongong; Luping Zhou, university of wollongong; Chang Tang, Tianjin University; Wanqing Li, University of Wollongong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1922</Data></Cell>
    <Cell><Data ss:Type="String">3D Indoor Scene Labelling with Hierarchical Higher-order Regression Forest Fields</Data></Cell>
    <Cell><Data ss:Type="String">This paper addresses the problem of semantic segmentation of 3D indoor scenes reconstructed from RGB-D images. Traditionally label prediction for 3D points is tackled  by employing graphical models that capture scene features and complex relations between different class labels. However, the existing work is restricted to pairwise conditional random fields, which are insufficient to encode rich scene context. In this work we propose models with higher-order potentials to describe complex relational information from the 3D scenes. Specifically, we relax the labelling problem to a regression, and generalize the higher-order associative Pn Potts model to a new family of arbitrary higher-order models based on regression forests. We show that these, like  the robust Pn models, can still be decomposed into the sum of pairwise terms by introducing auxiliary variables. Moreover, our proposed higher-order models also permit extension to hierarchical random fields, which allows for the integration of scene context and features computed at different scales. Our potential functions are constructed based on regression forests encoding Gaussian densities that admit efficient inference. The parameters of our model are learnt from training data using a structured learning approach. Results on two datasets show clear improvements over the state-ofthe-  art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Trung Pham*, The University of Adelaide; Ian Reid, &quot;University of Adelaide, Australia&quot;; Stephen Gould, Australian National University; Yasir Latif, University of Adelaide</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1929</Data></Cell>
    <Cell><Data ss:Type="String">Multiresolution hierarchy co-clustering for semantic segmentation in sequences with small variations </Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a co-clustering technique that, given a collection of images and their hierarchies, clusters nodes from these hierarchies to obtain a coherent multiresolution representation of the image collection. We formalize the co-clustering as Quadratic Semi-Assignment Problem and solve it   with a linear programming relaxation approach that makes effective use of information from hierarchies. Initially, we address the problem of generating an optimal, coherent partition per image and, afterwards, we extend this method to a multiresolution framework. Finally, we particularize this framework to an iterative multiresolution video segmentation algorithm in sequences with small variations. We evaluate the algorithm on the Video Occlusion/Object Boundary Detection Dataset, showing that it produces state-of-the-art results in these scenarios.  </Data></Cell>
    <Cell><Data ss:Type="String">David Varas*, UPC; Ferran Marques, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1943</Data></Cell>
    <Cell><Data ss:Type="String">AttentionNet: Aggregating Weak Directions for Accurate Object Detection</Data></Cell>
    <Cell><Data ss:Type="String">We present a new detection method using a convolutional neural network (CNN), named AttentionNet. We cast an object detection problem as an iterative classification problem, which is the most suitable form of a CNN. AttentionNet provides quantized weak directions pointing a target object and iterative evaluations of AttentionNet converges to an accurate object boundary box. Since AttentionNet is a unified network for object detection, it detects objects without any separated models from initial object proposal to accurate object localization. We evaluate AttentionNet by a human detection task and achieve the state-of-the-art performance of 65\% (AP) on PASCAL VOC 2007/2012 with an 8-layered architecture. </Data></Cell>
    <Cell><Data ss:Type="String">Donggeun Yoo*, KAIST; Sunggyun Park, KAISTR; Joon-Young Lee, KAIST; Anthony Paek, Cldi Inc.; In So Kweon, KAIST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1960</Data></Cell>
    <Cell><Data ss:Type="String">Objects2action: Classifying and localizing actions without any video example</Data></Cell>
    <Cell><Data ss:Type="String">The goal of this paper is to recognize actions in video  without the need for examples. Different from the traditional zero-shot approaches we do not demand the design and specification of attribute classifiers and class-to-attribute mappings to allow for transfer from seen classes to unseen classes. Our key contribution is objects2action, a semantic word embedding that is spanned by a skip-gram model of thousands of object categories. Action labels are assigned to an object encoding of unseen video based on a convex combination of action and object affinities. Our semantic embedding has three main characteristics to accommodate for the specifics of actions. First, we propose a mechanism to exploit multiple-word descriptions of actions  and objects. Second, we incorporate the automated selection of the most responsive objects per action. And finally, we demonstrate how to extend our zero-shot approach to the spatio-temporal localization of actions in video. Experiments on four action datasets demonstrate the potential of our approach.  </Data></Cell>
    <Cell><Data ss:Type="String">Mihir Jain*, University of Amsterdam; Jan van Gemert, University of Amsterdam; Thomas Mensink, University of Amsterdam; Cees Snoek, University of Amsterdam</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1961</Data></Cell>
    <Cell><Data ss:Type="String">Query Adaptive Similarity Measure for RGB-D Object Recognition</Data></Cell>
    <Cell><Data ss:Type="String">This paper studies the problem of improving the top-1 accuracy of RGB-D object recognition. Despite of the impressive top-5 accuracies achieved by existing methods, their top-1 accuracies are not very satisfactory. The reasons are in two-fold: (1) existing similarity measures are sensitive to object pose and scale changes, as well as intra-class variations; and (2) effectively fusing RGB and depth cues is still an open problem. To address these problems, this paper first proposes a new similarity measure based on dense matching, through which objects in comparison are warped and aligned, to better tolerate variations. Towards RGB and depth fusion, we argue that a constant and golden weight doesn't exist. The two modalities have varying contributions when comparing objects from different categories. To capture such a dynamic characteristic, a group of matchers equipped with various fusion weights is constructed, to explore the responses of dense matching under different fusion configurations. All the response scores are finally merged following a learning-to-combination way, which provides quite good generalization ability in practice. The proposed approach win the best results on several public benchmarks, e.g., achieves 92.7% top-1 test accuracy on the Washington RGB-D object dataset, with a 5.1% improvement over the state-of-the-art.</Data></Cell>
    <Cell><Data ss:Type="String">Yanhua Cheng*, Microsoft Research; CASIA; Rui Cai, Microsoft Research; Zhiwei Li, Microsoft Research; Xin Zhao, casia; Kaiqi Huang, National Laboratory of Pattern Recognition; Yong Rui, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1963</Data></Cell>
    <Cell><Data ss:Type="String">Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks</Data></Cell>
    <Cell><Data ss:Type="String">Visual appearance and motion dynamics of humans and objects comprise the videos. Most of the previous methods focus on extracting features from the spatial-temporal cuboids while these spatial-temporal action patterns coupling visual appearance and motion dynamics require an order of magnitude more training samples than their 2D spatial counterparts do. In this paper, we consider alternative deep architectures, called Factorized Spatio-Temporal Convolutional Neural Networks (FstCN) which could ideally learn semantically related features characterizing both spatial appearance and temporal motions, while replying on insufficient training video data. FstCN factorizes the straightforward 3D spatial-temporal convolution kernel learning as a sequential process of learning 2D spatial kernels in lower network layers (termed Spatial Convolutional Layers or SCLs) followed by learning 1D temporal kernels in upper network layers (termed Temporal Convolutional Layers or TCLs). A novel transformation and permutation operation is applied, facilitating the TCL to learn the robust features based representations. The whole architecture can be trained by back-propagation. Besides, a training and inference strategy based on sampling multiple video clips from a given action video sequence is proposed, in order to address the issue of sequence alignment. Our classification results on benchmark dataset, UCF-101, HMDB-51 achieve the comparable performance, if not better, to the state of the art even no additional data is used. Particularly, even on the UCF-101, we only have 0.1% performance gain, it means dozens of videos are recognized correctly.</Data></Cell>
    <Cell><Data ss:Type="String">LIN SUN*, HKUST; Kui Jia, University of Macau; Dit-Yan Yeung, Hong Kong University of Science and Technology; Bertram Shi, HKUST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1979</Data></Cell>
    <Cell><Data ss:Type="String">Fine-Grained Change Detection of Misaligned Scenes with Varied Illuminations</Data></Cell>
    <Cell><Data ss:Type="String">Detecting fine-grained subtle changes among a scene is critically important in practice. Previous change detection methods, focusing on detecting large-scale significant changes, cannot do this well. This paper proposes a feasible end-to-end approach to this challenging problem. We start from active camera relocation that quickly relocates camera to nearly the same pose and position of the last time observation. To guarantee detection sensitivity and accuracy of detailed changes, in an observation, we capture a group of images under multiple illuminations, which need only to be roughly aligned to the last time lighting conditions. Given two times observations, we formulate fine-grained change detection as a joint optimization problem of three related factors, i.e., normal-aware lighting difference, camera geometry correction flow, and real scene change mask. We solve the three factors in a coarse-to-fine manner and achieve reliable change decision by rank minimization. We build three real-world datasets to benchmark fine-grained change detection of misaligned scenes under varied multiple lighting conditions. Extensive experiments show the superior performance of our approach over state-of-the-art change detection methods and its ability to distinguish real scene changes from false ones caused by lighting variations.</Data></Cell>
    <Cell><Data ss:Type="String">Wei Feng*, Tianjin University; Fei-Peng Tian, Tianjin University; Qian Zhang, Tianjin University; Nan Zhang, Tianjin University; Liang Wan, Tianjin University; Jizhou Sun, Tianjin University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1987</Data></Cell>
    <Cell><Data ss:Type="String">A Wavefront Marching Method for Solving the Eikonal Equation on Cartesian Grids</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a new wavefront propagation method for dealing with the classic Eikonal equation. While classic Dijkstra-like graph-based techniques achieve the solution in O(M log M), they do not approximate the unique physically relevant solution very well. Fast Marching Methods (FMM) were created to efficiently solve the continuous problem. The proposed approximation tries to maintain the complexity, in order to make the algorithm useful in a wide range of contexts. The key idea behind our method is the creation of 'mini wave-fronts', which are combined to propagate the solution. Experimental results show the improvement in the accuracy with respect to the state of the art, while the average computational speed is maintained in O(M log M), similar to the FMM techniques.</Data></Cell>
    <Cell><Data ss:Type="String">Brais Cancela*, University of A Coruña; Marcos Ortega, Universidade da Coruña; Manuel Penedo, Universidade da Coruña</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1992</Data></Cell>
    <Cell><Data ss:Type="String">StereoSnakes: Contour Based Consistent Object Extraction For Stereo Images</Data></Cell>
    <Cell><Data ss:Type="String">Consistent object extraction plays an essential role for stereo image editing with the population of stereoscopic 3D medias. Most previous methods perform segmentation on entire images for both views using dense stereo correspondence constraints. We find that for such kind of methods the computation is highly redundant since the two views are near-duplicate. Besides, the consistency may be violated due to the imperfectness of current stereo matching algorithms. In this paper, we propose a contour based method which searches for consistent object contours instead of regions. It integrates both stereo correspondence and object boundary constraints into an energy minimization framework. The proposed method has several advantages compared to previous works. First, the searching space is restricted in object boundaries thus the efficiency significantly improved. Second, the discriminative power of object contours results in a more consistent segmentation. Furthermore, the proposed method can effortlessly extend existing single-image segmentation methods to work in stereo scenarios. The experiment on the Adobe benchmark shows superior extraction accuracy and significant improvement of efficiency of our method to state-of-the-art. We also demonstrate in a few applications how our method can be used as a basic tool for stereo image editing.</Data></Cell>
    <Cell><Data ss:Type="String">Ran Ju, Nanjing University; Tongwei Ren, Nanjing University; Gangshan Wu*, Nanjing University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2000</Data></Cell>
    <Cell><Data ss:Type="String">Example-Based Modeling of Facial Texture from Deficient Data</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we present an approach to modeling ear-to-ear, high-quality texture from one or more partial views of a face with possibly poor resolution and noise. Our approach is example-based in that we reconstruct texture with patches from a database composed of previously observed faces. We use a 3D morphable model to establish correspondence between observed data across views and training faces. The database is built on the mesh surface by segmenting it into uniform overlapping patches. Texture patches are selected by belief propagation so as to be consistent with neighbors and with observations in an appropriate image formation model. We also develop a variant that is insensitive to light and camera parameters. We obtain convincing results from incomplete views fewer than 10 pixels wide. We further show applications to super-resolution where we substantially improve quality compared to classical algorithms, and to texture completion where we fill in missing regions and remove facial clutter in a photorealistic manner.</Data></Cell>
    <Cell><Data ss:Type="String">Arnaud Dessein*, Université de Bordeaux; William Smith, University of York; Richard Wilson, University of York; Edwin Hancock, University of York</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2006</Data></Cell>
    <Cell><Data ss:Type="String">Common Subspace for Model and Similarity: Phrase Learning for Sentence Generation from Images</Data></Cell>
    <Cell><Data ss:Type="String">Generating sentences to describe images is a fundamental problem combining computer vision and natural language processing. Recent work focus on descriptive phrases, such as “a white dog” to explain visual composites of an input image. The phrases can not only express objects, attributes, events and their relations, but also reduce visual complexity. A sentence for input image can be generated by connecting estimated phrases using a grammar model. However, because phrases are combinations of various words, the number of phrases is much larger than that of single words. Consequently, the accuracy of phrase estimation suffers from too few training samples per phrase.    In this paper, we propose a novel phrase learning method: Common Subspace for Model and Similarity (CoSMoS). In order to overcome the shortage of training samples, CoSMoS obtains a subspace in which (a) all feature vectors associated with the same phrase are mapped as mutually close, (b) classifiers for each phrase are learned, and (c) training samples are shared among cooccurring phrases. Experimental results demonstrate that our system is more accurate than those in earlier work and that the accuracy increases when the dataset from the web increases.</Data></Cell>
    <Cell><Data ss:Type="String">Yoshitaka Ushiku*, Univ. of Tokyo; Yusuke Mukuta, The University of Tokyo; Masataka Yamaguchi, Univ. of Tokyo; Tatsuya Harada, University of Tokyo</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2015</Data></Cell>
    <Cell><Data ss:Type="String">Simultaneous foreground detection and classification with hybrid features</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a hybrid background model that relies on edge and non-edge features of the image to produce the model. We encode these features into a coding scheme, that we called Local Hybrid Pattern (LHP), that selectively models edges and non-edges features of each pixel. Furthermore, we model each pixel with an adaptive code dictionary to represent the background dynamism, and update it by adding stable codes and discarding unstable ones. We weight each code in the dictionary to enhance its description of the pixel it models. The foreground is detected as the incoming codes that deviate from the dictionary. We can detect (as foreground or background) and classify (as edge or inner region) each pixel simultaneously. We tested our proposed method in existing databases with promising results.</Data></Cell>
    <Cell><Data ss:Type="String">Jaemyun Kim*, KyungHee university; Adin Ramirez Rivera, Universidad Diego Portales; Byungyong Ryu, KyungHee university; Oksam Chae, KyungHee university</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2044</Data></Cell>
    <Cell><Data ss:Type="String">Classical Scaling Revisited</Data></Cell>
    <Cell><Data ss:Type="String">Multidimensional-scaling (MDS) is an information analysis  tool. It involves the evaluation of distances between  data points, which is a quadratic space-time problem. Then,  MDS procedures find an embedding of the points in a low  dimensional Euclidean (flat) domain, optimizing for the  similarity of inter-points distances. We present an efficient  solver for Classical Scaling (a specific MDS model) by extending  the distances measured from a subset of the points  to the rest, while exploiting the smoothness property of the  distance functions. The smoothness is measured by the L2  norm of the Laplace-Beltrami operator applied to the unknown  distance function. The Laplace Beltrami reflects the  local differential relations between points, and can be computed  in linear time. Classical-scaling is thereby reformulated  into a quasi-linear space-time complexities procedure.</Data></Cell>
    <Cell><Data ss:Type="String">Gil Shamai*, Technion; Yonathan Aflalo, ; Ron Kimmel, Technion; Michael Zibulevsky, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2049</Data></Cell>
    <Cell><Data ss:Type="String">Aggregating local deep features for image retrieval</Data></Cell>
    <Cell><Data ss:Type="String">Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It also has been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregating methods developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptor.    In this paper we investigate possible ways to aggregate local deep features to produce compact descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. In addition, we suggest a simple yet efficient query expansion scheme suitable for the proposed aggregation method. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably. E.g. on Oxford Buildings dataset the 256-dimensional global image descriptor achieves 0.71 mean average precision.</Data></Cell>
    <Cell><Data ss:Type="String">Artem Babenko*, MIPT/Yandex; Victor  Lempitsky, &quot;Skoltech Moscow, Russia&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2051</Data></Cell>
    <Cell><Data ss:Type="String">A Self-paced Multiple-instance Learning Framework for Co-saliency Detection</Data></Cell>
    <Cell><Data ss:Type="String">As an interesting and emerging topic, co-saliency detection aims at simultaneously extracting common salient objects in a group of images. Traditional co-saliency detection approaches rely heavily on human knowledge for designing hand-crafted metrics to explore the intrinsic patterns underlying co-salient objects. Such strategies, however, always suffer from poor generalization capability to flexibly adapt various scenarios in real applications, especially due to their lack of insightful understanding of the biological mechanisms of human visual co-attention. To alleviate this problem, we propose a novel framework for this task, by naturally reformulating it as a multiple-instance learning (MIL) problem and further integrating it into a self-paced learning (SPL) regime. The proposed framework on one hand is capable of fitting insightful metric measurements and discovering common patterns under co-salient regions in a self-learning way by MIL, and on the other hand tends to promise the learning reliability and stability by simulating the human learning process through SPL. Experiments on benchmark datasets have demonstrated the effectiveness of the proposed framework as compared with the state-of-the-arts.</Data></Cell>
    <Cell><Data ss:Type="String">Dingwen Zhang, NWPU; Deyu Meng, Xi'an Jiaotong University; Chao Li, Northwestern Polytechnical University; Lu Jiang, Carnegie Mellon University; Qian Zhao, Xi'an Jiaotong University; Junwei Han*, Northwestern Polytechnical U.</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2068</Data></Cell>
    <Cell><Data ss:Type="String">Matrix Backpropagation for Deep networks with Structured Layers</Data></Cell>
    <Cell><Data ss:Type="String">Deep convolutional neural network architectures have  recently produced excellent results in a variety of areas in  artificial intelligence and visual recognition, well surpass-  ing traditional shallow architectures trained using hand-  designed features. The power of ConvNets stems  both  from their ability to perform local computations followed  by pointwise non-linearities over increasingly larger re-  ceptive fields,  and  from the simplicity and scalability of  the gradient-descent training procedure based on back-  propagation. A fundamental open problem is the inclusion  of layers that perform global, structured matrix computa-  tions like segmentation (e.g. normalized cuts) or higher-  order pooling (e.g. log tangent space metrics defined over  the manifold of symmetric positive definite matrices) while  preserving the validity and efficiency of an end-to-end deep  training framework. In this paper we propose a sound math-  ematical apparatus to formally integrate global structured  computation into deep computation architectures. At the  heart of our methodology is the development of the the-  ory and practice of back-propagation that generalizes to  the calculus of matrix variations. We performed extensive  proof-of-concept segmentation experiments using the BSDS  and MSCOCO benchmarks and demonstrate that deep net-  works relying on second-order pooling and normalized cuts  layers, trained end-to-end using matrix back-propagation,  outperform counterparts that do not take advantage of such  global layers.</Data></Cell>
    <Cell><Data ss:Type="String">Catalin Ionescu*, University of bonn; Orestis Vantzos, University of Bonn; Cristian Sminchisescu, &quot;Lund University, Sweden&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2069</Data></Cell>
    <Cell><Data ss:Type="String">Bayesian non-parametric inference for manifold based MoCap representation</Data></Cell>
    <Cell><Data ss:Type="String">We propose a novel approach to human action recognition, with motion capture data (MoCap), based on  grouping sub-body parts. By representing configurations of actions as manifolds,  joint positions are mapped on a subspace via principal geodesic analysis. The reduced space is still highly informative and allows for classification based on a non-parametric Bayesian approach,  generating behaviors for each sub-body part. Having partitioned the set of joints,   poses relative to a sub-body part are exchangeable, given a specified prior and can elicit, in principle, infinite behaviors. The generation of these behaviors is specified by a Dirichlet process mixture. We show with several experiments that the recognition gives very promising results, outperforming methods requiring  temporal alignment.</Data></Cell>
    <Cell><Data ss:Type="String">Fabrizio Natola*, Sapienza; Valsamis Ntouskos, Sapienza University of Rome; Fiora Pirri, university of rome, Sapienza; Marta Sanzari, Sapienza University of Rome</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2071</Data></Cell>
    <Cell><Data ss:Type="String">Learning to Predict Saliency on Face Images</Data></Cell>
    <Cell><Data ss:Type="String">The previous work has demonstrated that integrating top-down features in bottom-up saliency methods can improve the saliency prediction accuracy. Therefore, this paper proposes to learn Gaussian mixture model (GMM) distribution of eye fixations over faces as the top-down features and to learn their corresponding weights, for predicting saliency on face images. Specifically, we obtain a database of eye tracking over extensive face images, via conducting an eye tracking experiment. With analysis on eye tracking database, we verify that fixations tend to cluster around facial features, when viewing images with large faces. Thus, the GMM is learnt from fixations of eye tracking data, for modeling the distribution of saliency in faces and facial features. Then, in our method, the top-down features (i.e., face and facial features) upon the learnt GMM are linearly combined with the conventional bottom-up features (i.e., color, intensity, and orientation), for saliency detection. In the linear combination, we argue that the weights corresponding to top-down feature channels depend on the face size in images, and the relationship between the weights and face size is thus investigated via learning from the training eye tracking data. Finally, experimental results validate that our learning-based method is capable of dramatically improving the accuracy of saliency prediction for face images.</Data></Cell>
    <Cell><Data ss:Type="String">Mai Xu*, Beihang University; Yun Ren, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2073</Data></Cell>
    <Cell><Data ss:Type="String">Person Re-Identification Ranking Optimisation by Discriminant Context Information Analysis</Data></Cell>
    <Cell><Data ss:Type="String">Person re-identification across non-overlapping cameras is still a challenging problem in smart video surveillance applications.  The most of the existing re-identification studies run into the improvement of the ranking providing sophisticated learning schemes for a pair of cameras.  These methods focus on the matching of the visual features due to the large variations of lighting, viewpoints, \etc, but hardly ever pay attention in the ranking optimisation where visual ambiguities earn a important place.   In this paper, we introduce a unsupervised discriminant context information analysis approach to refine the initial ranking removing the visual ambiguities from similar appearances which are provided by content and context informations.   We conduct extensive experiments over three baseline models demonstrating a remarkable improvement in the first positions of the ranking.  Also, we compare the performance of our approach with state-of-the-art methods on publicly available datasets as VIPeR, PRID and CUHK02.</Data></Cell>
    <Cell><Data ss:Type="String">Jorge García*, University if Alcala; Niki Martinel, University of Udine; christian Micheloni, University of Udine; Alfredo Gardel, University of Alcala</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2080</Data></Cell>
    <Cell><Data ss:Type="String">Introducing Geometry into Active Learning for Image Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">We propose  an Active  Learning approach to  training a  segmentation classifier that exploits geometric priors to streamline  the annotation process in 3D imagevolumes.  To  this end, we use  these priors not  only to select voxels  most in need of  annotation but  to guarantee that  they lie on  2D planar  patch, which makes it much easier  to annotate than if they were  randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images.    We evaluated  our approach on  Electron Microscopy and Magnetic  Resonance image volumes, as well  as on natural images.  Comparing our  approach against several accepted baselines demonstrates a marked performance increase.  </Data></Cell>
    <Cell><Data ss:Type="String">Ksenia Konyushkova*, EPFL; Raphael Sznitman, University of Bern; Pascal  Fua, &quot;EPFL, Switzerland&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2090</Data></Cell>
    <Cell><Data ss:Type="String">Hyperspectral Super-Resolution by Coupled Spectral Unmixing</Data></Cell>
    <Cell><Data ss:Type="String">Hyperspectral cameras capture images with many narrow spectral channels, which densely sample the electromagnetic spectrum. The detailed spectral resolution is useful for many image analysis problems, but it comes at the cost of much lower spatial resolution. Hyperspectral super-resolution addresses this problem, by fusing a low-resolution hyperspectral image and a conventional high-resolution image into a product of both high spatial and high spectral resolution. In this paper, we propose a method which performs hyperspectral super-resolution by jointly unmixing the two input images into the pure reflectance spectra of the observed materials and the associated mixing coefficients. The formulation leads to a coupled matrix factorization problem, with a number of useful constraints imposed by elementary physical properties of spectral mixing. In experiments with two benchmark datasets we show that the proposed approach delivers improved hyperspectral super-resolution, with approximately 30% lower error than competing methods.</Data></Cell>
    <Cell><Data ss:Type="String">Charis Alexandros Lanaras*, ETH Zurich; Emmanuel Baltsavias, ETH Zurich; Konrad Schindler, &quot;ETH Zurich, Switzerland&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2097</Data></Cell>
    <Cell><Data ss:Type="String">Efficient Classifier Training for Electron Microscopy Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Segmentation algorithms have recently gained prominence in neural reconstruction from Electron Microscopy (EM) images. In practical circumstances of such reconstruction, it is advantageous to be able to train the components of segmentation algorithms efficiently without compromising considerable accuracy. For the scenario where a subsequent correction, or other post-processing step, is essential, it is also desirable that these algorithms generate fewer under-segmentation than over-segmentation errors.  This study proposes a novel method for efficient classifier learning for segmentation of EM images. Instead of using an exhaustive pixel level groundtruth, an active semi-supervised algorithm is developed for sparse labeling of pixel and superpixel boundaries. In pursuit of the reduction of error correction effort, the proposed algorithm is designed to prioritize minimization of false-merges over false-split errors. The results on both 2D and 3D segmentation problems suggest the proposed method can achieve accuracies comparable to or better than those of the current state of the art techniques. </Data></Cell>
    <Cell><Data ss:Type="String">Toufiq Parag*, Janelia Research Campus-HHMI</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2101</Data></Cell>
    <Cell><Data ss:Type="String">Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images</Data></Cell>
    <Cell><Data ss:Type="String">Analysis by synthesis has been a successful approach for many tasks in computer vision, such as 6D pose estimation of an object in an RGB-D image which is the topic of this work. The idea is to compare the observation with the output of a forward process, such as a rendered image of the object of interest in a particular pose. Due to occlusion or complicated sensor noise, it can be difficult to perform this comparison in a meaningful way. We propose an approach that &quot;learns to compare&quot;, while taking these difficulties into account. This is done by describing the posterior density of a particular object pose with a convolutional neural network (CNN) that compares an observed and rendered image. The network is trained with the maximum likelihood paradigm. We observe empirically that the CNN does not specialize to the geometry or appearance of specific objects, and it can be used with objects of vastly different shapes and appearances, and in different backgrounds. Compared to state-of-the-art, we demonstrate a significant improvement on two different datasets which include a total of eleven objects, cluttered background, and heavy occlusion.</Data></Cell>
    <Cell><Data ss:Type="String">Alexander Krull*, TU Dresden; Eric Brachmann, TU Dresden; Frank Michel, TU Dresden; Michael Ying Yang, TU Dresden; Stefan Gumhold, TU Dresden; Carsten Rother, TU Dresden</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2103</Data></Cell>
    <Cell><Data ss:Type="String">Dense Continuous-Time Tracking and Mapping with Rolling Shutter RGB-D Cameras</Data></Cell>
    <Cell><Data ss:Type="String">We propose a dense continuous-time tracking and mapping method for RGB-D cameras.  We parametrize the camera trajectory using continuous B-splines and optimize the trajectory through dense, direct image alignment.  Our method also directly models rolling shutter in both RGB and depth images within the optimization, which improves tracking and reconstruction quality for low-cost CMOS sensors.    Using a continuous trajectory representation has a number of advantages over a discrete-time representation (e.g. camera poses at the frame interval).  With splines, less variables need to be optimized than with a discrete representation, since the trajectory can be represented with fewer control points than frames.  Splines also naturally include smoothness priors on derivatives of the trajectory estimate.  Finally, the continuous trajectory representation allows to compensate for rolling shutter effects, since a pose estimate is availabe at any exposure time of an image.  Our approach demonstrates superior quality in tracking and reconstruction compared to approaches with discrete-time or global shutter assumptions.</Data></Cell>
    <Cell><Data ss:Type="String">Christian Kerl*, TU Munich; Jörg Stückler, TU Munich; Daniel Cremers, TUM</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2115</Data></Cell>
    <Cell><Data ss:Type="String">Large Displacement 3D Scene Flow with Occlusion Reasoning</Data></Cell>
    <Cell><Data ss:Type="String">3D motion estimation is a fundamental problem with many computer vision applications. With the emergence of modern, affordable and increasingly accurate RGB-D sensors, single view approaches for estimating 3D motion, also known as scene flow, are becoming popular. In this paper we propose a novel coarse to fine correspondence-based scene flow approach to account for the effects of large displacements and to model occlusion, based on explicit geometric reasoning. Our methodology enforces piecewise motion rigidity at the level of the depth point cloud without explicitly smoothing the parameters of adjacent neighborhoods. By integrating all geometric and photometric components in a single, consistent, occlusion-aware energy model our method is able to deal with fast motions and large occlusions areas, as present in challenging datasets like MPI Sintel Flow Dataset, which have recently been augmented with depth information. By explicitly modeling large displacements and occlusion, we can now more successfully work with difficult sequences which cannot be currently processed by state of the art scene flow methods that rely on small inter-frame motion assumptions. We also show that by leveraging depth information, we can obtain superior correspondence fields compared to the best state of the art large-displacement (2D) optical flow methods. </Data></Cell>
    <Cell><Data ss:Type="String">Andrei Zanfir*, IMAR; Cristian Sminchisescu, &quot;Lund University, Sweden&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2119</Data></Cell>
    <Cell><Data ss:Type="String">Dense image registration and deformable surface reconstruction in presence of occlusions and minimal texture</Data></Cell>
    <Cell><Data ss:Type="String">Deformable surface tracking from monocular images is well-known to be under-constrained. Occlusions often make the task even more challenging, and can result in failure if the surface is not sufficiently textured. In this work, we explicitly address the problem of 3D reconstruction of poorly textured, occluded surfaces, proposing a framework based on a template-matching approach that scales dense robust features by a relevancy score. Our approach is extensively compared to current methods employing both local feature matching and dense template alignment. We test on standard datasets as well as on a new dataset (that will be made publicly available) of a sparsely textured, occluded surface. Our framework achieves state-of-the-art results for both well and poorly textured, occluded surfaces.</Data></Cell>
    <Cell><Data ss:Type="String">Dat Tien Ngo*, EPFL; Sanghyuk Park, KAIST; Anne Jorstad, ; Alberto Crivellaro, EPFL; Chang Yoo, KAIST; Pascal  Fua, &quot;EPFL, Switzerland&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2124</Data></Cell>
    <Cell><Data ss:Type="String">3D Surface Profilometry using Phase Shifting of De Bruijn Pattern</Data></Cell>
    <Cell><Data ss:Type="String">A novel structured light method for color 3D surface  profilometry is proposed. The proposed method does not  require color calibration of a camera-projector pair and  may be used for reconstruction of both dynamic and static  scenes. The method uses a structured light pattern that is  a combination of a De Bruijn color sequence and of a sinusoidal  fringe. For dynamic scenes a Hessian ridge detector  and a Gaussian mixture model are combined to extract  stripe centers and to identify color. Stripes are then  uniquely identified using dynamic programming based on  the Smith-Waterman algorithm and a De Bruijn window  property. For static scenes phase-shifting and De Bruijn  window property are combined to obtain a high accuracy  reconstruction. We have tested the proposed method on  multiple objects with challenging surfaces and different  albedos that demonstrate usability and robustness of the  method.</Data></Cell>
    <Cell><Data ss:Type="String">Matea Donlic*, Faculty of Electrical Engineering and Computing; Tomislav Petkovic, Faculty of Electrical Engineering and Computing; Tomislav Pribanic, Faculty of Electrical Engineering and Computing</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2126</Data></Cell>
    <Cell><Data ss:Type="String">Listening by Eyes: Towards a Practical Visual Speech Recognition System Using Deep Boltzmann Machines</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a novel feature learning method for visual speech recognition using Deep Boltzmann Machines (DBM). Unlike all existing visual feature extraction techniques which solely extracts features from video sequences, our method is able to explore both acoustic information and visual information to learn a better visual feature representation in the training stage. During the test stage, instead of using both audio and visual signals, only the videos are used for generating the missing audio feature, and both the given visual and given audio features are used to obtain a joint representation. We carried out our experiments on a large scale audio-visual data corpus, and experimental results show that our proposed techniques outperforms the performance of the hadncrafted features and features learned by other commonly used deep learning techniques.</Data></Cell>
    <Cell><Data ss:Type="String">Chao  Sui*, University of Western Australi; Mohammed Bennamoun, UWA; Roberto Togneri, UWA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2129</Data></Cell>
    <Cell><Data ss:Type="String">Multi-view Domain Generalization for Visual Recognition</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose a new multi-view domain generalization (MVDG) approach for visual recognition, in which we aim to use the source domain samples with multiple types of features (i.e., multi-view features) to learn robust classifiers that can generalize well to any unseen target domain. Considering the recent works show the domain generalization capability can be enhanced by fusing multiple SVM classifiers, we build upon exemplar SVMs to learn a set of SVM classifiers by using one positive sample and all negative samples in the source domain each time. When the source domain samples come from multiple latent domains, we expect the weight vectors of exemplar SVM classifiers can be organized into multiple hidden clusters. To exploit such cluster structure, we organize the weight vectors learnt on each view as a weight matrix and seek the low-rank representation by reconstructing this weight matrix using itself as the dictionary. To enforce the consistency of inherent cluster structures discovered from the weight matrices learnt on different views, we introduce a new regularizer to minimize the mismatch between any two representation matrices on different views. We also develop an efficient alternating optimization algorithm and further extend our MVDG approach for domain adaptation by exploiting the manifold structure of unlabeled target domain samples. Comprehensive experiments for visual recognition using the RGB-D videos from two datasets clearly demonstrate the effectiveness of our approaches for domain generalization and domain adaptation.</Data></Cell>
    <Cell><Data ss:Type="String">Li Niu*, NTU; Wen Li, ; Dong Xu, Nanyang Technological University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2130</Data></Cell>
    <Cell><Data ss:Type="String">Human Pose Estimation in Videos</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we present a method to estimate a sequence of human poses in unconstrained videos. In contrast to the commonly employed graph optimization framework, which is NP-hard and needs approximate solutions, we formulate this problem into a unified two stage tree-based optimization problem for which an efficient and exact solution exists. Although the proposed method finds an exact solution, it does not sacrifice the ability to model the spatial and temporal constraints between body parts in the frames; indeed it even models the symmetric parts better than the existing methods. The proposed method is based on two main ideas: `Abstraction' and `Association' to enforce the intra- and inter-frame body part constraints without inducing extra computational complexity to the polynomial time solution. Using the idea of `Abstraction', a new concept of `abstract body part' is introduced to model not only the tree based body part structure similar to existing methods, but also extra constraints between symmetric parts. Using the idea of `Association', the optimal tracklets are generated for each abstract body part, in order to enforce the spatiotemporal constraints between body parts in adjacent frames. Finally, a sequence of the best poses is inferred  from the abstract body part tracklets through the tree-based optimization. We evaluated the proposed method on three publicly available video based human pose estimation datasets, and obtained dramatically improved performance compared to the state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Dong Zhang*, University of Central Florida; Mubarak Shah, &quot;University of Central Florida, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2156</Data></Cell>
    <Cell><Data ss:Type="String">Semantic Segmentation of RGBD Images with Mutex Constraints</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we address the problem of semantic scene segmentation of RGB-D images of indoor scenes.     We propose a novel image region labeling method which augments CRF formulation with hard mutual exclusion (mutex) constraints. This way our approach can make use of rich and accurate 3D geometric structure coming from Kinect in a principled manner. The final labeling result must satisfy all mutex constraints,     which allows us to eliminate configurations that violate common sense physics laws     like placing a floor above a night stand.       Three classes of mutex constraints are proposed: global object co-occurrence constraint, relative height relationship constraint,     and local support relationship constraint.      We evaluate our approach on the NYU-Depth V2 dataset, which consists of 1449 cluttered indoor scenes, and also test generalization of our model trained on NYU-Depth V2 dataset directly on a recent SUN3D dataset without any new training. The experimental results show that we significantly outperform the state-of-the-art methods in scene labeling on both datasets.</Data></Cell>
    <Cell><Data ss:Type="String">Zhuo Deng*, Temple University; Sinisa Todorovic, Oregon State University; Longin Jan Latecki, Temple University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2163</Data></Cell>
    <Cell><Data ss:Type="String">Learning Deep Object Detectors from 3D Models</Data></Cell>
    <Cell><Data ss:Type="String">Crowdsourced 3D CAD models are becoming easily accessible  online, and can potentially generate an infinite  number of training images for almost any object category.  We show that adapting contemporary Deep Convolutional  Neural Net (DCNN) models to such data can be effective,  especially in the few-shot regime where none or only a few  annotated real images are available, or where the images  are not well matched to the target domain. Little is known  about the degree of realism necessary to train models with  deep features on CAD data. In a detailed analysis, we use  synthetic images to probe DCNN invariance to object-class  variations caused by 3D shape, pose, and photorealism,  with surprising findings. In particular, we show that DCNNs  used as a fixed representation exhibit a large amount of  invariance to these factors, but, if allowed to adapt, can still  learn effectively from synthetic data. These findings guide  us in designing a method for adaptive training of DCNNs  using real and synthetic data. We show that our approach  significantly outperforms previous methods on the benchmark  PASCAL VOC2007 dataset when learning in the fewshot  scenario, and outperform training with real data in a  domain shift scenario on the Office benchmark.</Data></Cell>
    <Cell><Data ss:Type="String">Xingchao Peng*, Umass Lowell; Kate Saenko, &quot;University of Massachusetts Lowel, USA&quot;; Baochen Sun, UMass Lowell; Karim Ali, Umass Lowell</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2165</Data></Cell>
    <Cell><Data ss:Type="String">External patch prior guided internal clustering for image denoising</Data></Cell>
    <Cell><Data ss:Type="String">Natural image modeling plays a key role in many vision problems such as image denoising. Image priors are widely used to regularize the denoising process, which is an illposed inverse problem. One category of denoising methods exploit the priors (e.g., TV, sparsity) learned from external clean images to reconstruct the given noisy image, while another category of methods exploit the internal prior (e.g., self-similarity) to reconstruct the latent image. Though the internal prior based methods have achieved impressive denoising results, the improvement of visual quality will become very difficult with the increase of noise level. In this paper, we propose to exploit image external patch prior and internal self-similarity prior jointly, and develop an external patch prior guided internal clustering algorithm for image denoising. It is known that natural image patches form multiple subspaces. By utilizing Gaussian mixture models (GMMs) learning, image similar patches can be clustered and the subspaces can be learned. The learned GMMs from clean images are then used to guide the clustering of noisypatches of the input noisy images, followed by a low-rank approximation process to estimate the latent subspace for image recovery. Numerical experiments show that the proposed method outperforms many state-of-the-art denoising algorithms such as BM3D and WNNM.</Data></Cell>
    <Cell><Data ss:Type="String">Fei Chen*, Fuzhou University; Lei Zhang, The Hong Kong Polytechnic University; Huimin Yu, Zhejiang University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2168</Data></Cell>
    <Cell><Data ss:Type="String">The Likelihood-Ratio Test and Efficient Robust Estimation</Data></Cell>
    <Cell><Data ss:Type="String">Robust estimation of model parameters in the presence of outliers is a key problem in computer vision. RANSAC inspired techniques are widely used in this context, although their application might be limited due to the need of a priori knowledge on the inlier noise level. We propose a new approach for jointly optimizing over model parameters and the inlier noise level based on the likelihood ratio test which allows control over the type I error incurred. We also propose an early bailout strategy for efficiency. Tests on both synthetic and real data show our method to outperform the state-of-the-art in a fraction of the time.</Data></Cell>
    <Cell><Data ss:Type="String">Andrea Cohen, ETHZ; Christopher Zach*, Toshiba Research Europe</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2178</Data></Cell>
    <Cell><Data ss:Type="String">Reflection Modeling for Passive Stereo</Data></Cell>
    <Cell><Data ss:Type="String">Stereo reconstruction in presence of reality faces many challenges that still need to be addressed.  This paper considers reflections, which introduce incorrect matches due to the observation violating the diffuse-world assumption underlying the majority of stereo techniques.  Unlike most existing work, which employ regularization or robust data terms to suppress such errors,   we derive two least squares models from first principles that generalize diffuse world stereo   and explicitly take reflections into account.    These models are parametrized by depth, orientation and material properties, resulting in a total of up to 5 parameters per pixel that have to be estimated.   Additionally large non-local interactions between viewed and reflected surface have to be taken into account.   These two properties make inference of the model appear prohibitive, but we present evidence that inference is actually possible using a variant of patch match stereo.</Data></Cell>
    <Cell><Data ss:Type="String">Rahul Nair*, Universität Heidelberg; Andrew Fitzgibbon, Microsoft Research; Daniel Kondermann, HCI, university of Heidelberg; Carsten Rother, TU Dresden</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2179</Data></Cell>
    <Cell><Data ss:Type="String">Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification</Data></Cell>
    <Cell><Data ss:Type="String">Recent work on scene classification still makes use of  generic CNN features in a rudimentary manner. In this paper,  we present a novel pipeline that builds upon deep CNN  features to harvest discriminative visual objects and parts  for scene classification. We first use a region proposal technique  to generate a set of high-quality patches potentially  containing objects, then apply weakly supervised learning  to screen these patches according to their discriminative  power for scene classification. We feed the remaining image  patches to a pre-trained CNN to obtain generic deep  features. Instead of directly using the chosen scene-specific  objects and parts, we further apply supervised clustering to  aggregate similar objects and parts into groups, called meta  objects. Finally, a scene image representation is built by  pooling the feature response maps of all the learned meta  objects. We have found that the scene image representation  built through this new pipeline is capable of delivering  state-of-the-art performance on two popular scene benchmark  datasets, MIT Indoor 67 [25] and Sun397 [34].</Data></Cell>
    <Cell><Data ss:Type="String">Ruobing Wu, HKU; Baoyuan Wang, Microsoft Research Asia; Yizhou Yu*, HKU, UIUC, ZJU</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2193</Data></Cell>
    <Cell><Data ss:Type="String">Cluster-based point set saliency</Data></Cell>
    <Cell><Data ss:Type="String">We propose a cluster-based approach to point set saliency detection. A point set is first decomposed into small clusters, using fuzzy clustering. We evaluate cluster uniqueness and spatial distribution at each cluster. Both scalar functions are combined into a cluster saliency function. Finally, the probabilities of points belonging to each cluster are used to assign a saliency to each point. We show that our approach performs better than previous mesh-based and point-based saliency detection methods. It detects fine-scale salient features and uninteresting regions consistently have lower saliency values. We evaluate our saliency model  by testing a saliency-based keypoint detection against a 3D interest point detection benchmark. The evaluation shows that our method achieves the best balance between false positive and false negative error rates, without the use of any topological information.</Data></Cell>
    <Cell><Data ss:Type="String">Flora Ponjou Tasse*, University of Cambridge; Jiri Kosinka, University of Cambridge; Neil Dodgson, University of Cambridge</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2195</Data></Cell>
    <Cell><Data ss:Type="String">Co-interest of Multiple Wearable Cameras</Data></Cell>
    <Cell><Data ss:Type="String">Wearable cameras, such as Google Glass and Go Pro, enable video data over larger areas and from different views. In this paper, we tackle a new problem of locating the target of co-interest from multiple, temporally synchronized videos taken by the multiple wearable cameras. More specifically, we propose a co-interest detection algorithm that can find persons that draw attention from most camera wearers, even if multiple similar-appearance persons are present in the videos. Our basic idea is to exploit the motion pattern, location, and size of persons detected in different synchronized videos and use them to correlate the detected persons across different videos &#45;- one person in a video may be the same person in another video at a time. We built a Conditional Random Field (CRF) to achieve this goal, by taking each frame as a node and the detected persons as the states at each node.     In calculating the CRF energy, we ignore the direction of the horizontal motion in the videos since the same 3D horizontal motion may be projected to opposite-direction 2D horizontal motions with the change of the camera view. In constructing CRF, we also introduce one idle state for each node to handle target missing, e.g., a wearer is usually not present in his egocentric video and a person of interest may be occluded by another person at a frame. We collect two sets of wearable-camera videos for testing the proposed algorithm where each set consists of six temporally synchronized videos. All the involved people have similar appearance in the collected videos and the experiments demonstrate the effectiveness of the proposed algorithm.</Data></Cell>
    <Cell><Data ss:Type="String">Yuewei Lin*, University of South Carolina; Kareem Abdelfatah, Fayoum University; Youjie Zhou, ; xiaochuan Fan, University of South Carolina; Hongkai Yu, University of South Carolina ; Hui Qian, ; Song Wang, University of South Carolina</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2203</Data></Cell>
    <Cell><Data ss:Type="String">Infinite Feature Selection</Data></Cell>
    <Cell><Data ss:Type="String">Filter-based feature selection has become crucial in many recent classification settings, and especially object recognition, recently faced with feature learning strategies that originate thousands of cues. In this paper we propose a method for feature selection exploiting the convergence properties of power series of matrices, and introducing the concept of infinite feature selection (Inf-FS). Considering a selection of features as a path among feature distributions, and letting these paths tend to infinite, permits to investigate the importance (relevance and redundancy) of a feature when injected in an arbitrary set of cues. Ranking the importance individuates candidate features which turn out to be effective from a classification point of view, this fact being proved by a thoroughly experimental section; Inf-FS has been tested on thirteen benchmarks of handwritten recognition, cancer classification/prediction, generic feature selection and in particular object recognition: on all of them, and considering comparative methods (filters, embedded methods and wrappers), we get top performances, setting also the absolute best in eight cases, notably on the PASCAL VOC 2007-2012. As a by-product, the result of the filtering confirms once again the strength of the deep-learning based features.</Data></Cell>
    <Cell><Data ss:Type="String">Giorgio Roffo*, University of Verona; Simone Melzi, University of Verona; Marco Cristani, U. Verona</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2208</Data></Cell>
    <Cell><Data ss:Type="String">Scalable Nonlinear Embeddings for Semantic Category-based Image Retrieval</Data></Cell>
    <Cell><Data ss:Type="String">We propose a novel algorithm for the task of supervised discriminative distance learning by nonlinearly embedding vectors into a low dimensional Euclidean space. We work in the challenging setting where supervision is with con straints on similar and dissimilar pairs while training. The proposed method is derived by an approximate kernelization of a linear Mahalanobis-like distance metric learning algorithm and can also be seen as a kernel neural network. The number of model parameters and test time evaluation complexity of the proposed method are O(dD) where D is the dimensionality of the input features and d is the dimension of the projection space–this is in contrast to the usual kernelization methods as, unlike them, the complexity does not scale linearly with the number of training examples. We propose a stochastic gradient based learning algorithm which makes the method scalable (w.r.t. number of training examples), while being nonlinear. We train the method with up to half a million training pairs of 4096 dimensional CNN features. We give empirical comparisons with relevant baselines on seven challenging datasets for the task of low dimensional semantic category based image retrieval.  </Data></Cell>
    <Cell><Data ss:Type="String">Gaurav  Sharma*, MPI Informatics; Bernt Schiele, MPI Informatics Germany</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2212</Data></Cell>
    <Cell><Data ss:Type="String">Deep Temporal Appearance-Geometry Network for Facial Expression Recognition</Data></Cell>
    <Cell><Data ss:Type="String">Temporal information has useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique, which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal appearance features from image sequences, while the other deep network extracts temporal geometry features from temporal facial landmark points. These two models are combined using a new integration method in order to boost the performance of the facial expression recognition. Through several experiments, we show that the two models cooperate with each other. As a result, we achieve superior performance to other state-of-the-art methods in the CK+ and Oulu-CASIA databases. Furthermore, we show that our new integration method gives more accurate results than traditional methods, such as a weighted summation and a feature concatenation  method.</Data></Cell>
    <Cell><Data ss:Type="String">Heechul Jung*, KAIST; Sihaeng Lee, KAIST; JUNHO YIM, KAIST; Sunjeong Park, KAIST; Junmo Kim, KAIST</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2215</Data></Cell>
    <Cell><Data ss:Type="String">Direct Intrinsics: Albedo and Shading Decomposition by CNN Regression</Data></Cell>
    <Cell><Data ss:Type="String">We introduce a new approach to intrinsic image decomposition, the task of decomposing a single image into albedo and shading components.  Our strategy, which we term direct intrinsics, is to learn a fully convolutional neural network (CNN) that directly predicts output albedo and shading channels from an input RGB image patch.  Direct intrinsics is a departure from classical techniques for intrinsic image decomposition, which typically rely on physically-motivated priors and graph-based inference algorithms.    The large-scale synthetic ground-truth of the MPI Sintel dataset plays the key role in training direct intrinsics.  We demonstrate results on both the synthetic images of Sintel and the real images of the classic MIT intrinsic image dataset.  On Sintel, direct intrinsics, using only RGB input, outperforms all prior work, including methods that rely on RGB+Depth input.  Direct intrinsics also generalizes across modalities; our Sintel-trained CNN produces quite reasonable decompositions on the real images of the MIT dataset.  Our results indicate that the marriage of CNNs with synthetic training data may be a powerful new technique for tackling classic problems in computer vision.  </Data></Cell>
    <Cell><Data ss:Type="String">Takuya Narihira*, UC Berkeley/ICSI/Sony Corp; Michael Maire, Toyota Technological Institute at Chicago; Stella Yu, UC Berkeley / ICSI</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2258</Data></Cell>
    <Cell><Data ss:Type="String">On Statistical Analysis of Neuroimages with Imperfect Registration</Data></Cell>
    <Cell><Data ss:Type="String">A variety of studies in neuroscience/neuroimaging seek to perform statistical inference on the acquired brain image scans for diagnosis as well as understanding the pathological manifestation of diseases. To do so, an important first step is to register (or co-register) all of the image data into a common coordinate system. This permits meaningful comparison of the intensities at each voxel across groups (e.g., diseased versus healthy) to evaluate the effects of the disease and/or use machine learning algorithms in a subsequent step. But errors in the underlying registration make this problematic, they either decrease the statistical power or make the follow-up inference tasks less effective/accurate. In this paper, we derive a novel algorithm which offers immunity to local errors in the underlying deformation field obtained from registration procedures. By deriving a deformation invariant representation of the image, the downstream analysis can be made more robust as if one had access to a (hypothetical) far superior registration procedure. Our algorithm is based on recent work on Scattering coefficients. Using this as a starting point, we show how results from harmonic analysis (especially, non-Euclidean wavelets) yields strategies for designing deformation and additive noise invariant representations of large 3-D brain image volumes. We present a set of results on synthetic and real brain images where we achieve robust statistical analysis even in the presence of substantial deformation errors; here, standard analysis procedures significantly under-perform and fail to identify the true signal.</Data></Cell>
    <Cell><Data ss:Type="String">Won Hwa Kim*, Univ. of Wisconsin - Madison; Sathya Ravi, University of Wisconsin Madiso; Ozioma Okonkwo, ; Sterling Johnson, ; Vikas Singh, University of Wisconsin-Madison</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2262</Data></Cell>
    <Cell><Data ss:Type="String">Detailed Full-Body Reconstructions of Moving People from Monocular RGB-D Sequences</Data></Cell>
    <Cell><Data ss:Type="String">We accurately estimate the 3D geometry and appearance of the human body from a monocular RGB-D sequence of a user moving freely in front of the sensor.  Range data in each frame is first brought into alignment with a multi-resolution 3D body model in a coarse-to-fine process.  The method then uses geometry and image texture over time to obtain accurate shape, pose, and appearance information despite unconstrained motion, partial views, varying resolution, occlusion, and soft tissue deformation. Our novel body model has variable shape detail, allowing it to  capture faces with a high-resolution deformable head model and body shape with lower-resolution. Finally we combine range data from an entire sequence to estimate a high-resolution displacement map that captures fine shape details. We compare our recovered models with high-resolution scans from a professional system and with avatars created by a commercial product. We extract accurate 3D avatars from challenging motion sequences and even capture soft tissue dynamics.</Data></Cell>
    <Cell><Data ss:Type="String">Federica Bogo*, MPI for Intelligent Systems; Javier Romero, MPI Intelligent Systems Tuebingen; Matthew Loper, Max Planck Institute; Michael Black, &quot;MPI, Tuebingen, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2280</Data></Cell>
    <Cell><Data ss:Type="String">Efficient Solution to the Relative Pose Problem for Radially Distorted Cameras</Data></Cell>
    <Cell><Data ss:Type="String">The estimation of the relative pose of two cameras from image matches is a fundamental problem in computer vision with many applications. While the closely related problem of estimating relative pose of two different uncalibrated cameras with radial distortion is of particular importance, none of the previously published methods is suitable for practical applications. These solutions are either numerically unstable, sensitive to noise, based on a large number of point correspondences, or simply too slow for real-time applications. In this paper, we present a new efficient solution to this problem from 10 image correspondences. By manipulating ten input polynomial equations, we derive a degree 10 polynomial equation in one variable. The solutions to this equation are efficiently found using the Sturm sequences method. In the experiments, we show that the proposed solution is both stable, noise resistant, and fast, and as such efficiently usable in a practical Structure-from-Motion pipeline.</Data></Cell>
    <Cell><Data ss:Type="String">Zuzana Kukelova*, Microsoft Research Cambridge; Jan Heller, Czech Technical University; Martin Bujnak, Capturing Reality s.r.o.; Andrew Fitzgibbon, Microsoft Research; Tomas Pajdla, Czech Technical University in Prague</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2281</Data></Cell>
    <Cell><Data ss:Type="String">Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently  significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort.</Data></Cell>
    <Cell><Data ss:Type="String">George Papandreou, Google Inc.; Liang-Chieh Chen*, UCLA; Kevin Murphy, Google Inc.; Alan Yuille, &quot;UCLA, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2301</Data></Cell>
    <Cell><Data ss:Type="String">Learning a Descriptor-specific 3D Keypoint Detector</Data></Cell>
    <Cell><Data ss:Type="String">Keypoint detection represents the first stage in the majority of modern computer vision pipelines based on automatically established correspondences between local descriptors. However, no standard solution has emerged yet in the case of 3D data such as point clouds or meshes, which exhibit high variability in level of detail and noise. More importantly, existing proposals for 3D keypoint detection rely on geometric saliency functions that attempt to maximize repeatability rather than distinctiveness of the selected regions, which may lead to sub-optimal performance of the overall pipeline. To overcome these shortcomings, we cast 3D keypoint detection as a binary classification between points whose support can be correctly matched by a predefined 3D descriptor or not, thereby learning a descriptor-specific detector that adapts seamlessly to different scenarios. Through  experiments on several public datasets, we show that this novel approach to the design of a keypoint detector represents a flexible solution that, nonetheless, can provide state-of-the-art descriptor matching performance.</Data></Cell>
    <Cell><Data ss:Type="String">Samuele Salti*, University of Bologna; Federico Tombari, University of Bologna; Riccardo Spezialetti, University of Bologna; Luigi Di Stefano, University of Bologna</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2314</Data></Cell>
    <Cell><Data ss:Type="String">Component-wise modeling of articulated objects</Data></Cell>
    <Cell><Data ss:Type="String">We introduce a novel framework for modeling articulated objects based on the aspects of their components. By decomposing the object into components, we divide the problem in smaller modeling tasks. After obtaining 3D models for each component aspect by employing a shape deformation paradigm, we merge them together forming the object components.  The final model is obtained by assembling the components using an optimization scheme which fits the respective 3D models to the corresponding apparent contours in a reference pose.   The results suggest that our approach can produce realistic 3D models of articulated objects in reasonable time.</Data></Cell>
    <Cell><Data ss:Type="String">Valsamis Ntouskos*, Sapienza University of Rome; Marta Sanzari, Sapienza University of Rome; Bruno Cafaro, &quot;Sapienza&quot; University of Rome; Federico Nardi, Sapienza University of Rome; Fabrizio Natola, Sapienza; Fiora Pirri, university of rome, Sapienza; Manuel Ruiz Garcia, Sapienza University of Rome</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2316</Data></Cell>
    <Cell><Data ss:Type="String">Semi-supervised Zero-Shot Classification with Label Representation Learning</Data></Cell>
    <Cell><Data ss:Type="String">Given the challenge of gathering labeled training data, zero-shot classification, which transfers information from observed classes to unseen classes, has become increasingly popular. Existing zero-shot learning methods require users to provide semantic visual attributes signature for each class to build connections across classes. In this paper, we propose a semi-supervised approach that can automatically learn label representations from the input data within a new max-margin classification framework. Moreover, when semantic label information is available, our proposed framework can incorporate them conveniently. We conduct experiments on a few standard zero-shot data sets. The results show the proposed approach outperforms the state-of-the-art methods.     </Data></Cell>
    <Cell><Data ss:Type="String">Xin Li, Temple University; Yuhong Guo*, Temple University; Dale Schuurmans, University of Alberta</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2321</Data></Cell>
    <Cell><Data ss:Type="String">Depth Selective Camera: A Direct, On-chip, Programmable Technique for Depth Selectivity in Photography</Data></Cell>
    <Cell><Data ss:Type="String">Time of flight (ToF) cameras use a temporally modulated light source and measure correlation between the reflected light and a sensor modulation pattern, in order to infer scene depth. In this paper, we show that such correlational sensors can also be used to selectively accept or reject light rays from certain scene depths. The basic idea is to carefully select illumination and sensor modulation patterns such that the correlation is non-zero only in the selected depth range – thus light reflected from objects outside this depth range do not affect the correlational measurements. We demonstrate a prototype depth-selective camera and highlight two potential applications: imaging through scattering media and virtual blue screening. This depthselectivity can be used to reject back-scattering and reflection from media in front of the subjects of interest, thereby significantly enhancing the ability to image through scattering media- critical for applications such as car navigation in fog and rain. Similarly, such depth selectivity can also be utilized as a virtual blue-screen in cinematography by rejecting light reflecting from background, while selectively retaining light contributions from the foreground subject.</Data></Cell>
    <Cell><Data ss:Type="String">Ryuichi Tadano*, Sony Corporation; Adithya Pediredla, Rice University; Ashok Veeraraghavan, Rice University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2355</Data></Cell>
    <Cell><Data ss:Type="String">A Supervised Low-rank Method for Learning Invariant Subspaces</Data></Cell>
    <Cell><Data ss:Type="String">Sparse representation and low-rank matrix decomposition approaches have been successfully applied to several computer vision problems. They build a generative representation of the data, which often requires complex training as well as testing to be robust against data variations induced by nuisance factors. We build on the same tools, but model and learn the invariant components, a representation invariant to nuisance factors. This enables a very efficient and robust classification based on simple nearest neighbor. We develop a framework based on geometry that ensures a uniform inter-class separation. In addition, we show how the approach is equivalent to a local metric learning, where the local metrics, one for each class, are learned jointly, and not independently, thus avoiding the risk of overfitting without the need for additional regularization. We evaluated the approach for face recognition with highly corrupted training and testing data, obtaining very promising results.</Data></Cell>
    <Cell><Data ss:Type="String">Gianfranco Doretto*, West Virginia University, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2372</Data></Cell>
    <Cell><Data ss:Type="String">Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts</Data></Cell>
    <Cell><Data ss:Type="String">Formulations of the image decomposition problem as a multicut problem (MP) w.r.t. a superpixel graph have received considerable attention. In contrast, instances of the MP w.r.t. a pixel grid graph have received little attention, firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are hard to solve in practice, and, secondly, due to the lack of long-range terms in the objective function of the MP. We propose a generalization of the MP with long-range terms (LMP). We design and implement two efficient algorithms (primal feasible heuristics) for the MP and LMP which allow us to study instances of both problems w.r.t. the pixel grid graphs of the images in the BSDS-500 benchmark. The decompositions we obtain do not differ significantly from the state of the art at the time of writing, suggesting that the LMP is a competitive formulation of the image decomposition problem. To demonstrate the generality of the LMP formulation, we apply it also to the mesh decomposition problem posed by the Princeton benchmark, obtaining state-of-the-art decompositions.</Data></Cell>
    <Cell><Data ss:Type="String">Margret Keuper, University of Freiburg; Evgeny Levinkov, MPI Informatics; Nicolas Bonneel, CNRS Lyon; Guillaume Lavoue, CNRS Lyon; Thomas Brox, &quot;University of Freiburg, Germany&quot;; Bjoern Andres*, MPI Informatics</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2384</Data></Cell>
    <Cell><Data ss:Type="String">Recursive Frechet Mean Computation on the Grassmann and its Applications to Computer Vision</Data></Cell>
    <Cell><Data ss:Type="String">In the past decade, the Grassmann manifolds have been commonly used in  mathematical formulations of many Computer Vision tasks. Averaging  points on the Grassmann manifold is a very common operation in many  applications including but not limited to, tracking, action  recognition, video-face recognition, face recognition, etc. Computing  the intrinsic/Fr\'{e}chet mean (FM) of a set of points on the  Grassmann can be cast as finding the global optimum (if it exists) of  the sum of squared geodesic distances cost function. A common approach  to solve this problem involves the use of the gradient descent method.  An alternative way to compute the FM is to develop a  recursive/inductive definition that does not involve optimizing the  aforementioned cost function.  In this paper, we propose one such  computationally efficient algorithm called the {\it Grassmann    inductive Fr\'{e}chet mean estimator} (GiFME). In developing the  recursive solution to find the FM of the given set of points, GiFME  exploits the fact that there is a closed form solution to find the FM  between two points on the Grassmann.  In the limit as the number of  samples tends to infinity, we prove that GiFME converges to the FM  (this is called the weak consistency result on the Grassmann  manifold).  Further, for the finite sample case, in the limit as the  number of sample paths (trials) goes to infinity, we show that GiFME  converges to the finite sample FM. Moreover, we derive a bound on the  geodesic distance between the estimate from GiFME and the true FM. We  present several experiments on synthetic and real data sets to  demonstrate the performance of GiFME in comparison to the gradient  descent based (batch mode) technique. Our goal in these applications  is to demonstrate the computational advantage and achieve comparable  accuracy to the state-of-the-art.</Data></Cell>
    <Cell><Data ss:Type="String">Rudrasis Chakraborty*, University of Florida; Baba Vemuri, &quot;University of Florida, Gainesville, Fl., USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2401</Data></Cell>
    <Cell><Data ss:Type="String">A collaborative filtering approach to real-time hand pose estimation</Data></Cell>
    <Cell><Data ss:Type="String">Collaborative filtering aims to predict unknown user ratings in a recommender system by collectively assessing known user preferences. In this paper, we first draw analogies between collaborative filtering and the pose estimation problem. Specifically, we recast the hand pose estimation problem as the cold-start problem for a new user with unknown item ratings in a recommender system. Inspired by fast and accurate matrix factorization techniques for collaborative filtering, we develop a real-time algorithm for estimating the hand pose from RGB-D data of a commercial depth camera. First, we efficiently identify nearest neighbors using local shape descriptors in the RGB-D domain from a library of hand poses with known pose parameter values. We then use this information to evaluate the unknown pose parameters using a joint matrix factorization and completion (JMFC) approach. Our quantitative and qualitative results suggest that our approach is robust to variation in hand configurations while achieving real time performance (29 FPS}) on a standard computer.</Data></Cell>
    <Cell><Data ss:Type="String">Chiho Choi*, Purdue University; Ayan Sinha, Purdue University; Joon Hee Choi, Purdue University; Sujin Jang, Purdue University; Karthik Ramani, Purdue University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2405</Data></Cell>
    <Cell><Data ss:Type="String">Self-calibration of optical lenses</Data></Cell>
    <Cell><Data ss:Type="String">Even high-quality lenses suffer from optical aberrations,  especially when used at full aperture. Furthermore, there  are significant lens-to-lens deviations due to manufactur-  ing tolerances, often rendering current software solutions  like DxO, Lightroom, and PTLens insufficient as they don’t  adapt and only include generic lens blur models.  We propose a method that enables the self-calibration of  lenses from a natural image, or a set of such images. To this  end we develop a machine learning framework that is able  to exploit several recorded images and distills the available  information into an accurate model of the considered lens.</Data></Cell>
    <Cell><Data ss:Type="String">Michael Hirsch*, Max Planck Inistute for Intelligent Systems; Bernhard Schölkopf, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2419</Data></Cell>
    <Cell><Data ss:Type="String">Multi-Modal Subspace Clustering</Data></Cell>
    <Cell><Data ss:Type="String">For many machine learning applications, the data sets distribute on some low dimensional subspaces. Subspace clustering is to find such underlying subspaces and classify the data points correctly. In this paper, we propose a novel multi-modal subspace clustering method. The proposed method performs clustering on the subspace representation of each modality simultaneously. Meanwhile, we propose to use a common cluster structure to guarantee the consistence among different modalities. In addition, an efficient algorithm is proposed to solve the problem. Extensive experiments have been performed to validate our proposed method. The promising results demonstrate the effectiveness of our method.</Data></Cell>
    <Cell><Data ss:Type="String">Hongchang Gao, University of Texas at Arlington; Feiping Nie , Northwestern Polytechnical University; Heng Huang*, University of Texas at Austin</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2423</Data></Cell>
    <Cell><Data ss:Type="String">Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions</Data></Cell>
    <Cell><Data ss:Type="String">One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo-attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end using the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves.  Our empirical results show that the proposed model significantly outperforms previous methods.</Data></Cell>
    <Cell><Data ss:Type="String">Jimmy Ba*, University of Toronto; Kevin Swersky, University of Toronto; Ruslan salakhutdinov, ; Sanja Fidler, University of Toronto</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2431</Data></Cell>
    <Cell><Data ss:Type="String">A Projection free method for Generalized Eigenvalue Problem with a nonsmooth Regularizer</Data></Cell>
    <Cell><Data ss:Type="String">Eigenvalue problems are ubiquitous in computer vision, covering a very broad spectrum of applications ranging from estimation problems in multi-view geometry to image segmentation. Few other linear algebra problems have a more mature set of numerical routines available and almost all modern computer vision libraries leverage such tools extensively. But progressively, as the needs of the computer vision or image analysis applications evolve, the ability to call the underlying solver only as a ``black box'' can often become restrictive. Many `human in the loop' settings in vision frequently exploit supervision from an expert, to the extent that the user can be considered a subroutine in the overall system. In other cases, there is additional domain knowledge, side information and even partial information that one may want to incorporate within the formulation somehow. In general,regularizing a (generalized) eigenvalue problem with such side information remains difficult. Motivated by these needs, this paper presents   an optimization scheme to solve generalized eigenvalue problems (GEP) involving a (potentially non-smooth) regularizer. We start from an alternative formulation of GEP where the feasibility set of the model involves the Stiefel manifold. The core of this paper presents an end to end stochastic optimization scheme for the resultant problem. While the framework is very general, as a concrete example, we show how the algorithm enables improved statistical analysis of brain imaging data where the regularizer   is derived from other `views' of the disease pathology, involving clinical measurements and other image-derived representations.  </Data></Cell>
    <Cell><Data ss:Type="String">Seong Jae Hwang*, Univ. of Wisconsin - Madison; Maxwell Collins, UW-Madison; Sathya Ravi, University of Wisconsin Madiso; Vamsi Ithapu, ; Nagesh Adluru, University of Wisconsin - Madison; Sterling Johnson, ; Vikas Singh, University of Wisconsin-Madison</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2458</Data></Cell>
    <Cell><Data ss:Type="String">Optimizing expected Intersection-over-Union with candidate-constrained CRFs</Data></Cell>
    <Cell><Data ss:Type="String">We study the question of how to make loss-aware predictions in image  segmentation settings where the target evaluation function is the  Intersection-over-Union (IoU) score that is used widely in evaluating  image segmentation systems [Everingham et al.]. There are two  dominant approaches. First, approximately perform Bayesian  decision theory by approximating the expected IoU score as expected  intersection over expected union (EIoEU)  [Nowozin, Tarlow et al.].  Second,  compute exact expected IoU but only over a small set of high-quality  candidate solutions [Premachandran et al.].  We begin by asking which approach we should favor for two  typical image segmentation tasks. Studying this question leads   to two new methods that draw ideas from both. The new methods use the   expected IoU approximation of the expected loss paired with  the high quality candidate solutions from [Premachandran et al.]  in two different ways. Experimentally we show that the new approaches  lead to improved performance on both image segmentation tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Faruk Ahmed*, Virginia Tech; Vittal Premachandran, UCLA; Dany Tarlow, MSR Cambridge; Dhruv Batra, Virginia Tech</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2517</Data></Cell>
    <Cell><Data ss:Type="String">Structured Feature Selection</Data></Cell>
    <Cell><Data ss:Type="String">Feature dimensionality reduction has been widely used in various computer vision tasks. We explore feature selection as the dimensionality reduction technique and propose to use a structured approach, based on the Markov Blanket (MB), to select features. We first introduce a new MB discovery algorithm, Simultaneous Markov Blanket (STMB) discovery algorithm,  that improves the efficiency of state-of-the-art algorithms. Then we theoretically justify three advantages of structured feature selection over traditional feature selection methods. Specifically, we show that the Markov Blanket is the minimum feature set that retains the maximal mutual information and also gives the lowest Bayes classification error. We then introduce a new method that enables MB-based feature selection algorithms to scale up. We apply the proposed MB discovery method to feature selection problems and show the competitive performance of our algorithm on large-scale image classification tasks. We also demonstrate the application of the proposed structured feature selection algorithm for hierarchical features and show the proposed method can lead to big performance gain with hierarchical features in facial expression and action unit (AU) recognition tasks. </Data></Cell>
    <Cell><Data ss:Type="String">Tian Gao*, RPI; Ziheng Wang, RPI; Qiang Ji, RPI</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2557</Data></Cell>
    <Cell><Data ss:Type="String">Higher-Order Inference for Multi-class Log-supermodular Models</Data></Cell>
    <Cell><Data ss:Type="String">Higher-order models have been shown to be very useful for a plethora of computer vision tasks. However, existing techniques have focused mainly on MAP inference. In this paper, we present the first efficient approach towards approximate Bayesian marginal inference in a general class of high-order, multi-label attractive models, where previous techniques slow down exponentially with the order (clique size). We formalize this task as performing inference in log-supermodular models under partition matroid constraints, and present an efficient variational inference technique. The resulting optimization problems are convex and yield bounds on the partition function. We also obtain a fully factorized approximation to the posterior, which can be used in lieu of the true complicated distribution. We empirically demonstrate the performance of our approach by comparing it to traditional inference methods on a challenging high-fidelity multi-label image segmentation dataset. We obtain state-of-the-art classification accuracy for MAP inference, and substantially improved ROC curves using the approximate marginals.</Data></Cell>
    <Cell><Data ss:Type="String">Jian Zhang*, ETH Zurich; Josip Djolonga, ETH Zurich; Andreas Krause, ETH Zurich</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2559</Data></Cell>
    <Cell><Data ss:Type="String">Semantic Video Entity Linking based on Visual Content and Metadata</Data></Cell>
    <Cell><Data ss:Type="String">Video entity linking, which connects online videos to the related entities in a semantic knowledge base, can enable a wide variety of video based applications including video retrieval and video recommendation. Most existing systems for video entity linking rely on video metadata. In this paper, we propose to exploit video visual content to improve video entity linking. In the proposed framework, videos are first linked to entity candidates using a text-based method. Next, the entity candidates are verified and reranked according to visual content. In order to properly handle large variations in visual content matching, we propose to use Multiple Instance Metric Learning to learn a &quot;set to sequence'' metric for this specific matching problem. To evaluate the proposed framework, we collect and annotate 1920 videos crawled from the YouTube open API. Experiment results have shown consistent gains by the proposed framework over several strong baselines.</Data></Cell>
    <Cell><Data ss:Type="String">Yuncheng Li*, University of Rochester; Xitong Yang, ; Jiebo Luo, University of Rochester</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2578</Data></Cell>
    <Cell><Data ss:Type="String">On the Equivalence of Moving Entrance Pupil and Radial Distortion for Camera Calibration</Data></Cell>
    <Cell><Data ss:Type="String">Radial distortion for ordinary (non-fisheye) camera lenses has traditionally been  modeled as an infinite series function of radial location of an image pixel  from the image center. While there has been enough empirical evidence   to show that such a model is accurate and sufficient for radial distortion calibration,  there has not been much analysis on the geometric/physical understanding of radial distortion  from a camera calibration perspective.  In this paper, we show using a thick-lens imaging model, that the variation of  entrance-pupil location as a function of incident image ray angle is directly  responsible for radial distortion in captured images. Thus, unlike as proposed  in the current state-of-the-art in camera calibration, radial distortion   and entrance pupil movement are equivalent and need not be modeled  together.    By modeling only entrance-pupil motion instead of radial distortion,   we achieve two main benefits;   first, we obtain comparable  if not better pixel re-projection error than traditional methods; second, and more  importantly, we directly back-project a radially distorted image pixel along the   true image ray which formed it. Using a thick-lens setting, we show that such a back-projection is more   accurate than the two-step method of undistorting an image pixel and   then back-projecting it. We have applied this calibration method to the  problem of generative depth-from-focus using focal stack to get  accurate depth estimates. </Data></Cell>
    <Cell><Data ss:Type="String">Avinash Kumar*, &quot;University of Illinois, Urbana-Champaign&quot;; Narendra Ahuja, &quot;University of Illinois at Urbana-Champaign, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2581</Data></Cell>
    <Cell><Data ss:Type="String">Group Membership Prediction</Data></Cell>
    <Cell><Data ss:Type="String">The group membership prediction (GMP) problem involves predicting whether or not a collection of instances share a certain semantic property. For instance, in kinship verification given a collection of images, the goal is to predict whether or not they share a familial relationship. In this context we propose a novel probability model and introduce latent view-specific and view-shared random variables to jointly account for the view-specific appearance and cross-view similarities among data instances. Our model posits that data from each view is independent conditioned on the shared variables. This postulate leads to a parametric probability model that decomposes group membership likelihood into a tensor product of data-independent parameters and data-dependent factors. We propose learning the data-independent parameters in a discriminative way with bilinear classifiers, and test our prediction algorithm on challenging visual recognition tasks such as multi-camera person re-identification and kinship verification. On most benchmark datasets, our method can significantly outperform the current state-of-the-art. </Data></Cell>
    <Cell><Data ss:Type="String">Ziming Zhang*, Boston University; Yuting Chen, Boston University; Venkatesh Saligrama, Boston University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2586</Data></Cell>
    <Cell><Data ss:Type="String">A Deep Visual Correspondence Embedding Model for Stereo Matching Costs</Data></Cell>
    <Cell><Data ss:Type="String">Stereo algorithms heavily rely on the matching cost for measuring the similarity between image patches. Nearly all the existing matching costs require some prior knowledge, such as the basic color-consistency assumption, or more robust ordering-constraints in some heuristically-designed transform, e.g. Census transform. In practice, however, the performance of traditional matching costs can degrade severely due to many complex factors which can hardly be modeled, such as high-light, shadow, bad weather etc. To overcome the limits of traditional methods, we in this paper propose a novel data-driven matching cost leveraging the powerful convolutional neural network (CNN).    Different from previous learning-based methods, our cost is efficiently obtained via simple dot product in an embedding feature space. To project the features, a transform is learned from the massive local patches, which greatly reduce the matching ambiguities and is independent of disparity levels. Further more, our learned transform generalize surprisingly well even on completely unseen scenarios. Experimental results on both KITTI and Middlebury data sets demonstrate the effectiveness of our new matching cost: combined with a generic semi-global matching based framework, results from our method rank in top 3 among all the two-frame algorithms on KITTI benchmark; Trained with KITTI data (outdoor), our cost can also work well on Middlebury data (indoor). The authors believe that this work is a solid step towards the practical learning-based stereo approach.  </Data></Cell>
    <Cell><Data ss:Type="String">Zhuoyuan Chen*, Baidu USA LLC; Xun Sun, Baidu; Yinan Yu, Baidu IDL; Wang Liang, Baidu; Chang Huang, Baidu; Haoyuan Gao, Baidu; Kai Yu, Baidu</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2598</Data></Cell>
    <Cell><Data ss:Type="String">A Linear Generalized Camera Calibration from Three Intersecting Reference Planes</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a new generalized (or ray-pixel,  raxel) camera calibration algorithm for camera systems  involving distortions by unknown refraction and reflection  processes. The key idea is use of intersections of calibration  planes, while conventional methods utilized collinearity  constraints of points on the planes. We show that intersections  of calibration planes can realize a simple linear  algorithm, and that our method can be applied to any distributions  while conventional methods require knowing the  ray-distribution class in advance. Evaluations using synthesized  and real datasets demonstrate the performance of  our method quantitatively and qualitatively.</Data></Cell>
    <Cell><Data ss:Type="String">Mai Nishimura, Kyoto University; Shohei Nobuhara*, Kyoto University; Takashi Matsuyama, Kyoto University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2605</Data></Cell>
    <Cell><Data ss:Type="String">Conditional High-order Boltzmann Machine: A Supervised Learning Model for Relation Learning</Data></Cell>
    <Cell><Data ss:Type="String">Relation learning is a fundamental operation in many  computer vision tasks. Recently, high-order Boltzmann machine  and its variants have exhibited the great power of  modelling various data relationships. However, most of  them are unsupervised learning models which are not very  discriminative and thus cannot servers as a standalone solution  to relation learning tasks. In this paper, we explore  supervised learning algorithms and propose a new model  named Conditional High-order Boltzmann Machine (CHBM),  which can be directly used as a bilinear classifier to  assign similarity scores for pairwise images. Then, to better  deal with complex data relationships, we propose a gated  version of CHBM which untangles factors of variation  by exploiting a set of latent variables to gate classification.  We perform four-order tensor factorization for parameter  reduction, and present two efficient supervised learning algorithms  from the perspectives of being generative and discriminative,  respectively. The experimental results of image  transformation visualization, binary-way classification and  face verification demonstrate that, by perform supervised  learning, our models can greatly improve the performance.</Data></Cell>
    <Cell><Data ss:Type="String">Yan Huang*, CRIPAC, CASIA; Wei Wang, NLPR,CASIA; Liang Wang, unknown</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2616</Data></Cell>
    <Cell><Data ss:Type="String">Text Flow: A Unified Text Detection System in Natural Scene Images</Data></Cell>
    <Cell><Data ss:Type="String">The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false character candidate removal, text line extraction, and  text line verification. However, errors occur and accumulate throughout each of these sequential steps which often lead to low detection performance. To address these issues, we propose a unified scene text detection system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With character candidates detected by cascade boosting, the min-cost flow network model integrates the last three sequential steps into a single process which solves the error accumulation problem at both character level and text line level effectively. The proposed technique has been tested on three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a multilingual dataset and it outperforms the state-of-the-art methods on all three datasets with much higher recall and F-score. The good performance on the multilingual dataset shows that the proposed technique can be used for the detection of texts in different languages.</Data></Cell>
    <Cell><Data ss:Type="String">Shangxuan Tian*, NUS; Yifeng Pan, ; Chang Huang, Baidu; Shijian Lu, Institute for Infocomm Research; Kai Yu, Baidu; Chew Lim Tan, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2646</Data></Cell>
    <Cell><Data ss:Type="String">Illumination Robust Color Naming via Label Propagation</Data></Cell>
    <Cell><Data ss:Type="String">Color composition is an important property for many computer vision tasks like image retrieval and object classification. In this paper we address the problem of inferring the color composition of the intrinsic reflectance of objects, where the shadows and highlights may change the observed color dramatically. We achieve this through color label propagation without recovering the intrinsic reflectance beforehand. Specifically, the color labels are propagated between regions sharing the same reflectance, and the direction of propagation is promoted to be from regions under full illumination and normal view angles to abnormal ones. We identify shadowed and highlighted regions as well as pairs of regions which have similar reflectance through detection. A joint inference process is adopted to trim the inconsistent identities and connections. For evaluation we collect two datasets with images under noticeable highlights and shadows. Experimental results show that our model can effectively describe the color composition of visual objects in real-world images.</Data></Cell>
    <Cell><Data ss:Type="String">yuanliu liu, Xi'an jiaotong University; Zejian Yuan*, Xi'an Jiaotong University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2647</Data></Cell>
    <Cell><Data ss:Type="String">Towards  Pointless Structure from Motion: 3D reconstruction and camera calibration from general 3D curves</Data></Cell>
    <Cell><Data ss:Type="String">Modern structure from motion (SfM) remains dependent on point features to recover camera positions, meaning that reconstruction is severely hampered in low-texture environments, for example scanning a plain coffee cup on an uncluttered table.    We show how 3D curves can be used to refine camera position estimation in challenging low-texture scenes.  In contrast to previous work, we allow the curves to be partially observed in all images, meaning that for the first time, curve-based SfM can be demonstrated in realistic scenes.    Furthermore, as has been previously shown, curves are a qualitatively richer class of 3D reconstruction, so an additional benefit of our research is to allow reconstruction of partially occluded curves even with fixed cameras.    The algorithm is a variant of bundle adjustment, so needs an initial estimate, but a poor estimate from a few point correspondences can be substantially improved by including curves, suggesting that this method would benefit many existing systems.  </Data></Cell>
    <Cell><Data ss:Type="String">Irina Nurutdinova, TU Berlin; Andrew Fitzgibbon*, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2650</Data></Cell>
    <Cell><Data ss:Type="String">Learning Image and User Features for Recommendation in Social Networks</Data></Cell>
    <Cell><Data ss:Type="String">We present a novel deep method for learning unified feature representations for both images and users in large social networks, in particular, social curation networks, where the extremely sparse links between users and images, and the extremely diverse visual contents of images are posing a great challenge for traditional recommender systems. Our approach tackles these two extremes by transforming the heterogeneous users and images into a homogeneous low-dimensional feature representations, which facilitates a recommender to trivially recommend images to users by feature similarity. Like any other ``end-to-end'' deep models, we start from modelling the most basic social behavior such as user ``likes'' or ``dislikes'' of images into a forward propagation along the deep architecture, where the latent features accumulates the belief of the behavior during the propagation. We develop a fast on-line algorithm that can easily scale up to large networks in an asynchronously parallel fashion. We conduct extensive experiments on a representative subset of Pinterest. The subset contains 1,456,540 image items and 1,000,000 users from Pinterest. Consistent improvements on several runs of image and user recommendation experiments demonstrate that our approach significantly outperforms other state-of-the-art recommendation methods.</Data></Cell>
    <Cell><Data ss:Type="String">Xue Geng, NUS; Hanwang Zhang*, NUS; Jingwen Bian, NUS; Tag-Seng Chua, NUS</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2681</Data></Cell>
    <Cell><Data ss:Type="String">Contour Box: Rejecting Object Proposals Without Explicit Closed Contours</Data></Cell>
    <Cell><Data ss:Type="String">Closed contours constitute one of the important objectness indicators. We propose a new closed contour measure subject to the completeness and tightness constraints, where the optimized closed contour should be tightly bounded within an object proposal.  The closed contour measure is defined using closed path integral, and we solve the optimization problem efficiently in polar coordinates with a global optimum guaranteed. Extensive experiments show that our method can reject a large number of false proposals, and achieve over 6% improvement in object recall at the challenging overlap threshold 0.8 on the VOC 2007 test dataset.   </Data></Cell>
    <Cell><Data ss:Type="String">Cewu Lu, HKUST; ChiKeung Tang*, &quot;The Hong Kong University of  Science and Technology, Hong Kong&quot;</Data></Cell>
   </Row>
  </Table>
  <WorksheetOptions xmlns="urn:schemas-microsoft-com:office:excel">
   <PageLayoutZoom>0</PageLayoutZoom>
   <Selected/>
   <ProtectObjects>False</ProtectObjects>
   <ProtectScenarios>False</ProtectScenarios>
  </WorksheetOptions>
 </Worksheet>
</Workbook>
