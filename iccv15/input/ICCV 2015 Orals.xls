<?xml version="1.0"?>
<Workbook xmlns="urn:schemas-microsoft-com:office:spreadsheet"
 xmlns:o="urn:schemas-microsoft-com:office:office"
 xmlns:x="urn:schemas-microsoft-com:office:excel"
 xmlns:ss="urn:schemas-microsoft-com:office:spreadsheet"
 xmlns:html="http://www.w3.org/TR/REC-html40">
 <DocumentProperties xmlns="urn:schemas-microsoft-com:office:office">
  <Version>14.0</Version>
 </DocumentProperties>
 <OfficeDocumentSettings xmlns="urn:schemas-microsoft-com:office:office">
  <AllowPNG/>
 </OfficeDocumentSettings>
 <ExcelWorkbook xmlns="urn:schemas-microsoft-com:office:excel">
  <WindowHeight>6240</WindowHeight>
  <WindowWidth>10000</WindowWidth>
  <WindowTopX>120</WindowTopX>
  <WindowTopY>140</WindowTopY>
  <ProtectStructure>False</ProtectStructure>
  <ProtectWindows>False</ProtectWindows>
 </ExcelWorkbook>
 <Styles>
  <Style ss:ID="Default" ss:Name="Normal">
   <Alignment ss:Vertical="Bottom"/>
   <Borders/>
   <Font ss:FontName="Calibri" x:Family="Swiss" ss:Size="12" ss:Color="#000000"/>
   <Interior/>
   <NumberFormat/>
   <Protection/>
  </Style>
  <Style ss:ID="s62">
   <Alignment ss:Vertical="Bottom"/>
   <Borders/>
   <Font x:Family="Swiss" ss:Bold="1"/>
   <Interior/>
   <NumberFormat/>
   <Protection/>
  </Style>
 </Styles>
 <Worksheet ss:Name="ICCV2015">
  <Table ss:ExpandedColumnCount="4" ss:ExpandedRowCount="59" x:FullColumns="1"
   x:FullRows="1" ss:DefaultColumnWidth="65" ss:DefaultRowHeight="15">
   <Row>
    <Cell><Data ss:Type="String">ICCV2015</Data></Cell>
   </Row>
   <Row ss:Index="3">
    <Cell ss:StyleID="s62"><Data ss:Type="String">Paper ID</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Paper Title</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Abstract</Data></Cell>
    <Cell ss:StyleID="s62"><Data ss:Type="String">Author Names</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">67</Data></Cell>
    <Cell><Data ss:Type="String">Multi-task Recurrent Neural  Network for Immediacy Prediction</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we propose the problem of predicting immediacy  for interacting persons from still images. A complete  immediacy set includes interactions, relative distance,  body leaning direction and standing orientation. These  measures are found to be related to the attitude, social relationship, social interaction, action, nationality, and religion  of the communicators. A large-scale dataset with 10; 000  images is constructed, in which all the immediacy measures  and the human poses are annotated. We propose a rich set  of immediacy representations that help to predict immediacy  from imperfect 1-person and 2-person pose estimation  results. A multi-task deep recurrent neural network is constructed  to take the proposed rich immediacy representation  as input and learn the complex relationship among immediacy  predictions multiple steps of refinement. The effectiveness  of the proposed approach is proved through extensive  experiments on the large scale dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Xiao Chu*, The Chinese University of HK; Wei Yang, CUHK; Wanli Ouyang, Chinese University of Hong Kong; Xiaogang Wang, The Chinese University of Hong Kong, Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">106</Data></Cell>
    <Cell><Data ss:Type="String">SPM-BP: Sped-up PatchMatch Belief Propagation for Continuous MRFs</Data></Cell>
    <Cell><Data ss:Type="String">Markov random fields are widely used to model many computer vision problems  that can be cast in an energy minimization  framework composed of unary and pairwise potentials. While  computationally tractable discrete optimizers such as Graph Cuts and  belief propagation (BP) exist for multi-label discrete problems,  they still face prohibitively high computational challenges when the  labels reside in a huge or very densely sampled space. Integrating  key ideas from PatchMatch of effective particle propagation and  resampling, PatchMatch belief propagation (PMBP) has been  demonstrated to have good  performance in addressing continuous labeling problems and runs  orders of magnitude faster than Particle BP (PBP). However, the  quality of the PMBP solution is tightly coupled with the local window size, over  which the raw data cost is aggregated to mitigate ambiguity in the  data constraint. This dependency heavily influences the overall  complexity, increasing linearly with the window size. This paper  proposes a novel algorithm called sped-up PMBP (SPM-BP) to tackle  this critical computational bottleneck and speeds up PMBP by 50-100 times.  The crux of SPM-BP is on unifying efficient  filter-based cost aggregation and message passing with  PatchMatch-based particle generation in a highly effective way.  Though simple in its formulation, SPM-BP achieves superior  performance for sub-pixel accurate stereo and optical-flow on  benchmark datasets when compared with more complex and  task-specific approaches.</Data></Cell>
    <Cell><Data ss:Type="String">YU LI*, NUS; Dongbo Min, Chungnam National University; Jiangbo Lu, ADSC Singapore; Michael Brown, &quot;National University of Singapore, Singapore&quot;; Minh Do, University of Illinois at Urbana-Champaign</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">140</Data></Cell>
    <Cell><Data ss:Type="String">3D Time-Lapse Reconstruction from Internet Photos</Data></Cell>
    <Cell><Data ss:Type="String">Given an Internet photo collection of a landmark, we compute a 3D time-lapse video sequence where a virtual camera moves continuously in time and space. While previous work assumed a static camera, the addition of camera motion during the time-lapse creates a very compelling impression of parallax. Achieving this goal, however, requires addressing multiple technical challenges, including solving for time-varying depth maps, regularizing 3D point color profiles over time, and reconstructing high quality, hole-free images at every frame from the projected profiles. Our results show photo-realistic time-lapses of skylines and natural scenes over many years, with compelling parallax effects.</Data></Cell>
    <Cell><Data ss:Type="String">Ricardo Martin*, University of Washington; Steve Seitz, Washington/Google; David Gallup, Google</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">141</Data></Cell>
    <Cell><Data ss:Type="String">Shape Interaction Matrix Revisited and Robustified: Efficient Subspace Clustering with Corrupted and Incomplete Data</Data></Cell>
    <Cell><Data ss:Type="String">The Shape Interaction Matrix (SIM) is one of the earliest approaches to performing subspace clustering (i.e., separating points drawn from a union of subspaces). In this paper, we revisit the SIM and reveal its connections to several recent subspace clustering methods. Our analysis lets us derive a simple, yet effective algorithm to robustify the SIM and make it applicable to realistic scenarios where the data is corrupted by noise. We justify our method by intuitive examples and the matrix perturbation theory. We then show how this approach can be extended to handle missing data, thus yielding an efficient and general subspace clustering algorithm. We demonstrate the benefits of our approach over state-of-the-art subspace clustering methods on several challenging motion segmentation and face clustering problems, where the data includes corruptions and missing measurements.</Data></Cell>
    <Cell><Data ss:Type="String">Pan Ji*, Australian National University; Mathieu Salzmann, NICTA; Hongdong Li, &quot;Australian National University, Australia&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">145</Data></Cell>
    <Cell><Data ss:Type="String">Panoptic Studio: A Massively Multiview System for Social Motion Capture</Data></Cell>
    <Cell><Data ss:Type="String"> We present an approach to capture the 3D structure and motion of a group of people engaged in a social interaction. The core challenges in capturing social interactions are: (1) occlusion is functional and frequent; (2) subtle motion needs to be measured over a space large enough to host a social group; and (3) human appearance and configuration variation is immense. The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the integration of perceptual analyses over a large variety of view points. We present a modularized system designed around this principle, consisting of integrated structural, hardware, and software innovations. The system takes, as input, 480 synchronized video streams of multiple people engaged in social activities, and produces, as output, the labeled time-varying 3D structure of anatomical landmarks on individuals in the space. The algorithmic contributions include a hierarchical approach for generating skeletal trajectory proposals, and an optimization framework for skeletal reconstruction with trajectory re-association.</Data></Cell>
    <Cell><Data ss:Type="String">Hanbyul  Joo*, Carnegie Mellon University; Hao Liu, Ocean University of China; Lei Tan, Hunan University; Lin Gui, Ocean University of China; Shohei Nobuhara, Kyoto University; Yaser Sheikh, Carnegie Mellon University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">166</Data></Cell>
    <Cell><Data ss:Type="String">Holistically-Nested Edge Detection</Data></Cell>
    <Cell><Data ss:Type="String">We develop a new edge detection algorithm that tackles two critical issues in this long-standing vision problem: (1) holistic image training; and (2) multi-scale feature learning. Our proposed method, holistically-nested edge detection (HED), turns pixel-wise edge classification into image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets.  HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are crucially important in order to approach the human ability to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of $0.782$) and the NYU Depth dataset (ODS F-score of $0.746$), and do so with an improved speed ($0.4$ second per image) that is orders of magnitude faster than recent CNN-based edge detection algorithms.</Data></Cell>
    <Cell><Data ss:Type="String">Saining Xie, UC San Diego; Zhuowen  Tu*, &quot;UCSD, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">173</Data></Cell>
    <Cell><Data ss:Type="String">Learning image representations equivariant to ego-motion</Data></Cell>
    <Cell><Data ss:Type="String">Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond systematically to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning system significantly out-performs previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in a disjoint domain.</Data></Cell>
    <Cell><Data ss:Type="String">Dinesh Jayaraman*, UT Austin ; Kristen Grauman, University of Texas at Austin</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">184</Data></Cell>
    <Cell><Data ss:Type="String">Robust Nonrigid Registration by Convex Optimization</Data></Cell>
    <Cell><Data ss:Type="String">We present an approach to nonrigid registration of 3D surfaces. We cast isometric embedding as MRF optimization and apply efficient global optimization algorithms based on linear programming relaxations. The MRF perspective suggests a natural connection with robust statistics and motivates robust forms of the intrinsic distortion functional. Our approach outperforms a large body of prior work by a significant margin, increasing registration precision on the challenging FAUST dataset by a factor of 3.</Data></Cell>
    <Cell><Data ss:Type="String">Qifeng Chen, Stanford University; Vladlen Koltun*, Intel Labs</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">192</Data></Cell>
    <Cell><Data ss:Type="String">Dense Semantic Correspondence where Every Pixel is a Classifier</Data></Cell>
    <Cell><Data ss:Type="String">Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms. Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ.    Motivated by object recognition literature and recent work on rapidly estimating linear classifiers, we treat semantic correspondence as a constrained detection problem, where an exemplar LDA classifier is learned for each pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher average precision than similarity metrics typically used in correspondence problems, and (ii) unlike exemplar SVM, can output globally interpretable posterior probabilities without calibration, whilst also being significantly faster to train.    We pose the correspondence problem as a graphical model, where the unary potentials are computed via convolution with the set of exemplar classifiers, and the joint potentials enforce smoothly varying correspondence assignment.</Data></Cell>
    <Cell><Data ss:Type="String">Hilton Bristow*, QUT; Jack Valmadre, QUT; Simon Lucey, CMU</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">224</Data></Cell>
    <Cell><Data ss:Type="String">Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views</Data></Cell>
    <Cell><Data ss:Type="String">Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with 3D annotations, and a lack of powerful features tailored specifically. Inspired by the growing availability of 3D models, we propose a framework to address both issues by synthesizing many images from 3D models, and using them to train a CNN~\cite{LeCun2004}. The potential of 3D models in generating vast amounts of images and the high learning capacity of CNNs form a perfect marriage. We prove experimentally that the 3D knowledge learned from the \textbf{synthesized} images can be applied to the viewpoint estimation task on \textbf{real} images. By carefully designing the training image synthesis process, we efficiently generated millions of images with high appearance variation, while avoiding the necessity of any human annotation effort on the training images. We fine-tune a CNN with this data and show that it significantly outperforms existing viewpoint estimation algorithms on the challenging PASCAL VOC 2012 dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Hao Su*, Stanford; Charles R. Qi, Stanford University; Leonidas Guibas, Stanford University; Yangyan Li, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">228</Data></Cell>
    <Cell><Data ss:Type="String">Human Parsing with Contextualized Convolutional Neural Network</Data></Cell>
    <Cell><Data ss:Type="String">In this work, we address the human parsing task with a novel Contextualized Convolutional Neural Network (Co-CNN) architecture, which well integrates the cross-layer context, global image-level context, within-super-pixel context and cross-super-pixel neighborhood context into a unified network. Given an input human image, Co-CNN produces the pixel-wise categorization in an end-to-end way. First, the cross-layer context is captured by our basic local-to-global-to-local structure, which hierarchically combines the global semantic structure and the local fine details within the cross-layers. Second, the global image-level label prediction is used as an auxiliary objective in the intermediate layer of the Co-CNN, and its outputs are further used for guiding the feature learning in subsequent convolutional layers to leverage the global image-level context. Finally, to further utilize the local super-pixel contexts, the within-super-pixel smoothing and cross-super-pixel neighbourhood voting are formulated as natural sub-components of the Co-CNN to achieve the local label consistency in both training and testing process. Comprehensive evaluations on two public datasets well demonstrate the significant superiority of our Co-CNN architecture over other state-of-the-arts for human parsing. In particular, the F-1 score on the large dataset reaches 76.95% by Co-CNN, significantly higher than 62.81% and 64.38% by the state-of-the-art algorithms, M-CNN and ATR, respectively.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaodan Liang, Sun Yat-sen University; Chunyan Xu, ; Xiaohui Shen, Adobe Research; Jianchao Yang, Snapchat; Liu Si, ; jinhui Tang, ; Liang Lin*, ; Shuicheng Yan, National University of Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">361</Data></Cell>
    <Cell><Data ss:Type="String">Weakly supervised graph based semantic segmentation by learning communities of image-parts</Data></Cell>
    <Cell><Data ss:Type="String">We present a weakly-supervised approach to semantic segmentation. The goal is to assign pixel-level labels given only partial information, for example, image-level labels. This is an important problem in many application scenarios where it is difficult to get accurate segmentation or not feasible to obtain detailed annotations. The proposed approach starts with an initial coarse segmentation, followed by a spectral clustering approach that groups related image parts into communities. A community-driven graph is then constructed that captures spatial and feature relationships between communities while a label graph captures correlations between image labels. Finally, mapping the image level labels to appropriate communities is formulated as a convex optimization problem. The proposed approach does not require location information for image level labels and can be trained using partially labeled datasets. Compared to the state-of-the-art weakly supervised approaches, we achieve a significant performance improvement of 9% on MSRC-21 dataset and 11% on LabelMe dataset, while being more than 300 times faster.</Data></Cell>
    <Cell><Data ss:Type="String">Niloufar Pourian*, UCSB; Karthikeyan Shanmuga Vadivel, ; B.S. Manjunath, UCSB</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">384</Data></Cell>
    <Cell><Data ss:Type="String">Learning to Track: Online Multi-Object Tracking by Decision Making</Data></Cell>
    <Cell><Data ss:Type="String">Online Multi-Object Tracking (MOT) has wide applications in time-critical video analysis scenarios, such as robot navigation and autonomous driving. In tracking-by-detection, a major challenge of online MOT is how to robustly associate noisy object detections on a new video frame with previously tracked objects. In this work, we formulate the online MOT problem as decision making in Markov Decision Processes (MDPs), where the lifetime of an object is modeled with a MDP. Learning a similarity function for data association is equivalent to learning a policy for the MDP, and the policy learning is approached in a reinforcement learning fashion which benefits from both advantages of offline-learning and online-learning for data association. Moreover, our framework can naturally handle the birth/death and appearance/disappearance of targets by treating them as state transitions in the MDP while leveraging existing online single object tracking methods. We conduct experiments on the MOT Benchmark to verify the efficiency of our method.</Data></Cell>
    <Cell><Data ss:Type="String">Yu Xiang*, University of Michigan; Alexandre Alahi, Stanford University; Silvio Savarese, Stanford University, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">389</Data></Cell>
    <Cell><Data ss:Type="String">Learning Complexity-Aware Cascades for Deep Pedestrian Detection</Data></Cell>
    <Cell><Data ss:Type="String">The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds. </Data></Cell>
    <Cell><Data ss:Type="String">Zhaowei Cai*, UCSD; Mohammad Saberian, Yahoo Research; Nuno Vasconcelos, UC San Diego, USA</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">452</Data></Cell>
    <Cell><Data ss:Type="String">Polarized 3D: High-Quality Depth Sensing with Polarization Cues</Data></Cell>
    <Cell><Data ss:Type="String">Coarse depth maps can be enhanced by using the shape information from polarization cues. We propose a framework to combine surface normals from polarization (hereafter polarization normals) with an aligned depth map. Polarization normals have not been used for depth enhancement before. This is because polarization normals suffer from physics-based artifacts, such as azimuthal ambiguity, refractive distortion and fronto-parallel signal degradation. We propose a framework to overcome these key challenges, allowing the benefits of polarization to be used to enhance depth maps. Our results show improved performance as compared to state-of-the-art techniques in 3D reconstruction.</Data></Cell>
    <Cell><Data ss:Type="String">Achuta Kadambi*, MIT Media Lab; Vage Taamazyan, MIT Media Lab; Boxin Shi, Singapore University of Technology and Design; Ramesh Raskar, Massachusetts Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">465</Data></Cell>
    <Cell><Data ss:Type="String">Webly Supervised Learning of Convolutional Networks</Data></Cell>
    <Cell><Data ss:Type="String">In the last few years, we have made an enormous progress in learning visual representations via convolutional neural networks (CNNs). We believe CNNs get their edge due to their ability to imbibe large amounts of data. Therefore, as we move forward a key questions arises: how do we move from million image datasets to billion image counterparts? Do we continue to manually label images with the hope of scaling up the labeling to billion images? It is in this context, webly supervised learning assumes huge importance. If we can exploit the images on the web for training CNNs without manually labeling them: it will be a win-win for everyone. We present a simple yet powerful approach to exploit web data for learning CNNs. Specifically, inspired by curriculum-learning algorithms, we present a  two-step approach for learning CNNs. First, we use simple, easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder Flickr style scene images by exploiting the structure of data and categories (using a relationship graph). We demonstrate that our 2-step CNN outperforms ImageNet-pretrained AlexNet architecture; without even using a single ImageNet training label. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style detector. To the best of our knowledge, we show the best performance on VOC 2007 where no VOC training data is used. </Data></Cell>
    <Cell><Data ss:Type="String">Xinlei  Chen*, CMU; Abhinav Gupta, &quot;Carnegie Mellon University, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">497</Data></Cell>
    <Cell><Data ss:Type="String">Camera Pose Voting for Large-Scale Image-Based Localization</Data></Cell>
    <Cell><Data ss:Type="String">Image-based localization approaches aim to determine the camera pose from which an image was taken. Finding correct 2D-3D correspondences between query image features and 3D points in the scene model becomes harder as the size of the model increases. Current state-of-the-art methods therefore combine elaborate matching schemes with camera pose estimation techniques that are able to handle large fractions of wrong matches. In this work we study the benefits and limitations of spatial verification compared to appearance based filtering. We propose a voting based pose estimation strategy that exhibits O(n) complexity in the number of matches and thus facilitates to consider much more matches than previous approaches – whose complexity grows at least quadratically. This new outlier rejection formulation enables us to evaluate pose estimation for 1-to-many matches and to surpass the state-of-the-art. At the same time, we show that using more matches does not automatically lead to a better performance.</Data></Cell>
    <Cell><Data ss:Type="String">Bernhard Zeisl*, ETH Zurich; Torsten Sattler, ETH Zurich; Marc Pollefeys, ETH</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">509</Data></Cell>
    <Cell><Data ss:Type="String">Mutual-Structure for Joint Filtering</Data></Cell>
    <Cell><Data ss:Type="String">We estimate mutual-structure for joint filtering, which is different from previous cross/joint/guided filters that directly transfer the reference image structural information to the target image. We consider possibly different structures that are contained in the reference image, which in several cases should not be simply passed to the target image. To address this problem, which is common in depth completion, optical flow estimation, image enhancement and stereo matching, we introduce the concept of mutual-structure. Extracting it with respect to both reference and target images for joint filtering can suppress a level of errors that are mistakenly introduced by joint filtering. We propose a novel objective function which can be very efficiently optimized to achieve this goal along with its analysis. Our method results in necessary and faithful edge preserving. Experimental results are presented in the paper and supplementary material.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaoyong Shen*, CUHK; Chao Zhou, ; Li Xu, Chinese University of Hong Kong; Jiaya Jia, Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">744</Data></Cell>
    <Cell><Data ss:Type="String">Flow Fields: Dense Unregularized Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation</Data></Cell>
    <Cell><Data ss:Type="String">Modern large displacement optical flow algorithms often use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While nearest neighbor fields have the advantage of being dense, they have the major disadvantage that they are not designed for optical flow estimation, but for finding the visually most similar correspondence. In this paper, we will demonstrate that it is possible to create unregularized dense correspondence fields that are much better suited for optical flow estimation than nearest neighbor fields. Our approach builds on novel purely data based search strategies (like non-local upsampling) to robustly find the optical flow. Moreover, we present advanced outlier filtering. We demonstrate that our Flow Fields are better suited for large displacement optical flow estimation than state-of-the-art descriptor matching techniques. We do so by initializing EpicFlow (so far the best method on MPI-Sintel) with our Flow Fields instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI and Middlebury.</Data></Cell>
    <Cell><Data ss:Type="String">Christian Bailer*, DFKI; Bertram Taetz, DFKI; Didier Stricker, DFKI</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">745</Data></Cell>
    <Cell><Data ss:Type="String">Structured Indoor Modeling</Data></Cell>
    <Cell><Data ss:Type="String">This paper presents a novel 3D modeling framework that reconstructs an  indoor scene as a structured model from panorama RGBD images. A scene  geometry is represented as a graph, where nodes correspond to structural  elements such as rooms, walls, and objects. The approach devises a  structure grammar that defines how a scene graph can be manipulated. The  grammar then drives a principled new reconstruction algorithm, where the  grammar rules are sequentially applied to recover a structured model.    The paper also proposes a new room segmentation algorithm and a depthmap  reconstruction algorithm that are used in the framework and can enforce  architectural shape priors far beyond existing state-of-the-art.  The  structured scene representation enables a variety of novel applications,  ranging from indoor scene visualization, automated floorplan generation,  Inverse-CAD, and more.  We have tested our framework and algorithms on  six synthetic and five real datasets with qualitative and quantitative  evaluations.</Data></Cell>
    <Cell><Data ss:Type="String">Yasutaka Furukawa, &quot;Washington University, St. Louis, USA&quot;; Hang Yang, Washington University in St. Louis; Satoshi Ikehata*, UW in St. Louis</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">749</Data></Cell>
    <Cell><Data ss:Type="String">Web-scale image clustering revisited</Data></Cell>
    <Cell><Data ss:Type="String">Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in fast approximate $k$-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized $k$-means (IQ-means). Its sampling-based initialization is a form of hashing and analogous to seed detection, while its update iterations are analogous to seed growing, but are now principled in the sense of distortion minimization. Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. We further design a dynamic variant that is able to determine the number of clusters $k$ in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single core in less than one hour.  </Data></Cell>
    <Cell><Data ss:Type="String">Yannis Avrithis*, UoA; Yannis Kalantidis, Yahoo Labs; Evangelos Anagnostopoulos, University of Athens; Ioannis Emiris, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">831</Data></Cell>
    <Cell><Data ss:Type="String">Piecewise Flat Embedding for Image Segmentation</Data></Cell>
    <Cell><Data ss:Type="String">Image segmentation is a critical step in many computer vision tasks, including high-level visual recognition and scene understanding tasks as well as low-level photo and video processing operations. In this paper, we propose a new nonlinear embedding, called piecewise flat embedding, for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding attempts to identify segment boundaries while significantly suppressing variations within segments. We adopt an $L_1$-regularized energy term in the formulation to promote sparse solutions. We further devise an effective two-stage numerical algorithm based on Bregman iterations to solve the proposed embedding. Piecewise flat embedding can be easily integrated into existing image segmentation frameworks, including segmentation based on spectral clustering and hierarchical segmentation based on contour detection. Experiments on BSDS500 indicate that segmentation algorithms incorporating this embedding can achieve significantly improved results in both frameworks.</Data></Cell>
    <Cell><Data ss:Type="String">Chaowei Fang, The University of Hong Kong; Zicheng Liao, ; Yizhou Yu*, HKU, UIUC, ZJU</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">833</Data></Cell>
    <Cell><Data ss:Type="String">Where to Buy It: Matching Street Clothing Photos in Online Shops</Data></Cell>
    <Cell><Data ss:Type="String">In this paper, we define a new task, Exact Street to Shop, where our goal is to match a query real-world example of a garment item to the same garment in an online shop. Challenges of this application include extreme visual differences between street photos (pictures of people wearing clothing, captured in everyday, uncontrolled settings) and online shop photos (pictures of clothing items on people, mannequins, or in isolation, captured by professionals in more controlled settings). We collect a new dataset for this application containing  404,683 shop photos collected from 25 different online retailers and 20,357 street photos,  providing a total of  39,479 clothing item matches between street and shop photos. We develop three different methods for exact street to shop retrieval, including two deep learning baseline methods, and a method to learn a similarity measure between the street and shop domains. Experiments demonstrate that our learned similarity significantly outperforms our baselines that use existing deep learning based representations.</Data></Cell>
    <Cell><Data ss:Type="String">Mohammadhadi Kiapour*, University of North Carolina; Xufeng Han, University of North Carolina - Chapel Hill; Svetlana Lazebnik, UIUC; Alex Berg, &quot;University of North Carolina, USA&quot;; Tamara Berg, University on North carolina</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">855</Data></Cell>
    <Cell><Data ss:Type="String">Issuing  Notifications for Missing Actions: Don't Forget to Turn the Lights Off!</Data></Cell>
    <Cell><Data ss:Type="String">We all have experienced forgetting habitual actions among our daily activities. For example, we probably have forgotten to turn the lights off before leaving a room or turn the stove off after cooking. In this paper, we propose a solution to the problem of issuing notifications on actions that may be missed. This involves learning about interdependencies between actions and being able to predict an ongoing action while segmenting the input video stream. In order to show a proof of concept, we collected a new egocentric dataset, in which people wear a camera while making lattes. We show promising results on the extremely challenging task of issuing correct and timely reminders. We also show that our model reliably segments the actions, while predicting the ongoing one when only a few frames from the beginning of the action are observed. The overall prediction accuracy is 45.9% when only 10 frames of an action are seen (2/3 of a sec). Moreover, the overall recognition and segmentation accuracy is shown to be 72.3% when the whole activity sequence is observed. Finally, the online prediction and segmentation accuracy is 67.9% when the prediction is made at every time step.</Data></Cell>
    <Cell><Data ss:Type="String">Bilge Soran*, University of Washington; Ali Farhadi, University of Washington; Linda Shapiro, University of Washington</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">901</Data></Cell>
    <Cell><Data ss:Type="String">Semantic Image Labeling via Deep Parsing Network</Data></Cell>
    <Cell><Data ss:Type="String">This paper addresses pixelwise image labeling by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, yielding a new state-of-the-art accuracy of 73.6% without outside training data.</Data></Cell>
    <Cell><Data ss:Type="String">Ziwei Liu*, The Chinese University of HK; Xiaoxiao Li, The Chinese University of HK; Ping Luo, The Chinese University of Hong Kong; Chen-Change Loy, the Chinese University of Hong Kong; Xiaoou Tang, The Chinese University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">922</Data></Cell>
    <Cell><Data ss:Type="String">On the Visibility of Point Clouds</Data></Cell>
    <Cell><Data ss:Type="String">Is it possible to determine the visible subset of points directly from a given point cloud? Interestingly, in [Katz et al '07] it was shown that this is indeed the case&#45;-despite the fact that points cannot occlude each other, this task can be performed without surface reconstruction or normal estimation. The operator is very simple&#45;-it first transforms the points to a new domain and then constructs the convex hull in that domain. Points that lie on the convex hull of the transformed set of points are the images of the visible points. This operator found numerous applications in computer vision, including face reconstruction, keypoint detection, finding the best viewpoints, reduction of points, and many more.  The current paper addresses a fundamental question: What properties should a transformation function satisfy, in order to be utilized in this operator? We show that three such properties are necessary&#45;-the sign of the function, monotonicity, and a condition regarding the function's parameter. The correctness of an algorithm that satisfies these three properties is proved. Finally, we show an interesting application of the operator, assignment of visibility-confidence score. This feature is missing from previous approaches, where a binary yes/no visibility is determined. This score can be utilized in various applications; we illustrate its use in view-dependent curvature estimation.</Data></Cell>
    <Cell><Data ss:Type="String">Sagi Katz, Technion; Ayellet Tal*, Technion</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">956</Data></Cell>
    <Cell><Data ss:Type="String">Global, Dense Multiscale Reconstruction for a Billion Points</Data></Cell>
    <Cell><Data ss:Type="String">We present a variational approach for surface reconstruction from a set of oriented points with scale information.  We focus particularly on scenarios with non-uniform point densities due to images taken from different distances.  In contrast to previous methods, we integrate the scale information in the objective and globally optimize the signed distance function of the surface on a balanced octree grid.  We use a finite element discretization on the dual structure of the octree minimizing the number of variables.  The tetrahedral mesh is generated efficiently from the dual structure, and also memory efficiency is optimized, such that robust data terms can be used even on very large scenes.  The surface normals are explicitly optimized and used for surface extraction to improve the reconstruction at edges and corners.</Data></Cell>
    <Cell><Data ss:Type="String">Benjamin Ummenhofer*, University of Freiburg; Thomas Brox, &quot;University of Freiburg, Germany&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">984</Data></Cell>
    <Cell><Data ss:Type="String">Registering Images to Untextured Geometry using Average Shading Gradients</Data></Cell>
    <Cell><Data ss:Type="String">Many existing approaches for image-to-geometry registration assume that either a textured 3D model or a good initial guess of the 3D pose is available to bootstrap the registration process. In this paper we consider the registration of photographs to 3D models even when no texture information is available. This is very challenging as we cannot rely on texture gradients, and even shading gradients are hard to estimate since the lighting conditions are unknown. To that end, we propose average shading gradients, a rendering technique that estimates the average gradient magnitude over all lighting directions under Lambertian shading. We use this gradient representation as the building block of a registration pipeline based on matching sparse features. To cope with inevitable false matches due to the missing texture information and to increase robustness, the pose of the 3D model is estimated in two stages. Coarse pose hypotheses are first obtained from a single correct match each, subsequently refined using SIFT flow, and finally verified. We apply our algorithm to registering images of real-world objects to untextured 3D models of limited accuracy.</Data></Cell>
    <Cell><Data ss:Type="String">Tobias Plötz*, TU Darmstadt; Stefan Roth, TU Darmstadt</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">999</Data></Cell>
    <Cell><Data ss:Type="String">Training a Feedback Loop for Hand Pose Estimation</Data></Cell>
    <Cell><Data ss:Type="String">We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU.  </Data></Cell>
    <Cell><Data ss:Type="String">Markus Oberweger*, Graz University of Technology; Paul Wohlhart, Graz University of Technology; Vincent Lepetit, TU Graz</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1021</Data></Cell>
    <Cell><Data ss:Type="String">Learning Discriminative Reconstructions for Unsupervised Outlier Removal</Data></Cell>
    <Cell><Data ss:Type="String">We study the problem of automatically removing outliers from noisy data, with application for removing outlier images from an image collection. We address this problem by utilizing the reconstruction errors of an autoencoder. We observe that when data are reconstructed from low-dimensional representations, the inliers and the outliers can be well separated according to their reconstruction errors. Based on this basic observation, we gradually inject discriminative information in the learning process of an autoencoder to make the inliers and the outliers more separable. Experiments on a variety of image datasets validate our approach.</Data></Cell>
    <Cell><Data ss:Type="String">Yan Xia*, USTC; Xudong Cao, ; Fang Wen, MSRA; Gang Hua, Stevens Institute of Technology; Jian Sun, Microsoft Research China</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1023</Data></Cell>
    <Cell><Data ss:Type="String">Removing rain from a single image via  discriminative sparse coding</Data></Cell>
    <Cell><Data ss:Type="String">Visual distortions on images caused by bad weather conditions can have a negative impact on the performance of many outdoor vision systems. One often seen bad weather is rain which causes  significant yet complex local  intensity fluctuations in images. The paper aims at developing an effective algorithm to remove visual effects of rain from a single rainy image, i.e. separate  the rain layer and the de-rained image layer from an rainy image.  Built upon a non-linear generative model of rainy image, namely screen blend mode, we proposed a dictionary learning based algorithm for single image de-raining. The basic idea is to sparsely approximate the patches of two layers by very high discriminative codes over a learned dictionary with strong mutual exclusivity property.  Such discriminative sparse codes lead to accurate separation of two layers from their non-linear composite. The experiments showed that the proposed method outperformed the existing single image de-raining methods on tested rain images.</Data></Cell>
    <Cell><Data ss:Type="String">Yu Luo, NUS; Yong Xu, South China University of Technology; Hui Ji*, National  University of  Singapore</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1064</Data></Cell>
    <Cell><Data ss:Type="String">Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose</Data></Cell>
    <Cell><Data ss:Type="String">We address the problem of hand pose estimation, formulated as an inverse problem.  Typical approaches optimize an energy function over pose parameters using a `black box' image generation procedure. This procedure knows little about either the relationships between the parameters or the form of the energy function.  In this paper, we show that we can significantly improving upon black box optimization by exploiting high-level knowledge of the structure of the parameters and using a local surrogate energy function. Our new framework, hierarchical sampling optimization, consists of a sequence of predictors organized into a kinematic hierarchy.  Each predictor is conditioned on its ancestors, and generates a set of samples over a subset of the pose parameters.  The highly-efficient surrogate energy is used to select among samples.  Having evaluated the full hierarchy, the partial pose samples are concatenated to generate a full-pose hypothesis.  Several hypotheses are generated using the same procedure, and finally the original full energy function selects the best result.  Experimental evaluation on three publically available datasets show that our method is particularly impressive in low-compute scenarios where it significantly outperforms all other state-of-the-art methods.</Data></Cell>
    <Cell><Data ss:Type="String">Danhang Tang*, Imperial College London; Jonathan Taylor, Microsoft Research; Pushmeet Kohli, Microsoft Research, UK; Cem Keskin, Microsoft; Tae-Kyun Kim, Imperial College London; Jamie Shotton, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1118</Data></Cell>
    <Cell><Data ss:Type="String">CV-HAZOP: Introducing Test Data Validation for Computer Vision</Data></Cell>
    <Cell><Data ss:Type="String">Test data plays an important role in computer vision (CV) but is plagued by two questions: Which situations should be covered by the test data and have we tested enough to reach a conclusion?  In this paper we propose a new solution answering these questions using a standard procedure devised by the safety community to validate complex systems:  The Hazard and Operability Analysis (HAZOP). It is designed to systematically search and identify difficult, performance-decreasing situations and aspects.  We introduce a generic CV model, that creates the basis for the hazard analysis, and for the first time apply an extensive HAZOP to the CV domain.  The result is a publicly available checklist with more than 900 identified individual hazards. This checklist can be used to evaluate existing test data sets by quantifying the amount of covered hazards.  We evaluate our approach by first analyzing and annotating the popular stereo vision test data sets Middlebury and KITTI using our checklist.  Second, we compare the performance of six popular stereo matching algorithms at the identified hazards from our checklist with their average performance and show, as expected, a clear negative influence of the hazards.  The presented approach is a useful tool to evaluate and improve test data sets and further creates a common basis for future data set designs.</Data></Cell>
    <Cell><Data ss:Type="String">Oliver Zendel*, AIT; Markus Murschitz, AIT; Martin Humenberger, ; Wolfgang Herzner, AIT</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1130</Data></Cell>
    <Cell><Data ss:Type="String">Minimum Barrier Salient Object Detection at 80 FPS</Data></Cell>
    <Cell><Data ss:Type="String"> We propose a highly efficient, yet powerful, salient object detection method based on the Minimum Barrier Distance (MBD) Transform. The MBD transform is robust to pixel-value fluctuation, and thus can be effectively applied on raw pixels without region abstraction. We present an approximate MBD transform algorithm with 100X speedup over the exact algorithm. An error bound analysis is also provided. Powered by this fast MBD transform algorithm, the proposed salient object detection method runs at 80 FPS, and significantly outperforms previous methods with similar speed on four large benchmark datasets, and achieves comparable or better performance than state-of-the-art methods. Furthermore, a technique based on color whitening is proposed to extend our method to leverage the appearance-based backgroundness cue. This extended version further improves the performance, while still being one order of magnitude faster than all the other leading methods.</Data></Cell>
    <Cell><Data ss:Type="String">Jianming Zhang*, Boston University; Stan Sclaroff, Boston University; Zhe Lin, &quot;Adobe Systems, Inc.&quot;; Xiaohui Shen, Adobe Research; Brian Price, ; Radomir Mech, Adobe</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1186</Data></Cell>
    <Cell><Data ss:Type="String">Multi-Image Matching via Fast Alternating Minimization</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we propose a global optimization-based approach to jointly matching a set of images. The estimated correspondences simultaneously maximize pairwise feature affinities and cycle consistency across multiple images. Unlike previous convex methods relying on semidefinite programming, we formulate the problem as a low-rank matrix recovery problem and show that the desired semidefiniteness of a solution can be spontaneously fulfilled. The low-rank formulation enables us to derive a fast alternating minimization algorithm in order to handle practical problems with thousands of features. Both simulation and real experiments demonstrate that the proposed algorithm can achieve a competitive performance with an order of magnitude speedup compared to the state-of-the-art algorithm. In the end, we demonstrate the applicability of the proposed method to match the images of different object instances and as a result the potential to reconstruct category-specific object models from those images.</Data></Cell>
    <Cell><Data ss:Type="String">Xiaowei Zhou*, University of Pennsylvania; Menglong Zhu, University of Pennsylvania; Kostas Danilidiis, University of Pennsylvania</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1291</Data></Cell>
    <Cell><Data ss:Type="String">Low-rank Matrix Factorization under General Mixture Noise Distributions</Data></Cell>
    <Cell><Data ss:Type="String">Many computer vision problems can be posed as learning a low-dimensional subspace from high dimensional data. The low rank matrix factorization (LRMF) represents a commonly utilized subspace learning strategy. Most of the current LRMF techniques are constructed on the optimization problem using $L_1$ norm and $L_2$ norm, which mainly deal with Laplacian and Gaussian noise, respectively. To make LRMF capable of adapting more complex noise, this paper proposes a new LRMF model by assuming noise as a Mixture of Exponential Power (MoEP) distribution and proposes a penalized MoEP model by combining the penalized likelihood method with the MoEP distribution. Such setting facilitates the learned LRMF model capable of automatically fitting the real noise through a mixture of EP distributions. Each component in this mixture is adapted from a series of preliminary  super- or sub-Gaussian candidates. An Expectation Maximization algorithm is also designed to infer the parameters involved in the proposed PMoEP model. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and hyperspectral image restoration.</Data></Cell>
    <Cell><Data ss:Type="String">Xiangyong Cao*, Xi'an Jiaotong University; Yang Chen, Xi'an Jiaotong University; Qian Zhao, Xi'an Jiaotong University; Deyu Meng, Xi'an Jiaotong University; Yao Wang, ; Dong Wang, ; Zongben Xu, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1600</Data></Cell>
    <Cell><Data ss:Type="String">Partial Person Re-identification</Data></Cell>
    <Cell><Data ss:Type="String">We address a new partial person re-identification (re-id) problem, where only a partial observation of a person is available for matching across different non-overlapping camera views. This differs significantly from the conventional person re-id setting where it is assumed that the full body of a person is detected and aligned. To solve this more challenging and realistic re-id problem without the implicit assumption of manual body-parts alignment, we propose a matching framework consisting of 1) a local patch-level matching model based on a novel sparse representation classification formulation with explicit patch ambiguity modelling, and 2) a global part-based matching model providing complementary spatial layout information. Our framework is evaluated on a new partial person re-id dataset as well as two existing datasets modified to include partial person images. The results show that the proposed method outperforms significantly existing re-id methods as well as other partial visual matching methods.</Data></Cell>
    <Cell><Data ss:Type="String">Wei-Shi Zheng*, Sun Yat-Sen University; Xiang Li, SYSU; Tao Xiang, Queen Mary University of London; Shengcai Liao, &quot;Institute of Automation, Chinese Academy of Sciences&quot;; Jianhuang Lai, Sun Yat-sen University; Shaogang Gong, QMUL</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1639</Data></Cell>
    <Cell><Data ss:Type="String">Discovering the Spatial Extent of Relative Attributes</Data></Cell>
    <Cell><Data ss:Type="String">We present a weakly-supervised approach that discovers the spatial extent of relative attributes, given only pairs of ordered images. In contrast to traditional approaches that use global appearance features or rely on keypoint detectors, our goal is to automatically discover the image regions that are relevant to the attribute, even when the attribute's appearance changes drastically across its attribute spectrum. To accomplish this, we first develop a novel formulation that combines a detector with local smoothness to discover a set of coherent visual chains across the image collection. We then introduce an efficient way to generate additional chains anchored on the initial discovered ones. Finally, we automatically identify the most relevant visual chains, and create an ensemble image representation to model the attribute. Through extensive experiments, we demonstrate our method's promise relative to several baselines in modeling relative attributes.</Data></Cell>
    <Cell><Data ss:Type="String">Fanyi Xiao*, Univ. of California, Davis; Yong Jae Lee, UC Davis</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1641</Data></Cell>
    <Cell><Data ss:Type="String">Bilinear CNN Models for Fine-grained Visual Recognition</Data></Cell>
    <Cell><Data ss:Type="String">Fine-grained recognition is challenging because the appearance variation within a category due to factors such as pose and viewpoint can be significantly higher than that across categories. We propose an architecture for fine-grained recognition that uses two separate feature extractors based on convolutional neural networks to model the appearance due to ``where&quot; the parts are and ``what&quot; the parts look like. The outputs of the two networks are combined bilinearly which allows simultaneous training of two networks using only category labels. Dataset specific fine-tuning improves performance significantly. For \eg, using networks initialized from the ImageNet dataset~\cite{deng09imagenet} followed by fine-tuning we obtain 80.9\% accuracy on the CUB-200-2011 dataset, requiring only category labels at training time, outperforming the current state-of-the-art of 75.7\%~\cite{branson14bird} by a large margin. Moreover, our model is fairly efficient running at 8 frames/sec on a modern GPU. We show that bilinear models generalize various orderless pooling methods such as VLAD, Fisher vector and second-order pooling, shedding some light on their remarkable success in fine-grained recognition tasks. We also present reduced dimensionality models that have fewer parameters, are faster to train and evaluate, and lead to better accuracies in some cases. Experiments on a number of datasets and visualization of the learned models are presented.</Data></Cell>
    <Cell><Data ss:Type="String">Tsung-Yu Lin*, UMass Amherst; Aruni RoyChowdhury, ; Subhransu Maji, University of Massachusetts Amherst</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1658</Data></Cell>
    <Cell><Data ss:Type="String">Multiple Hypothesis Tracking Revisited: Blending in Modern Appearance Model</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes to revisit the classical multiple hypotheses tracking (MHT) algorithm in a tracking-by-detection framework. Surprisingly state-of-the-art performance has been obtained using classical MHT implementations without engineering tricks, showing a promising direction. In order to further utilize the strength of MHT in exploiting higher-order information in the track, we propose to train online appearance models for each track hypothesis using modern appearance features such as the ones learned from deep convolutional neural networks. It is shown that such appearance models can be learned efficiently via a regularized least squares framework, requiring few extra operations for multiple hypothesis branches. State-of-the-art results are shown for the proposed approach on popular tracking-by-detection datasets such as PETS, TUD and ETH.</Data></Cell>
    <Cell><Data ss:Type="String">Chanho Kim*, Georgia Tech; Fuxin Li, Georgia Tech; Arridhana Ciptadi, Georgia Tech; James Rehg, Georgia Institute of Technology</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1679</Data></Cell>
    <Cell><Data ss:Type="String">Ask Your Neurons: A Neural-based Approach to Answering Questions about Images</Data></Cell>
    <Cell><Data ss:Type="String">We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation of this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (question). Our result doubles the performance of the previous best result on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. Further annotations were collected to study human consensus, which is related to the ambiguities inherent in this challenging task.</Data></Cell>
    <Cell><Data ss:Type="String">Mateusz Malinowski*, Max Planck for Informatics; Marcus Rohrbach, UC Berkeley; Mario Fritz, MPI Informatics, Saarbrucken, Germany</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1687</Data></Cell>
    <Cell><Data ss:Type="String">Semantic Component Analysis</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised and weakly-supervised visual learning in large image collections are critical in order to avoid the time-consuming and error-prone process of manual labeling.  Standard approaches rely on methods like multiple-instance learning or graphical models, which are typically computationally intensive and sensitive to initialization. On the other hand, simpler component analysis or clustering approaches usually cannot achieve meaningful invariances or semantic interpretability. To address the issues of previous work, we present a simple but effective method called Semantic Component Analysis (SCA), which provides a decomposition of images into semantic components.     Unsupervised SCA decomposes additive image representations into spatially-meaningful visual components that naturally correspond to object categories. SCA uses an over-complete representation that allows for rich instance-level constraints and spatial priors that lead to improved results and more interpretable components in comparison to traditional matrix factorization. If weakly-supervised information is available, SCA factorizes a set of images into semantic groups of superpixels based on image-level tags.  We also provide connections to traditional methods for component analysis (e.g. Grassmann averages, PCA, and NMF). The effectiveness of our approach is validated through synthetic data and on the MSRC2 and Sift Flow datasets, demonstrating competitive results in unsupervised and weakly-supervised semantic segmentation.</Data></Cell>
    <Cell><Data ss:Type="String">Calvin Murdock*, Carnegie Mellon University; Fernando De la Torre, &quot;Carnegie Mellon University, USA&quot;</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1698</Data></Cell>
    <Cell><Data ss:Type="String">Fast R-CNN</Data></Cell>
    <Cell><Data ss:Type="String">This paper proposes Fast R-CNN, a clean and fast framework for object  detection.  Compared to traditional R-CNN, and its accelerated version  SPPnet, Fast R-CNN trains networks using a multi-task loss in a single  fine-tuning run.  The multi-task loss simplifies and speeds up training.  Unlike SPPnet, all network layers can be learned during fine-tuning.  We show that this difference has practical ramifications for very deep  networks, such as VGG16, where mAP suffers when only the fully-connected  layers are fine-tuned.  Compared to ``slow'' R-CNN, Fast R-CNN is 9x  faster at training VGG16 for detection, 213x faster for detection, and  achieves a significantly higher mAP on PASCAL VOC 2012.  Fast R-CNN  is implemented in Python and C++ and will be released under an open  source license.</Data></Cell>
    <Cell><Data ss:Type="String">Ross Girshick*, Microsoft Research</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1700</Data></Cell>
    <Cell><Data ss:Type="String">Deep Fried Convnets</Data></Cell>
    <Cell><Data ss:Type="String">The fully connected layers of a deep convolutional neural network typically contain over 90% of the network parameters, and consume the majority of the memory required to store the network. Reducing the number of parameters while preserving predictive performance is critically important for deploying deep neural networks in memory constrained environments such as GPUs or embedded devices.  In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace the fully connected layers in a deep convolutional neural network. This deep fried network is end-to-end trainable in conjunction with convolutional layers. Our new architecture substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance</Data></Cell>
    <Cell><Data ss:Type="String">Zichao Yang*, CMU; Marcin Moczulski, University of Oxford; Misha Denil, ; Nando Freitas, University of Oxford; Alex Smola, CMU; Le Song, ; Ziyu Wang, </Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1754</Data></Cell>
    <Cell><Data ss:Type="String">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</Data></Cell>
    <Cell><Data ss:Type="String">Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.</Data></Cell>
    <Cell><Data ss:Type="String">Yukun Zhu*, University of Toronto; Ryan Kiros, University of Toronto; Rich Zemel, University of Toronto; Ruslan salakhutdinov, ; Raquel Urtasun, University of Toronto; Antonio Torralba, MIT; Sanja Fidler, University of Toronto</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1777</Data></Cell>
    <Cell><Data ss:Type="String">Unsupervised Visual Representation Learning by Context Prediction</Data></Cell>
    <Cell><Data ss:Type="String">We propose a simple way to create a supervisory signal to train a visual representation given only a large, unlabeled image collection. We extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We demonstrate that our model learns to represent object parts, since the presence of an object part provides information about its position relative to other parts of the same object. This simple cue, combined with a large training set, allows us to discover objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset, without any annotations. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.</Data></Cell>
    <Cell><Data ss:Type="String">Carl Doersch*, Carnegie Mellon University; Abhinav Gupta, &quot;Carnegie Mellon University, USA&quot;; Alexei Efros, UC Berkeley</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1779</Data></Cell>
    <Cell><Data ss:Type="String">Lost Shopping! Monocular Localization in Large Indoor Spaces</Data></Cell>
    <Cell><Data ss:Type="String">In this paper we propose a novel approach to  localization  in very large indoor spaces (i.e., 200+ store shopping malls)  that takes   a single image and a floor plan of the environment as input.   We formulate  the localization problem as inference in a Markov random field, which jointly reasons about text detection (localizing shop's names in the image with precise bounding boxes), shop facade segmentation, as well as   camera's  rotation and translation within the entire shopping mall. The power of our approach is that it does not use any prior  information about appearance  and instead  exploits text detections corresponding to the shop names. This makes our method applicable to a variety of domains and robust to store appearance variation across countries, seasons, and illumination conditions.  We demonstrate the performance of our approach in a new dataset we collected of two very large shopping malls, and show the power of holistic reasoning.   </Data></Cell>
    <Cell><Data ss:Type="String">Shenlong Wang*, University of Toronto; Sanja Fidler, University of Toronto; Raquel Urtasun, University of Toronto</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1863</Data></Cell>
    <Cell><Data ss:Type="String">Leave-One-Out Kernel Optimization for Shadow Detection</Data></Cell>
    <Cell><Data ss:Type="String">The objective of this work is to detect shadows in images. We pose this as the problem of labeling image regions, where each region corresponds to a group of superpixels. To predict the label of each region, we train a kernel Least-Squares SVM for separating shadow and non-shadow regions. The parameters of the kernel and the classifier are jointly learned to minimize the leave-one-out cross validation error. Optimizing the leave-one-out cross validation error is typically difficult, but it can be done efficiently in our framework. Experiments on two challenging shadow datasets, UCF and UIUC, show that our region classifier outperforms more complicated methods. We further enhance the performance of the region classifier by embedding it in an MRF framework and adding pairwise contextual cues. This leads to a method that outperforms the state-of-the-art by a wide margin.</Data></Cell>
    <Cell><Data ss:Type="String">Tomas F Yago Vicente*, Stony Brook University; Minh Hoai, Stony Brook University; Dimitris Samaras, SUNY Stonybrook</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1899</Data></Cell>
    <Cell><Data ss:Type="String">Deep Convolutional Neural Decision Forests</Data></Cell>
    <Cell><Data ss:Type="String">We present Deep Convolutional Neural Decision Forests &#45;- a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in a joint manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in lower level layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find on-par or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain a Top5-Error of only 7.84% on validation data when integrating our forest in a single-model GoogLeNet architecture, without any form of training data set augmentation.  </Data></Cell>
    <Cell><Data ss:Type="String">Madalina Fiterau*, Carnegie Mellon University; Peter Kontschieder, ; Antonio Criminisi, Microsoft Research; Samuel Rota Bulò, FBK-irst</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1945</Data></Cell>
    <Cell><Data ss:Type="String">Uncovering Interactions and Interactors: Joint Estimation of Head, Body Orientation and F-formations from Surveillance Videos</Data></Cell>
    <Cell><Data ss:Type="String">We present a novel approach for jointly estimating tar-  gets’ head, body orientations and conversational groups  called F-formations from a distant social scene (e.g., a  cocktail party captured by surveillance cameras). Differing  from related works that have (i) coupled head and body pose  learning by exploiting the limited range of orientations that  the two can jointly take, or (ii) determined F-formations  based on the mutual head (but not body) orientations of in-  teractors, we present a unified framework to jointly infer  both (i) and (ii). Apart from exploiting spatial and orien-  tation relationships, we also integrate cues pertaining to  temporal consistency and occlusions, which are beneficial  while handling low-resolution data under surveillance set-  tings. Efficacy of the joint inference framework reflects via  increased head, body pose and F-formation estimation ac-  curacy over the state-of-the-art, as confirmed by extensive  experiments on two social datasets.  </Data></Cell>
    <Cell><Data ss:Type="String">Elisa Ricci*, U. Perugia; Jagannadan Varadarajan, ; Ramanathan Subramanian, ADSC; Samuel Rota Bulò, FBK-irst; Narendra Ahuja, &quot;University of Illinois at Urbana-Champaign, USA&quot;; Oswald Lanz, FBK</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">1955</Data></Cell>
    <Cell><Data ss:Type="String">Learning Query and Image Similarities with Ranking Canonical Correlation Analysis</Data></Cell>
    <Cell><Data ss:Type="String">One of the fundamental problems in image search is to learn the ranking functions, i.e., similarity between the query and image. The research on this topic has evolved through two paradigms: feature-based vector model and image ranker learning. The former relies on the image surrounding texts, while the latter learns a ranker based on human labeled query-image pairs. Each of the paradigms has its own limitation. The vector model is sensitive to the quality of text descriptions, and the learning paradigm is difficult to be scaled up as human labeling is always too expensive to obtain.    We demonstrate in this paper that the above two limitations can be well mitigated by jointly exploring subspace learning and the use of click-through data. The former aims to create a low-dimensional latent subspace with the ability in directly comparing information from two original incomparable views (i.e., textual and visual), while the latter enjoys the largely available click-through data from image search engine for bridging both semantic and intent gaps. Specifically, we propose a novel subspace learning method for learning query and image similarities, named Ranking Canonical Correlation Analysis (RCCA). The proposed RCCA initially finds linear projections of two views to a common subspace by maximizing their correlations, and further simultaneously learns a bilinear query-image similarity function and adjusts the subspace to preserve the preference relations implicit in the click-through data. Once the subspace is finalized, query-image similarity can be computed by the bilinear similarity function on their mappings in this subspace. On a large-scale click-based image dataset with 11.7 million queries and one million images, we demonstrate that RCCA is a powerful technique for image search with superior performance over several state-of-the-art learning methods for both keyword-based and query-by-example tasks.</Data></Cell>
    <Cell><Data ss:Type="String">Ting Yao*, Microsoft Research; Tao Mei, Microsoft Research Asia; Chong-Wah Ngo, City University of Hong Kong</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2012</Data></Cell>
    <Cell><Data ss:Type="String">Robust and Optimal SoS-based Point-to-Plane Registration of Image Sets and Structured Scenes</Data></Cell>
    <Cell><Data ss:Type="String"> This paper deals with the problem of registering a known structured 3D scene and its metric Structure-from-Motion (SfM) counterpart. The proposed work relies on a prior plane segmentation of the 3D scene and aligns the data obtained from both modalities by solving the point-to-plane assignment problem. An inliers-maximization approach within a Branch-and-Bound (BnB) search scheme is adopted. For the first time in this paper, a Sum-of-Squares optimization theory framework is employed for identifying point-to-plane mismatches (i.e. outliers) with certainty. This allows to iteratively building potential inliers sets and converging to the solution satisfied by the largest number of point-to-plane assignments. Furthermore, our approach is boosted by new plane visibility conditions which are also introduced in this paper. Using this framework, we solve the registration problem in two cases: (i) a set of putative point-to-plane correspondences (with possibly overwhelmingly many outliers) is given as input and (ii) no initial correspondences are given. In both cases, our approach yields outstanding results in terms of robustness and optimality.</Data></Cell>
    <Cell><Data ss:Type="String">Danda Pani Paudel*, Le2i; Adlane Habed, ICube, University of Strasbourg; Cédric Demonceaux, Le2i, University of Bourgogne; Pascal Vasseur, LITIS EA, University of Rouen</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2300</Data></Cell>
    <Cell><Data ss:Type="String">Airborne Three-Dimensional Cloud Tomography</Data></Cell>
    <Cell><Data ss:Type="String">There are significant uncertainties in atmospheric contents and their role in Earth's radiation balance, particularly aerosols and clouds. Clouds in their different evolution states lead to local effects (precipitation, shadows on solar facilities). We thus seek to sense these large three dimensional (3D) volumetric distributions of scatterers (droplets). Our approach is computational tomography using passive multi-angular imagery. For light-matter interaction that accounts for multiple-scattering, we use the 3D radiative transfer equation as a forward model. Inverting this model (recovery) is a computational bottleneck on large scales (many unknowns). Steps taken make this tomography tractable, without approximating the scattering order or angle range.</Data></Cell>
    <Cell><Data ss:Type="String">Aviad Levis*, Technion; Amit Aides, Technion; Yoav Schechner, &quot;Technion Haifa, Israel&quot;; Anthony Davis, Jet Propulsion Laboratory</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2327</Data></Cell>
    <Cell><Data ss:Type="String">Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing</Data></Cell>
    <Cell><Data ss:Type="String">We introduce a segment-phrase table, a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.</Data></Cell>
    <Cell><Data ss:Type="String">Hamid Izadinia*, University of Washington; Fereshteh Sadeghi, University of Washington; Santosh Kumar Divvala, Allen Institute for Artificial Intelligence; Hannaneh Hajishirzi, University of Washington; Yejin Choi, University of Washington; Ali Farhadi, University of Washington</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2494</Data></Cell>
    <Cell><Data ss:Type="String">3D-Assisted Image Feature Synthesis for Novel Views of an Object</Data></Cell>
    <Cell><Data ss:Type="String">Comparing two images from different views has been a long-standing challenging problem in computer vision, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize its features for other views, leveraging an existing modestly-sized 3D model collection of related but not identical objects.To accomplish this, we study the relationship of image patches between different views of the same object, seeking what we call surrogate patches &#45;- patches in one view whose feature content predicts well the features of a patch in another view. Based upon these surrogate relationships, we can create feature sets for all views of the latent object on a per patch basis, providing us an augmented multi-view representation of the object. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the augmented features  in fine-grained image retrieval/recognition and instance retrieval tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than other traditional approaches in this respect.</Data></Cell>
    <Cell><Data ss:Type="String">Hao Su*, Stanford; Fan Wang, Stanford University; Li Yi, ; Leonidas Guibas, Stanford University</Data></Cell>
   </Row>
   <Row>
    <Cell><Data ss:Type="String">2636</Data></Cell>
    <Cell><Data ss:Type="String">MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation</Data></Cell>
    <Cell><Data ss:Type="String">We present a novel global stereo model designed for view interpolation. Unlike existing stereo models which only output a disparity map, our model is able to output a 3D triangular mesh, which can be directly used for view interpolation. To realize this feature, we partition the input stereo images into 2D triangles with shared vertices. Lifting the 2D triangulation to 3D naturally generates a corresponding mesh. A technical difficulty is to properly split vertices to multiple copies when they appear at depth discontinuous boundaries. To deal with this problem, we formulate our objective as a two-layer MRF, with the upper layer modeling the splitting properties of the vertices and the lower layer optimizing a region-based stereo matching. Experiments on the Middlebury and the Herodion datasets demonstrate that our model is able to synthesize visually coherent new view angles with high PSNR, as well as outputting high quality disparity maps which rank at the first place on the new challenging high resolution Middlebury 3.0 benchmark.</Data></Cell>
    <Cell><Data ss:Type="String">Chi Zhang*, Sun Yat-Sen University; Zhiwei Li, Microsoft Research; Hongyang CHAO, Sun Yet-sen University; Rui Cai, Microsoft Research</Data></Cell>
   </Row>
  </Table>
  <WorksheetOptions xmlns="urn:schemas-microsoft-com:office:excel">
   <PageLayoutZoom>0</PageLayoutZoom>
   <Selected/>
   <Panes>
    <Pane>
     <Number>3</Number>
     <ActiveRow>15</ActiveRow>
     <ActiveCol>3</ActiveCol>
    </Pane>
   </Panes>
   <ProtectObjects>False</ProtectObjects>
   <ProtectScenarios>False</ProtectScenarios>
  </WorksheetOptions>
 </Worksheet>
</Workbook>
