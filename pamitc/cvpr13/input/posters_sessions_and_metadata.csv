Poster Paper ID,Paper Title,Abstract,Author Names,Author Emails,Session Subject Area - editable,Primary Subject Area,Secondary Subject Area,Tertiary Subject Area,,Notes,Session name,Session time
252,Manhattan Scene Understanding Via XSlit Imaging,A Manhattan World (MW) is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper# we present a novel single-image MW reconstruction algorithm from the perspective of non-pinhole cameras. We show that by acquiring the MW using an XSlit camera# we can instantly resolve coplanarity ambiguities. Specifically# we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition# if the lines are coplanar# their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally# we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results on both synthetic and real data show that our new XSlit-camera based solution provides an effective and reliable solution for MW understanding.,Jinwei Ye*# University of Delaware; Yu Ji# University of Delaware; Jingyi Yu# ,jye@cis.udel.edu; yuji@cis.udel.edu; yu@eecis.udel.edu,05.01 3D modeling and reconstruction,01.03 Computational Photography and Video*,01.05 Early and Biologically-inspired Vision,01.12 Sensors,05.01 3D modeling and reconstruction, ,3D and Stereo,P 1A
1643,Discovering the Structure of a Planar Mirror System from  Multiple Observations of a Single Point,We investigate the problem of identifying the position of a viewer inside a room of mirrors with unknown geometry in conjunction with the room's shape parameters.                                            We theoretically analyze the problem and show that it is possible to recover the geometry of convex polyhedral rooms from measurements of the apparent depth of a single point that is multiply reflected in the room. Applications of this problem statement include areas such as calibration# acoustic echo cancelation and time-of-flight imaging.       We demonstrate the feasibility of our solution on extensive       simulations as well as in a real-world calibration scenario.    ,Ilya Reshetouski# Saarland University; Alkhazur  Manakov# Saarland University; Ayush Bhandari# MIT Media Lab; Ramesh Raskar# MIT Media Lab; Hans-Peter Seidel# MPI Informatik; Ivo  Ihrke*# Saarland University,iresheto@mmci.uni-saarland.de; amanakov@mmci.uni-saarland.de; Ayush.Bhandari@googlemail.com; raskar@media.mit.edu; hpseidel@mpi-inf.mpg.de; ihrke@mmci.uni-saarland.de,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,01.03 Computational Photography and Video,05.03 Calibration and pose estimation,10.07 Additional applications, ,3D and Stereo,P 1A
1847,Joint 3D Scene Reconstruction and Class Segmentation,Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being 'too noisy'. Unfortunately# these priors generally yield overly smooth reconstructions or segmentations in certain regions whereas fail in other areas to constrain the solution sufficiently. In this paper we argue that image segmentation and dense 3D reconstruction contribute valuable information to each others task. Therefore# this paper proposes a rigorous mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. Image segmentations provide geometric cues which surface orientations are more likely to appear at a certain location in space whereas a dense 3D reconstruction yields a suitable regularization for the segmentation problem by lifting the labeling from 2D images to 3D space. We show how image segmentation cues and cues from surface orientations in 3D can be learnt from training data and then be used as class-specific regularization priors. Experimental results on several real data sets highlight the advantages of our joint formulation.,Christian Haene# ETH Zurich; Christopher Zach*# Microsoft Research; Andrea Cohen# ETH Zurich; Roland Angst# Stanford University; marc pollefeys# ETHZ,chaene@inf.ethz.ch; chzach@microsoft.com; andrea.cohen@inf.ethz.ch; rangst@stanford.edu; marc.pollefeys@inf.ethz.ch,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,03.03 Image segmentation,05.05 Multi-view stereo,08.07 Regularization, ,3D and Stereo,P 1A
2128,Tensor-Based Human Body Modeling,In this paper# we present a novel approach to model 3D human body with variations on both human shape and pose# by exploring a tensor decomposition technique. 3D human body modeling is important for 3D reconstruction and animation of realistic human body# which can be widely used in Tele-presence and video game applications. It is challenging due to a wide range of shape variations over different people and poses. The existing SCAPE model is popular in computer vision for modeling 3D human body. However# it considers shape and pose deformation separately# which is not accurate since pose deformation is person-dependent. Our tensor-based model addresses this issue by jointly modeling shape and pose deformation. Experimental results demonstrate that our tensor-based model outperforms the SCAPE model quite significantly. We also apply our model to capture human body using Microsoft Kinect sensor with excellent result.,Yinpeng Chen*# Microsoft Research; Zicheng Liu# ; Zhengyou Zhang# Microsoft,yiche@microsoft.com; zliu@microsoft.com; zhang@microsoft.com,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,03.05 Shape Representation and Matching,04.04 Model-based reconstruction and tracking,05.11 Three-dimensional modeling and manipulation, ,3D and Stereo,P 1A
1996,City-Scale Change Detection in Cadastral 3D Models using Images,In this paper# we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city.  We designed our approach to account for all the challenges involved in a large scale application of change detection# such as# inaccuracies in the input geometry# errors in the geo-location data of the images# as well as# the limited amount of information due to sparse imagery.  We evaluated our approach on an area of 6 square kilometers inside a city# using 3420 images downloaded from Google StreetView. These images besides being publicly available# are also a good example of panoramic images captured with a driving vehicle# %on the scale of a city# and hence demonstrating all the possible challenges resulting from such an acquisition.  We also quantitatively compared the performance of our approach with respect to a ground truth# as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.,Aparna Taneja*# Eth Zurich; Luca Ballan# ; marc pollefeys# ETHZ,aparna.taneja@gmail.com; luca.ballan@inf.ethz.ch; marc.pollefeys@inf.ethz.ch,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,04.04 Model-based reconstruction and tracking,05.03 Calibration and pose estimation,05.11 Three-dimensional modeling and manipulation, ,3D and Stereo,P 1A
1126,Improving the Visual Comprehension of Point Sets,A point set is the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is# might ``hide'' the prominent features of the object the points are sampled from. Our goal is to reduce the number of points in a point set# for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set# so as to create bright regions (low density) and dark regions (high density)# producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition# we introduce a new dual problem# for determining visibility of a point from infinity# and show how a limitation of its solution can be leveraged in a similar way.,Sagi Katz# Technion; Ayellet Tal*# Technion,sagikatz@tx.technion.ac.il; ayellet@ee.technion.ac.il,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,05.02 Active rangefinding and depth sensors,09.01 Applications: humans,10.06 Vision for Graphics, ,3D and Stereo,P 1A
903,Mirror Surface Reconstruction from a Single Image,This paper tackles the problem of reconstructing the shape of a smooth mirror surface from a single image. In particular# we consider the case where the camera is observing the reflection of a static reference plane in the unknown mirror. We first study the reconstruction problem given dense correspondences between 3D points on the reference plane and image locations. In such conditions# our differential geometry analysis provides a theoretical proof that the shape of the mirror surface can be uniquely recovered if the pose of the reference plane is known. We then relax our assumptions by considering the case where only sparse correspondences are available. In this scenario# we formulate reconstruction as an optimization problem# which can be solved using a nonlinear least-squares method. We demonstrate the effectiveness of our method on both synthetic and real images. ,Miaomiao Liu*# NICTA; Richard Hartley# ; Mathieu Salzmann# NICTA,Miaomiao.Liu@Nicta.com.au; richard.hartley@anu.edu.au; mathieu.salzmann@nicta.com.au,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,05.07 Shape from shading and specularities,,, ,3D and Stereo,P 1A
955,Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-mounted Camera,This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images# we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically# not deterministically# and based on their estimates# it evaluates the probability of structural changes in the scene# where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection# behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically# it should be easier to detect their changes.  The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.,Ken Sakurada*# Tohoku univ.; Takayuki Okatani# ; Koichiro Deguchi# Tohoku univ.,sakurada@fractal.is.tohoku.ac.jp; okatani@fractal.is.tohoku.ac.jp; kodeg@fractal.is.tohoku.ac.jp,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,05.09 Stereo correspondence,10.01 Aerial and outdoor image analysis and modeling,, ,3D and Stereo,P 1A
1441,Template-less Quasi-Rigid Shape Modeling with Implicit Loop-Closure,This paper presents a method for quasi-rigid objects modeling from depth scans which are captured at different time instances. The quasi-rigid object# such as a human body# hardly keeps fully still when a depth camera moves around the object to capture depth scans from different views. Our method adopts a model-to-part way to gradually integrate sample nodes of depth scans into a deformation graph and then update it. Under the as-rigid-as-possible assumption# the model-to-part way is able to adjust the deformation graph non-rigidly so as to avoid obvious alignment error accumulation# which also exhibits implicit loop-closure. To help the deformation graph be flexible to adjustment against drift and topological error# two key algorithms are designed. For the registration# we use a two-stage registration to largely keep the rigid motion part. For the integration and update# we dynamically control the regularization effect in a topological-aware way.  We illustrate the effectiveness and robustness of our method on several depth sequences of quasi-rigid objects. We also show the result of human shape modeling as an application of our method.,Ming Zeng*# Zhejiang University; Jiaxiang Zheng# Zhejiang University; Xuan Cheng# Zhejiang University; Xinguo Liu# Zhejiang University,mingzeng85@gmail.com; zhengjiaxiang@zjucadcg.cn; chengxuan@zjucadcg.cn; xgliu@cad.zju.edu.cn,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,05.11 Three-dimensional modeling and manipulation,09.01 Applications: humans,, ,3D and Stereo,P 1A
438,Understanding Bayesian rooms using composite 3D object models,We develop a comprehensive Bayesian generative model for understanding indoor scenes. While it is common in this domain to approximate objects with 3D bounding boxes# we propose using strong representations with finer granularity. For example# we model a chair as a set of four legs# a seat and a backrest. We find that modeling detailed geometry improves recognition and reconstruction# and enables more refined use of appearance for scene understanding. We demonstrate this with a new likelihood function that rewards 3D object hypotheses whose 2D projection is more uniform in color distribution. Such a measure would be confused by background pixels if we used a bounding box to represent a concave object like a chair.  Further# we propose modeling complex objects using a set or re-usable parts.  We designed specific data-driven inference mechanisms for each part that are shared by all objects containing that part. This helps make inference transparent to the modeler. We also show how to exploit contextual relationships to detect more objects# by# for example# proposing chairs around and underneath tables.  We present results showing the benefits of each of these innovations.  The performance of our approach often exceeds that of state-of-the-art methods on the two tasks of room layout estimation and object recognition# as evaluated on two bench mark data sets used in this domain. ,Luca del Pero*# ; Joshua Bowdish# University of Arizona; Emily Hartley# University of Arizona; Bonnie Kermgard# University of Arizona; Kobus Barnard# ,prusso83@gmail.com; jbowdish@email.arizona.edu; elh@email.arizona.edu; kermgard@email.arizona.edu; kobus@cs.arizona.edu,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,07.03 Context and scene understanding,,, ,3D and Stereo,P 1A
580,Shape from Silhouette Probability Maps: reconstruction of thin objects in the presence of silhouette extraction and calibration error,This paper considers the problem of reconstructing the shape of thin# texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration.   Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally.  We provide a voxel-based formalism that penalizes false negative and positive error equally# by casting the reconstruction problem as a pseudo-Boolean minimization problem# where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty.  Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions# we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words# silhouettes) or continuously-valued probability maps identically# and places no constraints on camera placement.  The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm).  Results show that our approach reconstructs the complicated branching structure of thin# texture-less objects in the presence of error while intersection-based approaches fail.,Amy Tabb*# AFRS-ARS-USDA,amy.tabb@ars.usda.gov,05.01 3D modeling and reconstruction*,05.01 3D modeling and reconstruction*,10.05 Robot vision,,, ,3D and Stereo,P 1A
1903,Photon Mapping based Simulation of Multi-Path Reflection Artifacts in Time-of-Flight Sensors,Time-of-Flight cameras which use specialized sensors and modulated infrared light are able to measure distances as well as intensity images.  The resulting depth maps show various types of errors such as flying pixels# noise and incorrect depth. In this paper we present a novel method to simulate time-of-flight data# based on modern computer graphics. We modify the photon mapping global illumination algorithm to take the time-dependent propagation of modulated light in  a scene into consideration. This allows us to correctly simulate the depth output of a time-of-flight camera given scene geometry and material properties. So far camera simulators have only been able to provide a physically correct simulation of the sensor electronics and the image creation process. The phenomenon of multi-reflection or multipath interference could not be fully simulated yet. These effects are caused by indirect light paths between camera and lightsource and are therefore dependent on scene geometry. Our method enables us to create ground truth for evaluation# denoising and analysis of such effects.,Stephan Meister*# University of Heidelberg; Rahul Nair# ; Bernd JÃ¤hne# University of Heidelberg; Daniel Kondermann# HCI# Heidelberg University,stephan.meister@iwr.uni-heidelberg.de; rahul.nair@iwr.uni-heidelberg.de; bernd.jaehne@iwr.uni-heidelberg.de; daniel.kondermann@iwr.uni-heidelberg.de,05.02 Active rangefinding and depth sensors*,05.02 Active rangefinding and depth sensors*,01.12 Sensors,,, ,3D and Stereo,P 1A
897,Joint Geodesic Upsampling of Depth Images,We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. Specifically# it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. Though this is closely related to the all-pair-shortest-path problem which has $O(n^2\log n)$ complexity# we develop a novel approximation algorithm whose complexity grows linearly with the image size and achieve real-time performance. We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. In addition# we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps# an important sensor fusion application.,"Ming-Yu Liu*# """"""""""""""Umd.edu#merl.com""""""""""""""; Oncel Tuzel# MERL; Yuichi Taguchi# Mitsubishi Electric Research Labs",mingyliu@umiacs.umd.edu; oncel@merl.com; taguchi@merl.com,05.02 Active rangefinding and depth sensors*,05.02 Active rangefinding and depth sensors*,05.01 3D modeling and reconstruction,,, ,3D and Stereo,P 1A
1987,Relative Volume Constraints for Single View Reconstruction,We introduce the concept of relative volume constraints in order to account for missing information in the reconstruction of 3D objects from a single image.  The key idea is to formulate a variational reconstruction approach with  shape priors in form of relative depth profiles or volume ratios relating object parts.   Such shape priors can easily be derived either from a user sketch or from the object's shading profile in the image. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware.  In contrast to existing single view reconstruction algorithms# the proposed algorithm provides substantially more flexibility to recover shape details such as sharp corners# protuberances# dents and even holes.  Furthermore# we show that the proposed reconstructions can be used for novel view synthesis# relighting and augmented reality applications.,Eno TÃ¶ppe# TU MÃ¼nchen; Claudia Nieuwenhuis*# ; Daniel Cremers# TUM,eno.toeppe@in.tum.de; claudia.nieuwenhuis@in.tum.de; cremers@tum.de,05.04 Image-based Modeling*,05.04 Image-based Modeling*,01.11 PDEs and level-set methods,05.01 3D modeling and reconstruction,08.06 Optimization Methods, ,3D and Stereo,P 1A
1641,Is there a Procedural Logic to Architecture?,Urban models are key to navigation# architecture and entertainment. Apart from visualizing facades# a number of goals have been left aside and largely remain manual# tedious tasks. These goals include compression# generating new facade designs and structurally comparing facades for classification# retrieval and localization.  We propose a novel procedural modelling based method to automatically learn a set of rules for a single or a set of facades# generate new \facade instances and compare facade between them. Our method supersedes manual expert work and cuts the time required to built a procedural model of a facade from several days to a few milliseconds. To deal with the difficulty of grammatical inference# we propose to reformulate the problem. Instead of inferring a compromising# one-size-fits-all# single grammar for all tasks# we infer a model whose successive refinements are rule sets tailored for each task. We demonstrate our automatic rule inference on multiple datasets of regular and landmark facade images.,Julien Weissenberg*# ETHZ; Hayko Riemenschneider# ETH Zurich; Mukta Prasad# ETHZ; Luc  Van Gool# Computer Vision Lab#ETH Zurich,julienw@vision.ee.ethz.ch; hayko@vision.ee.ethz.ch; prasad@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch,05.04 Image-based Modeling*,10.06 Vision for Graphics*,03.05 Shape Representation and Matching,07.04 Image and Video Retrieval,,Façade modeling,3D and Stereo,P 1A
610,Just a Single Labeling for Structural Knowledge Modeling,An object model base that covers a large number of object categories is of great value for many computer vision tasks. As artifacts are usually designed to have various textures# their structure is the primary distinguishing feature between different categories. Thus# how to encode this structural information and how to start the model learning with a minimum of human labeling become two key challenges for the construction of the model base. We design a graphical model that uses object edges to represent object structures# and this paper aims to incrementally learn this category model from one labeled object and a number of not-well-prepared scenes. However# the incremental model learning may be biased due to the limited human labeling. Therefore# we propose a new strategy that uses the depth information in RGBD images to guide the model learning for object detection in ordinary RGB images. Experiments demonstrate the effectiveness of the proposed method. ,"Quanshi Zhang*# University of Tokyo; Xuan Song# University of Tokyo; Xiaowei Shao# """"""""""""""Center for Spatial Information Science# University of Tokyo""""""""""""""; Ryosuke Shibasaki# University of Tokyo; Huijing Zhao# ",zqs1022@csis.u-tokyo.ac.jp; songxuan@csis.u-tokyo.ac.jp; shaoxw@iis.u-tokyo.ac.jp; shiba@csis.u-tokyo.ac.jp; zhaohj@cis.pku.edu.cn,05.04 Image-based Modeling*,05.04 Image-based Modeling*,07.02 Category recognition,,, ,3D and Stereo,P 1A
579,Bayesian Grammar Learning for Inverse Procedural Modeling,In the fields of urban reconstruction and city modeling# shape grammars have emerged as a powerful tool for both synthesizing novel designs and reconstructing existing buildings. So far# a human expert was required to write grammars for specific building styles# which limited the large-scale applicability of these methods. We present a way to automatically learn two-dimensional stochastic context-free grammars (2D-SCFGs) from a set of labeled building facades. To this end# we use Bayesian Model Merging# a technique originally developed in the field of natural language processing# which we extend to the domain of two-dimensional languages. Given a set of labeled positive examples# we induce a grammar which can be sampled to create novel instances of the same building style. Additionally# we demonstrate that our learned grammar can be used for parsing existing facade imagery. Experiments conducted on the dataset of Haussmannian buildings in Paris show that our parsing with learned grammars outperforms bottom-up classifiers and is on par with approaches that use a manually designed style grammar.,Andelo Martinovic*# KU Leuven; Luc Van Gool# KU Leuven,andelo.martinovic@esat.kuleuven.be; Luc.VanGool@esat.kuleuven.be,05.04 Image-based Modeling*,05.04 Image-based Modeling*,08.02 Bayesian modeling and inference,10.06 Vision for Graphics,,Façade modeling,3D and Stereo,P 1A
779,Fusing Depth from Defocus and Stereo with Coded Apertures,In this paper we propose a novel depth measurement method by fusing depth from defocus (DFD) and stereo. One of the problems of passive stereo method is the difficulty of finding correct correspondence between images when an object has a repetitive pattern or edges parallel to the epipolar line. On the other hand# the accuracy of DFD method is inherently limited by the effective diameter of the lens. Therefore# we propose the fusion of stereo method and DFD by giving different focus distances for left and right cameras of a stereo camera with coded apertures. Two types of depth cues# defocus and disparity# are naturally integrated by the magnification and phase shift of a single point spread function (PSF) per camera. In this paper we give the proof of the proportional relationship between the diameter of defocus and disparity which makes the calibration easy.  We also show the outstanding performance of our method which has both advantages of two depth cues through simulation and actual experiments.,Yuichi Takeda# mixi# Inc.; Shinsaku Hiura*# Hiroshima City University; Kosuke Sato# Osaka University,ginrou799@gmail.com; hiura@hiroshima-cu.ac.jp; sato@sys.es.osaka-u.ac.jp,05.06 Shape from focus,01.03 Computational Photography and Video*,01.04 De-blurring and super-resolution,05.06 Shape from focus,05.09 Stereo correspondence, ,3D and Stereo,P 1A
1956,Bayesian Depth-from-Defocus with Shading Constraints,We present a method that enhances the performance of depth-from-defocus (DFD) through the use of shading information. DFD suffers from important limitations -- namely coarse shape reconstruction and poor accuracy on textureless surfaces -- that can be overcome with the help of shading. We integrate both forms of data within a Bayesian framework that capitalizes on their relative strengths. Shading data# however# is challenging to recover accurately from surfaces that contain texture. To address this issue# we propose an iterative technique that utilizes depth information to improve shading estimation# which in turn is used to elevate depth estimation in the presence of textures. With this approach# we demonstrate improvements over existing DFD techniques# as well as effective shape reconstruction of textureless surfaces.,Chen Li*# Zhejiang University; Shuochen Su# Tsinghua University; Yasuyuki Matsushita# MSRA; Kun Zhou# Zhejiang University; Stephen Lin# ,lich0622@gmail.com; sscsuda@gmail.com; yasumat@microsoft.com; kunzhou@acm.org; stevelin@microsoft.com,05.06 Shape from focus*,05.06 Shape from focus*,05.07 Shape from shading and specularities,,, ,3D and Stereo,P 1A
697,Multi-Scale Curve Detection on Surfaces,This paper extends to surfaces the multi-scale approach of edge detection on images. The common practice for detecting curves on surfaces requires the user to first select the scale of the features# apply an appropriate smoothing# and detect the edges on the smoothed surface. This approach suffers from two drawbacks. First# it relies on a hidden assumption that all the features on the surface are of the same  scale. Second# manual user intervention is required. In this paper# we propose a general framework for automatically detecting the optimal scale for each point on the surface. We smooth the surface at each point according to this optimal scale and run the curve detection algorithm on the resulting surface. Our multi-scale algorithm solves the two disadvantages of the single-scale approach mentioned above. We demonstrate how to realize our approach on two commonly-used special cases: ridges & valleys and relief edges. In each case# the optimal scale is found in accordance with the mathematical definition of the curve.,Michael Kolomenkin# Technion; Ilan Shimshoni# University of Haifa; Ayellet Tal*# Technion,michael.kolomenkin@gmail.com; ishimshoni@mis.haifa.ac.il; ayellet@ee.technion.ac.il,05.11 Three-dimensional modeling and manipulation*,05.11 Three-dimensional modeling and manipulation*,01.10 Multi-scale processing,02.02 Edge and contour detection,10.06 Vision for Graphics, ,3D and Stereo,P 1A
683,Intrinsic Characterization of Dynamic Surfaces,This paper presents a novel approach to characterize deformable surface using intrinsic property dynamics. 3D dynamic surfaces representing humans in motion can be obtained using multiple view stereo reconstruction methods or depth cameras. Nowadays these technologies have become capable to capture surface variations in real-time# and give details such as clothing wrinkles and deformations. Assuming repetitive patterns in the deformations# we propose to model complex surface variations using sets of linear dynamical systems (LDS) where observations across time are given by surface intrinsic properties such as local curvatures. We introduce an approach based on bags of dynamical systems# where each surface feature to be represented in the codebook is modeled by a set of LDS equipped with timing structure. Experiments are performed on datasets of real-world dynamical surfaces and show compelling results for description# classification and segmentation.,Tony Tung*# Kyoto University; Takashi Matsuyama# Kyoto University,tony2ng@gmail.com; tm@i.kyoto-u.ac.jp,05.11 Three-dimensional modeling and manipulation*,05.11 Three-dimensional modeling and manipulation*,01.13 Texture analysis and synthesis,02.03 Feature descriptors,03.05 Shape Representation and Matching, ,3D and Stereo,P 1A
640,Pattern-Driven Colorization of 3D Surfaces,Colorization refers to the process of adding color to black & white images or videos. This paper extends the term to handle surfaces in three dimensions. This is important for applications in which the colors of an object need to be restored and no relevant image exists for texturing it. We focus on surfaces with patterns and propose a novel algorithm for adding colors to these surfaces. The user needs only to scribble a few color strokes on one instance of each pattern# and the system proceeds to automatically colorize the whole surface. For this scheme to work# we address not only the problem of colorization# but also the problem of pattern detection on surfaces.,Leifman George*# ; Ayellet Tal# Technion,gleifman@tx.technion.ac.il; ayellet@ee.technion.ac.il,05.11 Three-dimensional modeling and manipulation*,05.11 Three-dimensional modeling and manipulation*,03.05 Shape Representation and Matching,10.06 Vision for Graphics,, ,3D and Stereo,P 1A
690,Three-dimensional bilateral symmetry plane estimation in the phase domain,We show that bilateral symmetry plane estimation for three-dimensional (3-D) shapes may be carried out accurately# and efficiently# in the spherical harmonic  domain.  Our methods are valuable for applications where spherical harmonic expansion is already employed#  such as 3-D shape registration# morphometry# and retrieval. We show that the presence of bilateral symmetry in the 3-D shape is equivalent to a linear phase structure in the corresponding spherical harmonic coefficients# and provide algorithms for estimating the orientation of the symmetry plane.  The benefit of using spherical harmonic phase is that symmetry estimation reduces to matching a compact set of descriptors# without the need to solve a correspondence problem.   Our methods works on point clouds as well as large-scale mesh models of 3-D shapes.  ,Ramakrishna Kakarala*# NTU; Prabhu Kaliamoorthi# Nanyang Technological University; Vittal Premachandran# NanyangTechnologicalUniversity,ramakrishna@ntu.edu.sg; PRAB0009@e.ntu.edu.sg; vittalp@pmail.ntu.edu.sg,05.11 Three-dimensional modeling and manipulation*,05.11 Three-dimensional modeling and manipulation*,03.05 Shape Representation and Matching,05.01 3D modeling and reconstruction,, ,3D and Stereo,P 1A
542,Axially Symmetric 3D Pots Con?guration System using Axis of Symmetry and Break Curve,This paper introduces a completely new approach for reassembling pot sherds found at archaeological excavation sites# for the purpose of reconstructing clay pots that had been made on a wheel. These pots and the sherds into which they have broken are axially symmetric. The reassembly process can be viewed as 3D puzzle solving or generalized cylinder learning from broken fragments. The estimation exploits both local and semi-global geometric structure# thus making it a fundamental problem of geometry estimation from noisy fragments in computer vision and pattern recognition. The data used are densely digitized 3D laser scans of each fragmentÂ’s outer surface. The proposed reassembly system is automatic and functions when the pile of available fragments is from one or multiple pots# and even when pieces are missing from any pot. The geometric structure used are curves on the pot along which the surface had broken and the silhouette of a pot with respect to an axis# called axis-profile curve (APC). For reassembling multiple pots with or without missing pieces# our algorithm estimates the APC from each fragment# then reassembles into configurations the ones having distinctive APC. Further growth of configurations is based on adding remaining fragments such that their APC and break curves are consistent with those of a configuration. The method is novel# more robust and handles the largest numbers of fragments to date.,Kilho Son*# Brown University; Eduardo Almeida# Brown University; David Cooper# Brown University,kilho_son@brown.edu; Eduardo_Almeida@brown.edu; david_cooper@brown.edu,05.11 Three-dimensional modeling and manipulation*,05.11 Three-dimensional modeling and manipulation*,"08.01 Learning# statistics# and inference""",,, ,3D and Stereo,P 1A
1357,Wide-baseline Hair Capture using Strand-based Refinement,We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands are first extracted in each 2D view# and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency# the visual hull constraint and smoothness constraints defined at the strand# wisp and global levels. Based on the refined strands# the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of about 3mm. We also show real-world examples to demonstrate the capability to capture full-head hair styles as well as hair in motion with as few as 8 cameras. ,Linjie Luo*# Princeton University; Cha Zhang# ; Zhengyou Zhang# Microsoft; Szymon Rusinkiewicz# Princeton University,linjiel@princeton.edu; chazhang@microsoft.com; zhang@microsoft.com; smr@princeton.edu,05.05 Multi-view stereo,05.01 3D modeling and reconstruction*,05.04 Image-based Modeling,05.05 Multi-view stereo,, ,3D and Stereo,P 1A
1414,Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera,Motion blur frequently occurs in dense 3D reconstruction using a single moving camera# and it degrades the quality of the 3D reconstruction. To handle motion blur caused by rapid camera shakes# we propose a blur-aware depth reconstruction method# which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. Motion blur is dependent on 3D geometry# thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. The estimated depth is then converted into pixel-wise blur kernels# and non-uniformmotion blur is easily removed with low computational cost. The obtained blur kernel is depth-dependent# thus it effectively addresses scene-depth variation# which is a challenging problem in conventional non-uniform deblurring methods.,Heeseok Lee# Seoul National University; Kyoung Mu Lee*# Seoul National University,ultra21@snu.ac.kr; kyoungmu@snu.ac.kr,05.05 Multi-view stereo*,05.05 Multi-view stereo*,01.04 De-blurring and super-resolution,05.01 3D modeling and reconstruction,, ,3D and Stereo,P 1A
1417,Simultaneous Depth Reconstruction and Image Super-Resolution with a Single Camera,In this paper# we propose a convex optimization framework for simultaneous estimation of super-resolved depth map and images from a single moving camera. The pixel measurement error in 3D reconstruction is directly related to the resolution of the images at hand. In turn# even a small measurement error can cause significant errors in reconstructing 3D scene structure or camera pose. Therefore# enhancing image resolution can be an effective solution for securing the accuracy as well as the resolution of 3D reconstruction. In the proposed method# depth map estimation and image super-resolution are formulated in a single energy minimization framework with a convex function and solved efficiently by a first-order primal-dual algorithm. Explicit inter-frame pixel correspondences are not required for our super-resolution procedure# thus we can avoid a huge computation time and obtain improved depth map in the accuracy and resolution as well as highresolution images with reasonable time. The superiority of our algorithm is demonstrated by presenting the improved depth map accuracy# image super-resolution results# and camera pose estimation.,Heeseok Lee# Seoul National University; Kyoung Mu Lee*# Seoul National University,ultra21@snu.ac.kr; kyoungmu@snu.ac.kr,05.05 Multi-view stereo*,05.05 Multi-view stereo*,01.04 De-blurring and super-resolution,05.01 3D modeling and reconstruction,, ,3D and Stereo,P 1A
839,Recovering Stereo Pairs from Anaglyphs,An anaglyph is a single image created by selecting complementary colors from a stereo color pair; the user can perceive depth by viewing it through color-filtered glasses. We propose a technique to reconstruct the original color stereo pair given such an anaglyph. We modified SIFT-Flow and use it to initially match the different color channels across the two views. Our technique then iteratively refines the matches# selects the good matches (which defines the ``anchor'' colors)# and propagates the anchor colors. We use a diffusion-based technique for the color propagation# and added a step to suppress unwanted colors. Results on a variety of inputs are shown to illustrate the robustness of our technique. We also extended our method to anaglyph videos by using optic flow between time frames.,Armand Joulin*# Inria; Sing Bing Kang# Microsoft Research,armand.joulin@ens.fr; sbkang@microsoft.com,05.09 Stereo correspondence,01.01 Image processing*,"01.08 Image enhancement# restoration# and denoising""",,,Stereo,3D and Stereo,P 1A
989,Exploiting the Power of Stereo Confidences,Applications based on stereo vision are becoming increasingly common# ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level# far less attention has been dedicated to the use of stereo confidence cues. Mostly# a threshold is applied to the confidence values for further processing# which is essentially a sparsified disparity map. This approach is straightforward but it does not take full advantage of the available information. In this paper# we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information# a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World# a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a posteriori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level. ,David Pfeiffer*# Daimler AG; Nicolai Schneider# IT-Designers GmbH; Stefan Gehrig# Daimler AG,david@dltmail.de; nicolai.schneider@it-designers.de; stefan.gehrig@daimler.com,05.09 Stereo correspondence,05.01 3D modeling and reconstruction*,03.03 Image segmentation,05.09 Stereo correspondence,07.06 Object Detection, ,3D and Stereo,P 1A
1685,Ensemble Learning for Confidence Measures in Stereo Vision,With the aim to improve accuracy of stereo confidence measures# we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawn from the recently introduced KITTI dataset# which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm# which is the best performing real- time capable stereo method known to date. On KITTI images# SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically# our method performs best in all but one out of 194 frames of the KITTI dataset. ,Ralf Haeusler*# The University of Auckland; Rahul Nair# ; Daniel Kondermann# HCI# Heidelberg University,rhae001@aucklanduni.ac.nz; rahul.nair@iwr.uni-heidelberg.de; daniel.kondermann@iwr.uni-heidelberg.de,05.09 Stereo correspondence*,05.09 Stereo correspondence*,,,, ,3D and Stereo,P 1A
1857,Segment-Tree based Cost Aggregation for Stereo Matching,"   This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants# a new tree structure# ""Segment-Tree""# is proposed for non-local matching cost aggregation. Conceptually# the segment-tree is constructed in a three-step process: first# the pixels are grouped into a set of segments with the reference color or intensity image; second# a tree graph is created for each segment; and in the final step# these independent segment graphs are linked to form the segment-tree structure. In practice# this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights# our method introduces some 'non-local' decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities# and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree in two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable with previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore# the tree structure can be refined with the estimated disparities# which leads to consistent visual scene segmentation and significantly better aggregation results.",Xing Mei*# CASIA; Xun Sun# ; Weiming Dong# CAS Institute of Automation; Xiaopeng ZHANG# Institute of Automation# CAS,xmei@nlpr.ia.ac.cn; xun.s@qq.com; Weiming.Dong@ia.ac.cn; xpzhang@nlpr.ia.ac.cn,05.09 Stereo correspondence*,05.09 Stereo correspondence*,,,, ,3D and Stereo,P 1A
1373,Multi-Class Video Co-Segmentation with a Generative Multi-Video Model,Video data provides a rich source of information that is available to us today in large quantities e.g. from online resources. Tasks like segmentation benefit greatly from the analysis of spatio-temporal motion patterns in videos and recent advances in video segmentation has shown great progress in exploiting these addition cues. However# observing a single video is often not enough to predict meaningful segmentations and inference across videos becomes necessary in order to predict segmentations that are consistent with objects classes. Therefore the task of video cosegmentation is being proposed# that aims at inferring segmentation from multiple videos. But current approaches are limited to only considering binary foreground/background segmentation and multiple videos of the same object. This is a clear mismatch to the challenges that we are facing with videos from online resources or consumer videos. We propose to study multi-class video co-segmentation where the number of object classes is unknown as well as the number of instances in each frame and video. We achieve this by formulating a non-parametric bayesian model across videos sequences that is based on a new videos segmentation prior as well as a global appearance model that links segments of the same class. We present the first multi-class video co-segmentation evaluation. We show that our method is applicable to real video data from online resources and outperforms state-of-the-art video segmentation and image co-segmentation baselines.,Wei-Chen Chiu*# MPI-INF; Mario Fritz# MPI for Informatics,walon@mpi-inf.mpg.de; mfritz@mpi-inf.mpg.de,08.02 Bayesian modeling and inference*,08.02 Bayesian modeling and inference*,03.03 Image segmentation,,, ,Statistics and Learning,P 1B
433,A Bayesian Approach to Multimodal Visual Dictionary Learning,Despite significant progress# most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. However# Web images are often associated with text data which may carry substantial information regarding image semantics# and may be exploited for visual dictionary learning. This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering# in addition to information of image descriptors. Existing co-clustering algorithms are not optimal here# because they completely ignore the structure of image descriptors in continuous space# which is crucial to capture visual properties of images. In this paper# we propose a novel Bayesian co-clustering model# which jointly estimates the underlying distributions of image descriptors over continuous space as well as the relationship between the distributions and textual words through a unified Bayesian inference. Extensive experiments on image categorization and retrieval have validated the substantial values of textual words in improving visual dictionary learning# where our model shows superior performance over several recent methods.,Go Irie*# NTT; Dong Liu# Columbia University; Zhenguo Li# Columbia University; Shih-Fu Chang# Columbia University,irie.go@lab.ntt.co.jp; dongliu@ee.columbia.edu; zl2229@columbia.edu; sfchang@ee.columbia.edu,08.02 Bayesian modeling and inference*,08.02 Bayesian modeling and inference*,08.09 Statistical Methods and Learning,,, ,Statistics and Learning,P 1B
1634,A Statistical Model for Recreational Trails in Aerial Images,We present a statistical model of aerial images of recreational trails# and a method to infer trail routes in such images. We learn a set of textons describing the images# and use them to divide the image into super-pixels represented by their texton. We then learn# for each texton# the frequency of generating on-trail and off-trail pixels# and the direction of trail through on-trail pixels. From these# we derive an image likelihood function. We combine that with a prior model of trail length and smoothness# yielding a posterior distribution for trails# given an image. We search for good values of this posterior using a novel stochastic variation of DijkstraÂ’s algorithm. Our experiments on trail images and groundtruth collected in the western continental USA# show substantial improvement over those of the previous best trail-finding method. ,Andrew Predoehl*# University of Arizona; Scott Morris# Topofusion; Kobus Barnard# University of Arizona,predoehl@cs.arizona.edu; smorris@topofusion.com; kobus@sista.arizona.edu,08.02 Bayesian modeling and inference*,08.02 Bayesian modeling and inference*,08.09 Statistical Methods and Learning,10.01 Aerial and outdoor image analysis and modeling,, ,Statistics and Learning,P 1B
527,Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution,This paper addresses the problem of learning over-complete dictionaries for the coupled feature spaces# where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous dictionary learning methods where single sparse model is used for concatenated spaces# the proposed method provides the flexibility to model the two spaces separately while still capturing the relationship between them. We use the same beta process prior but different sparse coding for the two feature spaces; allowing the sparse coding for different feature spaces to have the same dictionary atom indicator but different coefficient values. In this way the algorithm can minimize the learning errors while still maintaining the relationship of the two feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods by applying this method to single image super-resolution. The experimental results show that dictionaries learned by our method produces the best super-resolution results compared to other state-of-the-art methods.,Li He*# University of Tennessee# Knoxville; Hairong Qi# University of Tennessee# Knoxville; russell Zaretzki# University of Tennessee# Knoxville,lhe4@utk.edu; hqi@utk.edu; rzaretzk@utk.edu,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,01.04 De-blurring and super-resolution,08.02 Bayesian modeling and inference,, ,Statistics and Learning,P 1B
1314,Dictionary Learning from Ambiguously Labeled Data,We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification# where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm# two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.,Yi-Chen Chen*# UMIACS# University of Maryland; Vishal Patel# University of Maryland; Jai Shanker Pillai# University of Maryland# College Park; Rama Chellappa# UMD; Jonathon Phillips# NIST,chenyc08@umiacs.umd.edu; pvishalm@umiacs.umd.edu; jsp@umiacs.umd.edu; rama@umiacs.umd.edu; jonathon.phillips@nist.gov,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,07.01 Recognition,07.07 Object Recognition,, ,Statistics and Learning,P 1B
1503,Generalized Domain-Adaptive Dictionaries,Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However# when the target data has a different distribution than the source data# the learned sparse representation may not be optimal. In this paper# we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically# we describe a technique which jointly learns projections of data in the two domains# and a latent dictionary which can succinctly represent both the domains in the projected low dimensional space. An efficient optimization technique is presented# which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary# which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains# and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the method performs on par or better than the competitive state-of-the-art methods.,Sumit Shekhar*# University of Maryland; Vishal Patel# University of Maryland; Hien Nguyen# ; Rama Chellappa# UMD,sshekha@umiacs.umd.edu; pvishalm@umiacs.umd.edu; hien@umiacs.umd.edu; rama@umiacs.umd.edu,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,07.01 Recognition,07.07 Object Recognition,08.01 Learning# statistics# and inference, ,Statistics and Learning,P 1B
339,Tag Taxonomy Aware Dictionary Learning for Region Tagging,Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper# using the given tag taxonomy# we propose to jointly learn multilayer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically# we generate a node-specific dictionary for each tag node in the taxonomy# and then concatenate the node-specific dictionaries from the same level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously# the sparse codes obtained using the level-specific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers from lower levels# but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that the proposed approach yields the best performance over all previous methods.,Jingjing Zheng*# University of Maryland; Zhuolin Jiang# University of Maryland; Rama Chellappa# UMD,zjngjng@gmail.com; zhuolin@umiacs.umd.edu; rama@umiacs.umd.edu,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,07.02 Category recognition,07.03 Context and scene understanding,07.04 Image and Video Retrieval, ,Statistics and Learning,P 1B
2055,Block and Group Regularized Sparse Modeling for Dictionary Learning,This paper proposes a dictionary learning framework that combines two novel block/group sparse coding (BGSC) or Reconstructed Block/Group Sparse Coding} (R-BGSC) schemes  with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group  while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that  can be considered as a direct consequence of incorporating both the group structure  for the input data and the block structure for the dictionary in the learning  process.   The optimization problems for both the dictionary learning  and sparse coding can be solved efficiently using block-coordinate descent#  and the details of the optimization algorithms are presented. We evaluate the proposed methods on several classification (supervised)  and clustering (unsupervised) problems using well-known datasets. Favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.,Yu-Tseh Chi*# University of Florida; Mohsen Ali# University of Florida; Ajit Rajwade# ; Jeffrey Ho# University of Florida,ychi@cise.ufl.edu; moali@cise.ufl.edu; avr@cise.ufl.edu; jho@cise.ufl.edu,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,07.02 Category recognition,07.05 Instance recognition,, ,Statistics and Learning,P 1B
106,Multi-Level Discriminative Dictionary Learning towards Hierarchical Visual Categorization,For the task of visual categorization# the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure# or rely on a set of pre-computed visual features# they may not be appreciated for dealing with large numbers of categories. In this paper# we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure# a discriminative dictionary and a set of class models are learnt for classification# and the dictionaries in different layers aim to exploit the discriminative visual properties of different granularity. Moreover# the dictionaries in lower level also inherit the dictionary of ancestor nodes# so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large number of classes compared with some state-of-the-art methods# and is more efficient in processing large number of categories.,Li Shen# VIPL#ICT.CAS; Shuhui Wang*# VIPL#ICT#CAS; Shuqiang Jiang# ; Qingming Huang# Graduate Univ of Chinese Academy of Sciences,lshen@jdl.ac.cn; wangshuhui@ict.ac.cn; sqjiang@jdl.ac.cn; qmhuang@jdl.ac.cn,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,07.07 Object Recognition,08.09 Statistical Methods and Learning,, ,Statistics and Learning,P 1B
262,Fast Convolutional Sparse Coding,Sparse coding has become an increasingly popular method in learning and vision for a variety of classification# reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however# sparse coding is applied to sub-elements (i.e. patches) of the signal# where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator# however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper# we draw upon ideas from signal processing and Augmented Lagrange multipliers (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence. ,Hilton Bristow*# CSIRO; Simon Lucey# CSIRO,hilton.bristow+papers@gmail.com; simon.lucey@csiro.au,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,08.01 Learning# statistics# and inference,"08.06 Optimization Methods""",, ,Statistics and Learning,P 1B
1451,In Defense of Sparsity Based Face Recognition,The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a ``prototype plus variation'' representation model for sparsity based face recognition. Based on the new model# a linear discriminant SRC (LDSRC)# in which the dictionary is assembled by the class centroids and the sample-to-centroid differences# leads to a substantial improvement on SRC when the training images are uncontrolled and the number of samples per class is insufficient. Our experimental results on the AR# FERET# and FRGC databases validate that# if the proposed prototype plus variation representation model is applied# sparse coding plays a crucial role in face recognition# and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.,Weihong Deng*# Beijing Univ. Post. & Telecom.; Jiani Hu# Beijing Univ. Post. & Telecom.; Jun Guo# ,whdeng@bupt.edu.cn; jnhu@bupt.edu.cn; junguo@bupt.edu.cn,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,09.04 Face recognition,,, ,Statistics and Learning,P 1B
4,Transfer Sparse Coding for Robust Image Representation,Sparse coding# which learns a set of high-level semantics and encodes images in sparse representations# has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However# when labeled and unlabeled images are sampled from different distributions# they may be quantized into different visual words of the codebook and encoded with different representations# which may severely violate classification performance. In this paper# we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically# we aim to minimize the distribution divergence between the labeled and unlabeled images# and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-of-the-art methods on two types of computer vision datasets.,Mingsheng Long# Tsinghua University; Guiguang Ding# Tsinghua University; Jianmin Wang*# Tsinghua University; Yuchen Guo# Tsinghua University; Philip Yu# University of Illinois at Chicago,longmingsheng@gmail.com; dinggg@tsinghua.edu.cn; jimwang@tsinghua.edu.cn; guoyc09@gmail.com; psyu@uic.edu,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,,,, ,Statistics and Learning,P 1B
346,Online Robust Dictionary Learning,Online dictionary learning is particularly useful for processing large scale and dynamic data in computer vision. It# however# still faces major difficulty to incorporate robust function# rather than the simple $\ell^2$-norm data term# to handle outliers in training data. In this paper# we propose a new online framework enabling the use of the $\ell^1$-norm data term in robust dictionary learning# notably expanding the usability and practicality of this important technique. Extensive experiments are carried out to evaluate our new framework.,Cewu Lu# The Chinese University of Hong; Jianping Shi*# CUHK; Jiaya Jia# ,cwlu@cse.cuhk.edu.hk; jpshi@cse.cuhk.edu.hk; leojia@cse.cuhk.edu.hk,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,,,, ,Statistics and Learning,P 1B
1453,Multi-Task Sparse Learning with Beta Process Prior for Action Recognition,In this paper# we formulate human action recognition as a novel Multi-Task Sparse Learning(MTSL) framework which aims to construct a test sample with multiple features from as few bases as possible. Learning the sparse representation under each feature modality is considered as a single task in MTSL. Since the tasks are generated from multiple features associated with the same visual input# they are not independent but inter-related. We introduce a Beta process(BP) prior to the hierarchical MTSL model# which efficiently learns a compact dictionary and infers the sparse structure shared across all the tasks. The MTSL model enforces the robustness in coefficient estimation compared with performing each task independently. Besides# the sparseness is achieved via the Beta process formulation rather than the computationally expensive l1 norm penalty. In terms of non-informative gamma hyper-priors# the sparsity level is totally decided by the data. Finally# the learning problem is solved by Gibbs sampling inference which estimates the full posterior on the model parameters. Experimental results on the KTH# UCF sports# and UCF films datasets demonstrate the effectiveness of the proposed MTSL approach for action recognition.,Chunfeng Yuan*# Institute of Automation; Weiming Hu# ; shuang Yang# ; Guodong Tian# ,cfyuan@nlpr.ia.ac.cn; wmhu@nlpr.ia.ac.cn; syang@nlpr.ia.ac.cn; gdtian@nlpr.ia.ac.cn,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,,,, ,Statistics and Learning,P 1B
1461,Approximate Sparse Subspace Clustering for Large Scale Data Sets,"Recently# Sparse Subspace Clustering (SSC)# which constructs a sparse similarity graph for spectral clustering by using $\ell^1$-minimization based coefficients# has achieved state-of-the-art results for image clustering and motion segmentation. However# SSC is not suitable for large scale data sets owing to over-high time complexity and space complexity. This paper proposes a simple but efficient method# named as Approximate Sparse Subspace Clustering (aSSC)# that makes SSC feasible for large scale data sets. The solution of aSSC adopts a ""sampling# clustering# coding# and classifying"" strategy. Firstly# it randomly selects a small number of instances as in-sample data; secondly# constructs a sparse similarity graph and calculates the cluster membership over the chosen samples following the scheme of SSC; thirdly# represents the remaining instances (out-of-sample data) as the linear combinations of these in-sample data; finally# assigns out-of-sample data to the cluster which produces minimal residual over in-sample data. Extensive experimental results on several popular data sets demonstrate the effectiveness and efficiency of our method comparing with the state-of-the-art algorithms.",Xi PENG# Sichuan University; Lei Zhang*# Sichuan University; Zhang Yi# Sichuan University,pangsaai@gmail.com; leizhang@scu.edu.cn; zhangyi@scu.edu.cn,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,,,, ,Statistics and Learning,P 1B
1667,Separable Dictionary Learning,Many techniques in computer vision# machine learning# and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically# or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation# learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery# unfortunately# the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information.   The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand# this permits larger patch-sizes for the learning phase# on the other hand# the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole# thus enforcing basic dictionary properties such as mutual incoherence explicitly into the learning procedure. In the special case where no separable structure is enforced# our method competes with state-of the art dictionary learning methods like K-SVD.,Matthias Seibert# Technische UniversitÃ¤t MÃ¼nchen; Martin Kleinsteuber# Technische UniversitÃ¤t MÃ¼nchen; Simon Hawe*# Technische UniversitÃ¤t MÃ¼nchen,m.seibert@tum.de; kleinsteuber@tum.de; simon.hawe@tum.de,08.08 Sparse coding and dictionaries*,08.08 Sparse coding and dictionaries*,,,, ,Statistics and Learning,P 1B
304,Compressed Hashing,Recent studies have shown that hashing methods are effective for high dimensional nearest neighbor search. A common problem shared by many existing hashing methods is that in order to achieve a satisfied perfor- mance# a large number of hash tables (i.e.# long code- words) are required. To address this challenge# in this paper we propose a novel approach called Compressed Hashing by exploring the techniques of sparse coding and compressed sensing. In particular# we introduce a sparse coding scheme# based on the approximation the- ory of integral operator# that generate sparse represen- tation for high dimensional vectors. We then project s- parse codes into a low dimensional space by effectively exploring the Restricted Isometry Property (RIP)# a key property in compressed sensing theory. Both of the the- oretical analysis and the empirical studies on two large data sets show that the proposed approach (i) scales well to large data sets# and (ii) is significantly more effective than the state-of-the-art hashing algorithms for high di- mensional nearest neighbor search.,Yue Lin*# Zhejiang University; Rong Jin# ; Deng Cai# ; Xuelong Li# ,linyue29@gmail.com; rongjin@cse.msu.edu; dengcai@gmail.com; xuelong_li@opt.ac.cn,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,02.01 Feature extraction and matching,,, ,Statistics and Learning,P 1B
1208,Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces,Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once# with fixed clusters used for matching with other image sets. However# this may result in the two closest clusters to represent different characteristics of an object# due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem# we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace# we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error# we approach the nearest point on a Grassmann manifold. Experiments on Honda# ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques# such as Affine Hull based Image Set Distance (AHISD)# Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).,Shaokang Chen*# NICTA; Conrad Sanderson# NICTA; Mehrtash Harandi# National ICT Australia (NICTA); Brian Lovell# NICTA,shaokangchenuq@gmail.com; Conrad.Sanderson@nicta.com.au; mehrtash.harandi@nicta.com.au; Brian.Lovell@nicta.com.au,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,07.01 Recognition,07.07 Object Recognition,, ,Statistics and Learning,P 1B
1264,Optimizing 1-Nearest Prototype Classifiers,Although the development of complex# powerful classifiers and their constant improvement have contributed much to the progress in many computer vision fields# the trend towards large scale datasets and as a consequence reduced test time has revived the interest in simpler classifiers like nearest neighbor algorithms. In recent related work# the main idea is to represent data samples by assigning them to prototypes that partition the input feature space and to afterwards apply linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper# we go a step beyond these approaches and purely focus on simple 1-nearest neighbor classification# where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable# parameter free# avoids noise overfitting and since during testing only comparisons to the derived prototypes are required# highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods# while even getting close to the results of more complex classifiers.,Paul Wohlhart*# Graz University of Technology; Michael Donoser# ; Peter Roth# ; Horst Bischof# ,wohlhart@icg.tugraz.at; michael.donoser@tugraz.at; pmroth@icg.tugraz.at; bischof@icg.tugraz.at,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,07.01 Recognition,07.07 Object Recognition,, ,Statistics and Learning,P 1B
1913,Sparse Subspace Denoising for Image Manifolds,With the increasing availability of high dimensional data and demand in sophisticated data analysis algorithms# manifold learning becomes a critical technique to perform dimensionality reduction# unraveling the intrinsic data structure. The real-world data however often come with noises and outliers; seldom# all the data live in a single linear subspace. Inspired by the recent advances in sparse subspace learning and diffusion-based approaches# we propose a new manifold denoising algorithm in which data neighborhoods are adaptively inferred via sparse subspace reconstruction; we then derive a new formulation to perform denoising to the original data. Experiments carried out on both toy and real applications demonstrate the effectiveness of our method; it is insensitive to parameter tuning and we show significant improvement over the competing algorithms. ,Bo Wang*# ; Zhuowen Tu# ,wangbo.yunze@gmail.com; zhuowen.tu@gmail.com,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,07.01 Recognition,07.07 Object Recognition,08.01 Learning# statistics# and inference, ,Statistics and Learning,P 1B
2064,Weakly Supervised Learning of Mid-Level Features for Object Recognition,The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. Attributes provide an intermediate feature representation in between low level features and the class categories# and offer several attractive properties among which are improved learning of novel categories based on few examples# as well as facilitating the task of zero-shot learning. However the major caveat is that learning semantic attributes is an arduous task# requiring a significant amount of time and human intervention to providing labels. In order to address this issue we propose a weakly supervised approach to learn mid level features# where the only supervision is provided by the category classes of the training examples. We develop a new type of restricted Boltzmann machine (RBM) which unlike the standard RBM uses the class labels to promote more efficient sharing of information by different categories. This tends to improve the generalization performance. By using semantic attributes for which annotations are available# we show that we can find correspondences between the mid-level features that we learn# and the labeled attributes. Therefore the mid-level features have distinct semantic characterization which is very similar to that given by the semantic attributes# even though their labeling was not used during the training process. Our experimental results in object recognition tasks show significant performance gains# outperforming methods which rely on manually labeled semantic attributes.,Roni Mittelman*# University of Michigan; Honglak Lee# ; Benjamin Kuipers# University of Michigan; Silvio Savarese# University of Michigan,rmittelm@umich.edu; honglak@eecs.umich.edu; kuipers@umich.edu; silvio@eecs.umich.edu,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,07.01 Recognition,07.02 Category recognition,07.07 Object Recognition, ,Statistics and Learning,P 1B
673,Learning Binary Codes for High-dimensional Data using Bilinear Projections,Recent advances in recognition indicate that to achieve good retrieval and classification accuracy on extremely large-scale datasets# high-dimensional visual descriptors# e.g.# Fisher Vectors# are needed. We present a novel method for converting such descriptors to similarity-preserving binary codes that exploits their natural matrix structure to reduce their dimensionality using compact bilinear projections instead of a single large projection matrix. This method achieves comparable retrieval and classification accuracy to the original descriptors and to the state of-the-art Product Quantization approach while having a much faster code generation time and smaller memory footprint.,Yunchao Gong*# UNC Chapel Hill; Sanjiv Kumar# Google; Henry Rowley# Google; Svetlana Lazebnik# UIUC,yunchao@cs.unc.edu; sanjivk@google.com; har@google.com; slazebni@illinois.edu,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,07.02 Category recognition,07.04 Image and Video Retrieval,, ,Statistics and Learning,P 1B
1160,Semi-supervised Node Splitting for Random Forest Construction,Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper# we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular# we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality# we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmentation. Experimental results on publicly available datasets demonstrate the superiority of our method.,Xiao Liu# ; Mingli Song*# Zhejiang University; Dacheng Tao# University of Technology# Sydney; Zicheng Liu# ; Luming Zhang# ; Chun Chen# ; Jiajun Bu# ,ender_liux@zju.edu.cn; brooksong@zju.edu.cn; dacheng.tao@gmail.com; zliu@microsoft.com; zglumg@zju.edu.cn; chenc@zju.edu.cn; bjj@zju.edu.cn,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,07.02 Category recognition,08.01 Learning# statistics# and inference,"08.02 Bayesian modeling and inference""", ,Statistics and Learning,P 1B
2151,Admixture models with topics as overlapping windows into a grid of counts: Componential Counting Grids,Recently# the counting grid (CG) model \cite{cg2} was developed to represent each input image as a point in a large grid of feature (SIFT# color# high level feature) counts. This latent point is a corner of a window of grid points which are all uniformly combined to form feature counts that match the (normalized) feature counts in the image. As bag of words model with a spatial layout in the latent space# the CG model has superior handling of field of view changes in comparison to other bag of word models# but with the price of being essentially a mixture# mapping the entire scene to a single window in the grid. Here# we extend the model so that each input image is represented by multiple latent locations# rather than just one (Fig. \ref{fig:gm}). In this way# we make a substantially more flexible admixture model -- the componential counting grid (CCG) -- which can break each image into its parts and map them to separate windows in a counting grid allowing for smooth topic transitions. Furthermore# the CCG model creates connections between two popular generative modeling strategies in computer vision# previously seen as very different: By varying the image tessellation and window size of CCG# we get a variety of models among which we find latent Dirichlet allocation \cite{lda#lda1} as well as flexible sprites/layered epitomes \cite{flex} at two ends# or rather corners Fig.\ref{fig:figure1}# of the spectrum. In each of these corners# substantial research effort has been invested to refine and apply these basic approaches# but it turns out that the CCG models at neither end of the spectrum tend to perform best in our experiments.,Alessandro Perina*# Microsoft Research; Nebojsa Jojic# Microsoft Research,alperina@microsoft.com; jojic@microsoft.com,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,07.03 Context and scene understanding,08.04 Graphical models,, ,Statistics and Learning,P 1B
532,Alternating Decision Forests,This paper introduces a novel classification method termed Alternating Decision Forests (ADF)# which formulates the training of Random Forests explicitly as a global loss  minimization problem. During training# the losses are minimized via keeping an adaptive weight distribution over the training samples# similar to Boosting methods. In order to keep the method as flexible and general as possible# we adopt the principle of employing gradient descent in function space# which allows to minimize arbitrary  losses. Contrary to boosted decision trees# in our method the weight minimization is an inherent part of the tree growing process# thus allowing to keep the benefits of common Random Forests# such as# parallel processing. In the paper# we derive the new classifier and give a detailed discussion and evaluation on standard machine learning data sets. We further show a specific vision application# i.e.# object detection# that makes direct use of the new Alternating Decision Forest formulation. In contrast to standard Random Forests# ADF leads to higher performance# more compact models and better understanding of the overall learning process.,Samuel Schulter*# TUGraz; Peter Roth# ,schulter@icg.tugraz.at; pmroth@icg.tugraz.at,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,"08.01 Learning# statistics# and inference""",,, ,Statistics and Learning,P 1B
1467,Exploring Implicit Image Statistics for Visual Representativeness Modeling,In this paper# we proposed a computational model of visual representativeness by integrating cognitive theories of representativeness heuristics with computer vision and machine learning techniques. Unlike previous models that build their representativeness measure based on the visible data# our model takes the initial inputs as explicit positive reference and extend the measure by exploring the implicit negatives. Given a group of images that contains obvious visual concepts# we create a customized image ontology consisting of both positive and negative instances by mining the most related and confusable neighborhoods of the positive concept in ontological semantic knowledge bases. The representativeness of a new item is then determine by its likelihoods for both the positive and negative references. To ensure the effectiveness of probability inference as well as the cognitive plausibility# we discover the potential prototypes and treat them as an intermediate representation of semantic concepts. In the experiment# we evaluate the performance of representativeness models based on both human judgements and user-click logs of commercial image search engine. Experimental results on both ImageNet and image sets of general concepts demonstrate the superior performance of our model against the state-of-the-arts.,Xiaoshuai Sun*# Harbin Institute of Technology; Xin-Jing Wang# ; Hongxun Yao# Harbin Institute of Technology; Lei Zhang# Microsoft Research Asia,xiaoshuaisun@hit.edu.cn; xjwang@microsoft.com; h.yao@hit.edu.cn; leizhang@microsoft.com,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,"08.01 Learning# statistics# and inference""",,, ,Statistics and Learning,P 1B
630,A Divide-and-Conquer Method for Scalable Low-rank Latent Matrix Pursuit,Data fusion# which effectively fuses multiple models from different kinds of features to obtain an accurate model# is a crucial component in various computer vision applications. Robust late fusion (RLF) is a recent proposed method that fuses multiple output score lists from different models via pursuing a shared low-rank latent matrix. Despite showing promising performance# the repeated full Singular Value Decomposition operations in RLF's optimization algorithm limits its scalability in real world vision datasets which usually have large number of test examples. To address this issue# we provide a scalable solution for large-scale low-rank latent matrix pursuit by a divide-and-conquer method. The proposed method divides the original low-rank latent matrix learning problem into two size-reduced subproblems# which may be solved via any base algorithm# and combines the results from the subproblems to obtain the final solution. Our theoretical analysis shows that with fixed probability# the proposed divide-and-conquer method has recovery guarantees comparable to those of its base algorithm. Moreover# we develop an efficient base algorithm for the corresponding subproblems by factorizing a large matrix into the product of two size-reduced matrices. We also provide high probability recovery guarantees of the base algorithm. The proposed method is evaluated on various fusion problems in object categorization and video event detection. Under comparable accuracy# the proposed method performs more than $180$ times faster than the state-of-the-art baselines on the CCV dataset with about 4#500 test examples for video event detection. ,Yan Pan*# Sun Yat-sen University; Hanjiang Lai# Sun Yat-sen University; Cong Liu# Sun Yat-sen University; Shuicheng YAN# NUS,panyan5@mail.sysu.edu.cn; lyfflhj@126.com; gzcong@gmail.com; eleyans@nus.edu.sg,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,08.06 Optimization Methods,09.09 Video Analysis and Event Recognition,, ,Statistics and Learning,P 1B
2056,Supervised Descent Method and its Applications to Face Alignment,Many computer vision problems (e.g.# camera calibration# image alignment# structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust# fast and reliable approach for nonlinear optimization of a general smooth function. However# in the context of computer vision 2nd order descent methods have two main drawbacks: (1) the function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite.  To address these issues# this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training# the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing# SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples# and show how SDM achieves state-of-the-art performance in the problem of facial feature detection.,Xuehan Xiong*# Carnegie Mellon University; Fernando DelaTorre# ,xiong828@gmail.com; ftorre@cs.cmu.edu,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,09.03 Face detection and head tracking,,, ,Statistics and Learning,P 1B
2020,Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences,Data sequences alignment of human behaviour# especially from visual data# is a very challenging problem due to a numerous reasons# including possible large temporal scale differences# inter/intra subject variability and# more importantly# due to the presence of gross errors and outliers in the data. In such data gross errors are often in abundance due to incorrect localization and tracking# presence of partial occlusion etc. Furthermore# such errors rarely follow a Gaussian distribution# which is the de-facto assumption undertaken in the majority of methodologies. In this paper# building on recent advances on rank minimization# a novel# robust to gross errors temporal alignment method is proposed. While past approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences# we aim at learning two underlying projection matrices (one for each sequence)# which not only maximally correlate the sequences but# in the same time# efficiently remove the possible corruptions in any datum in the sequences. Learning is conducted by solving a nuclear-norm regularized minimization problem# which is convex and can be solved in polynomial time. Time series alignment is performed by combing the proposed optimization problem with DTW. The superiority of the proposed method against state-of-the-art alignment methods# namely the canonical time warping and the generalized time warping# is indicated by the experimental results on both synthetic and real datasets.,Stefanos Zafeiriou*# Imperial; Maja Pantic# ; Mihalis Nicolaou# Imperial College London; Yannis Panagakis# Imperial College London,s.zafeiriou@imperial.ac.uk; m.pantic@imperial.ac.uk; mihalis@imperial.ac.uk; i.panagakis@imperial.ac.uk,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,09.05 Gesture Analysis,,, ,Statistics and Learning,P 1B
1030,Relative Hidden Markov Models for Evaluating Motion Skills,This paper is concerned with a novel problem: learning temporal models using only relative information. Such a problem arises naturally in many applications involving motion or video data. Our focus in this paper is on video-based surgical training# in which a key task is to rate the performance of a trainee based on a video capturing his motion. Compared with the conventional method of relying on ratings from senior surgeons# an automatic approach to this problem is desirable for its potential lower cost# better objectiveness# and real-time availability. To this end# we propose a novel formulation termed {\it Relative Hidden Markov Model}  and develop an algorithm for obtaining a solution under this model. The proposed method utilizes only a relative ranking (based on an attribute of interest) between pairs of the inputs# which is easier to obtain and often more consistent# especially for the chosen application domain. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration is linked to the likelihood of the inputs under the learned model. Hence the model can be used to compare new sequences. Synthetic data are first used to systematically evaluate the model and the algorithm# and then we experiment with real data from a surgical training system. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video.,Qiang Zhang# Arizona State Univerisity; Baoxin Li*# ,qzhang53@asu.edu; baoxin.li@asu.edu,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,09.09 Video Analysis and Event Recognition,,, ,Statistics and Learning,P 1B
1666,A Fast Approximate AIB Algorithm for Distributional Word Clustering,Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation# compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegence# AIB has one main issue on its computational efficiency# especially when clustering a large number of words. Different from existing solutions to this issue# we analyze the characteristics of its objective function Â— the loss of mutual information# and show that by merely using the ratio of word-class joint probabilities of each word# good candidate word pairs for merging can be easily identified. Based on this finding# we propose a fast approximate AIB algorithm and show that that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.,Lei Wang*# University of Wollongong; Jianjia Zhang# University of Wollongong; Luping  Zhou# University of Wollongong; Wanqing Li# University of Wollongong,leiw@uow.edu.au; jz163@uowmail.edu.au; luping.zhou.jane@gmail.com; Wanqing@uow.edu.au,08.09 Statistical Methods and Learning*,08.09 Statistical Methods and Learning*,,,, ,Statistics and Learning,P 1B
399,Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback,"Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. ""I think this is a forest# what do you think?"". If the learner is wrong# the supervisor provides an explanation e.g. ""No# this is too open to be a forest"". With access to a pre-trained set of relative attribute predictors# the learner fetches all unlabeled images more open than the query image# and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work# we propose three improvements over this set-up. First# we incorporate a weighting scheme that instead of making a hard decision reasons about the likelihood of an image being a negative example. Second# we do away with pre-trained attributes and instead learn the attribute models on the fly# alleviating overhead and restrictions of a pre-determined attribute vocabulary. Finally# we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. We demonstrate significant improvement in classification accuracy on faces and shoes. We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories.",Arijit Biswas*# University of Maryland; Devi Parikh# TTIC,arijitbiswas87@gmail.com; dparikh@ttic.edu,07.01 Recognition*,07.01 Recognition*,07.02 Category recognition,07.07 Object Recognition,, ,Recognition,P 1C
859,Expanded Parts Model for Human Attribute and Action Recognition in Still Images,We propose a new model for recognizing human attributes (e.g. wearing a suit# sitting# short hair) and actions (e.g. running# riding a horse) in still images. The proposed model includes a collection of discriminative part templates which are learnt to explain specific discriminative scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models# which learn a few (i.e. a mixture of) average discriminative templates. To learn our model# we propose an algorithm which automatically mines out the parts and learns corresponding discriminative templates with their respective locations from a very large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7]# (ii) 27 Human Attributes (HAT) [25]# and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.,Gaurav Sharma*# INRIA UniCaen; Frederic Jurie# ; Cordelia Schmid# INRIA,gaurav.sharma@unicaen.fr; frederic.jurie@unicaen.fr; cordelia.schmid@inria.fr,07.01 Recognition*,07.01 Recognition*,07.02 Category recognition,,, ,Recognition,P 1C
1626,Multipath Sparse Coding Using Hierarchical Matching Pursuit,Complex real-world signals# such as images# contain discriminative structures that differ in many aspects including scale# invariance# and data channel. While progress in deep learning shows the importance of learning features through multiple layers# it is equally important to learn features through multiple paths. We propose Multipath Hierarchical Matching Pursuit (M-HMP)# a novel feature learning architecture that combines a collection of hierarchical sparse features for image classification to capture multiple aspects of discriminative structures. Our building block is MI-KSVD# a codebook learning algorithm that balances the reconstruction error and the mutual incoherence of the codebook# and batch orthogonal matching pursuit (OMP)# and we apply them recursively at varying layers and scales. The result is a highly discriminative image representation that leads to large improvements to the state-of-the-art on many standard benchmarks# e.g. Caltech-101# Caltech-256# MIT-Scenes# Oxford-IIIT Pet and Caltech-UCSD Bird-200.,Liefeng Bo*# Intel Labs; Xiaofeng Ren# ; Dieter Fox# University of Washington,lfb@cs.washington.edu; xren.cn@gmail.com; fox@cs.washington.edu,07.01 Recognition*,07.01 Recognition*,07.02 Category recognition,07.07 Object Recognition,08.08 Sparse coding and dictionaries, ,Recognition,P 1C
1319,Semi-Supervised Domain Adaptation with Instance Constraints,Most successful object classification and detection methods rely on classifiers trained on large labeled datasets.  However# for domains where labels are limited# simply borrowing labeled data from existing datasets can hurt performance# a phenomenon known as ``dataset bias.'' We propose a general framework for adapting classifiers from ``borrowed'' data to the target domain using a combination of available labeled and unlabeled examples. Specifically# we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences# e.g. when the same object or individual is observed simultaneously from multiple views# or tracked between video frames. In these cases# the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships# and demonstrate empirically that they improve recognition accuracy in two scenarios: multi-category image classification and object detection in video. ,Jeff Donahue*# UC Berkeley; Judy Hoffman# UC Berkeley; Erik Rodner# ; Kate Saenko# University of Massachusetts Lowell; Trevor Darrell# UC Berkeley,jdonahue@cs.berkeley.edu; jhoffman@eecs.berkeley.edu; Erik.Rodner@uni-jena.de; saenko@cs.uml.edu; trevor@eecs.berkeley.edu,07.01 Recognition*,07.01 Recognition*,07.06 Object Detection,07.07 Object Recognition,08.09 Statistical Methods and Learning, ,Recognition,P 1C
1230,Learning Structured Low-rank Representations for Image Classification,An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term# we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability# this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.,Yangmuzi Zhang*# University of Maryland; Zhuolin Jiang# University of Maryland; Larry Davis# ,ymzhang@umiacs.umd.edu; zhuolin@umiacs.umd.edu; lsd@umiacs.umd.edu,07.01 Recognition*,07.01 Recognition*,07.07 Object Recognition,08.08 Sparse coding and dictionaries,09.04 Face recognition, ,Recognition,P 1C
2218,Manifold Kernel Partial Least Squares for Lipreading and Speaker identification,Visual speech recognition is a challenging problem due to confusion between visual speech features.  The speaker identification problem is usually coupled with speech recognition. Moreover# speaker identification is important to several applications such as automatic access control# biometrics# authentication# and personal privacy issues. In this approach we propose a novel approach for lipreading and speaker identification. We propose a new approach for manifold parameterization in a low-dimensional latent space# where each manifold is represented as a point in that space. We initially parameterize each instance manifold using a nonlinear mapping from a unified manifold representation. We then factorize the parameter space using Kernel Partial Least Squares KPLS to achieve a low-dimension manifold latent space.  We use two-way projections to achieve two manifold latent spaces# one for the speech content and one for the speaker. We apply our approach on two public databases AvLetters and OuluVS.  We show the results for three different settings of lipreading speaker independent# speaker dependent# and speaker semi-dependent. Our approach outperforms for the speaker semi-dependent setting by at least $15\% $ of the baseline# and competes in the other two settings.,Amr Bakry*# Rutgers University; Ahmed Elgammal# ,amrbakry@cs.rutgers.edu; elgammal@cs.rutgers.edu,07.01 Recognition*,07.01 Recognition*,08.01 Learning# statistics# and inference,08.09 Statistical Methods and Learning,09.01 Applications: humans, ,Recognition,P 1C
734,Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation,Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain# which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation# where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further# we introduce a quantitative measure to characterize the shift between two domains# which enables us to select the optimal domain to adapt given multiple source domains. We present experiments on face recognition across pose# illumination and blur variations# cross dataset object recognition# and report improved performance over the state of the art.,Jie Ni*# University of Maryland; Qiu Qiang# ; Rama Chellappa# UMD,jni@umiacs.umd.edu; qiu@umiacs.umd.edu; rama@umiacs.umd.edu,07.01 Recognition*,07.01 Recognition*,08.08 Sparse coding and dictionaries,,, ,Recognition,P 1C
136,Graph-Based Discriminative Learning for Location Recognition,Recognizing the location of a query image by matching it to a database is an important problem in computer vision# and one for which the representation of the database is a key issue.  We explore new ways for exploiting the structure of a database by representing it as a graph# and show how the rich information embedded in a graph can improve a bag-of-words-based location recognition method.  In particular# starting from a graph on a set of images based on visual connectivity# we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques.  For a query image# each database image is ranked according to these local distance functions in order to place the image in the right part of the graph.  In addition# we propose a probabilistic method for increasing the diversity of these ranked database images# again based on the structure of the image graph.  We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.,Song Cao*# Cornell University; Noah Snavely# Cornell University,caosong@cs.cornell.edu; snavely@cs.cornell.edu,07.01 Recognition*,07.01 Recognition*,08.09 Statistical Methods and Learning,,, ,Recognition,P 1C
1671,Learning by Associating Ambiguously Labeled Images,We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance# in news image collections# each image contains some samples of interest (\emph{e.g.#} human faces)# and its associated caption has labels with the true ones included. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels# both within each image and across the image set. To this end# we propose a novel framework to address this problem. Our framework is motivated by the observation that samples of the same class repetitively appear in the training images# while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set# the matrix formed by each class of samples would be ideally low-rank. By leveraging such a low-rank assumption# we can simultaneously optimize a partial permutation matrix (PPM) for each image# which is formulated in order to exploit all information between samples and labels in a principled way. The so obtained PPMs can be readily used to assign estimated labels to samples in training images. Following it a standard SVM classifier can be trained and used in unseen data. Experiments on benchmark datasets show the efficacy of our proposed method.,Zinan Zeng*# ADSC# Singapore; Shijie Xiao# NTU# Singapore; Kui Jia# ADSC; Tsung-Han Chan# Advanced Digital Sciences Cent; Shenghua Gao# ADSC; Dong Xu# ,zeng.zinan@gmail.com; xiaoshijiehithonor@gmail.com; chris.jia@adsc.com.sg; th.chan@adsc.com.sg; shenghua.gao@adsc.com.sg; dongxu@ntu.edu.sg,07.01 Recognition*,07.01 Recognition*,09.04 Face recognition,,, ,Recognition,P 1C
634,HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences,We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus# they often fail to capture the complex joint shape-motion cues at pixel-level. In contrast# we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time# depth# and spatial coordinates. To build the histogram# we create 4D projectors# which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently# we refine the projectors using a discriminative density measure# such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments# we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence# and thus outperforms the state-of-the-art on all relevant benchmarks.,Omar Oreifej*# University of Central Florida; Zicheng Liu# ,oreifej@gmail.com; zliu@microsoft.com,07.01 Recognition*,07.01 Recognition*,09.05 Gesture Analysis,09.09 Video Analysis and Event Recognition,, ,Recognition,P 1C
708,3D R Transform on Spatio-Temporal Interest Points for Action Recognition,Spatio-temporal interest points serve as the elementary building blocks in many modern action recognition algorithms# and most of them exploit the local spatio-temporal volume features using a Bag of Visual Words (BOVW) representation. Such representation# however# ignores potentially valuable information about the global spatio-temporal distribution of interest points. In this paper# we propose a new global feature to capture the detailed geometrical distribution of interest points.  It is calculated by using the R transform which is defined as an extended 3D discrete Radon transform# followed by applying a two-directional two-dimensional principal component analysis. Such R feature captures the geometrical information of the interest points and keeps invariant to geometry transformation and robust to noise. In addition# we propose a new fusion strategy to combine the R feature with the BOVW representation for further improving recognition accuracy. We utilize a context-dependent fusion method to capture both the pairwise similarity and higher-order contextual interactions of the videos. Experimental results on several dataset demonstrate the effectiveness of the proposed approach for action recognition.,Chunfeng Yuan*# Institute of Automation; Xi Li# ; Weiming Hu# ; Haibin Ling# ; Stephen Maybank# ,cfyuan@nlpr.ia.ac.cn; lixichinanlpr@gmail.com; wmhu@nlpr.ia.ac.cn; hbling@temple.edu; sjmaybank@dcs.bbk.ac.uk,07.01 Recognition*,07.01 Recognition*,,,, ,Recognition,P 1C
1952,Learning Cross-domain Information Transfer for Location Recognition and Clustering,Estimating geographic location from images is a challenging problem that is receiving recent attention. In contrast to many existing methods that primarily model discriminative information corresponding to different locations# we propose joint learning of information that images across locations share and vary upon. Starting with generative and discriminative subspaces pertaining to domains# which are obtained by a hierarchical grouping of images from adjacent locations# we present a top-down approach that first models cross-domain information transfer by utilizing the geometry of these subspaces# and then encodes the model results onto individual images to infer their location. We report competitive results for location recognition and clustering on two public datasets# im2GPS and San Francisco# and empirically validate the utility of various design choices involved in the approach.,Raghuraman Gopalan*# AT&T Research,raghuram@research.att.com,07.01 Recognition*,07.01 Recognition*,,,, ,Recognition,P 1C
2106,Studying Relationships Between Human Gaze# Description# and Computer Vision,The record of natural user behavior when viewing an image#  particularly their eye movements or natural language descriptions# could provide enormous information about how people interpret and interact with images.  We design behavioral experiments to better understand the relationship between human subject descriptions# eye movements# and image content when freely viewing images. To explore these complex relationships we collect human behavioral data on images from two commonly used computer vision datasets.  Next# we propose research questions to relate human cues with visual content# and finally with current computational estimates of that content (deformable part model detection).,Kiwon Yun*# Stony Brook University; Yifan Peng# Stony Brook University; Tamara Berg# Stony Brook University; Greg Zelinsky# Stony Brook U; Dimitris Samaras# ,kyun@cs.stonybrook.edu; yipeng@cs.stonybrook.edu; tlberg@cs.sunysb.edu; gzelinsky@ms.cc.sunysb.edu; samaras@cs.sunysb.edu,07.01 Recognition*,07.01 Recognition*,,,, ,Recognition,P 1C
1229,BoF meets HOG: Feature Extraction based on Histograms of Oriented p.d.f Gradients for Image Classification,Image classification methods have been significantly developed in the last decade. Most methods stem from bag-of-features (BoF) approach and it is recently extended to a vector aggregation model# such as using Fisher kernels. In this paper# we propose a novel feature extraction method for image classification. Following the BoF approach# a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image# we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients# especially their orientations# effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the codes. The proposed image features# imposing no specific assumption on the targets# are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recognition and scene classification using various datasets# the proposed method exhibits superior performances compared to the other existing methods.,Takumi Kobayashi*# National Institute of AIST,takumi.kobayashi@aist.go.jp,07.02 Category recognition*,07.02 Category recognition*,02.03 Feature descriptors,07.01 Recognition,07.07 Object Recognition, ,Recognition,P 1C
1602,Class Generative Models based on Feature Regression for Pose Estimation of Object Categories,In this paper# we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models# ie# regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time# the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset and the Pointing'04 face dataset. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.,Michele Fenzi*# Inst. for Inform. Processing; Laura Leal-TaixÃ©# Leibniz University Hannover; Bodo Rosenhahn# ; JÃ¶rn Ostermann# Institute for Information Processing Hannover,fenzi@tnt.uni-hannover.de; leal@tnt.uni-hannover.de; rosenhahn@tnt.uni-hannover.de; ostermann@tnt.uni-hannover.de,07.02 Category recognition*,07.02 Category recognition*,02.03 Feature descriptors,07.01 Recognition,07.07 Object Recognition, ,Recognition,P 1C
1975,Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification,In this paper we propose a new technique for learning a discriminative codebook for local feature descriptors# specifically designed for scalable landmark classification. The key contribution lies in exploiting the knowledge of correspondences within sets of feature descriptors during codebook learning. Feature correspondences are obtained using structure from motion (SfM) computation on Internet photo collections which serve as the training data. Our codebook is defined by a random forest that is trained to map corresponding feature descriptors into identical codes. Unlike prior forest-based codebook learning methods# we utilize fine-grained descriptor labels and address the challenge of training a forest with an extremely large number of labels. Our codebook is used with various existing feature encoding schemes and also a variant we propose for importance-weighted aggregation of local features. We evaluate our approach on a public dataset of 25 landmarks and our new dataset of 620 landmarks (614K images). Our approach significantly outperforms the state of the art in landmark classification. Furthermore# our method is memory efficient and scalable.,Alessandro Bergamo*# Dartmouth College; Sudipta Sinha# ; Lorenzo Torresani# ,aleb@cs.dartmouth.edu; sudipta.sinha@microsoft.com; lorenzo@cs.dartmouth.edu,07.02 Category recognition*,07.02 Category recognition*,05.10 Structure from Motion,07.05 Instance recognition,08.08 Sparse coding and dictionaries, ,Recognition,P 1C
336,Designing Category-Level Attributes for Discriminative Visual Recognition,"Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However# human efforts are usually involved in the attribute designing process# making the representation costly to obtain. In this paper# we propose a novel formulation to automatically design discriminative ""category-level attributes""# which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability# learnability) in a principled and elegant way. The designed attributes can be used for tasks of cross-category knowledge transfer# achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). The unique capability of zero-shot learning and providing human interpretable cues for visualizing the machine reasoning process# differentiate the proposed methodology from recent approaches of automatic attribute representation. ",Felix Yu*# Columbia University; Liangliang Cao# IBM; Rogerio Feris# ; Shih-Fu Chang# Columbia University; John Smith# IBM Thomas J. Watson Research Center,yuxinnan@ee.columbia.edu; liangliang.cao@us.ibm.com; rsferis@us.ibm.com; sfchang@ee.columbia.edu; jsmith@us.ibm.com,07.02 Category recognition*,07.02 Category recognition*,07.01 Recognition,07.04 Image and Video Retrieval,, ,Recognition,P 1C
1592,Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop,"Recent work in computer vision has addressed zero-shot learning or unseen class detection# which involves categorizing objects without observing any training examples.  However# these problems assume that attributes or defining characteristics of these unobserved classes are known# leveraging this information at test time to detect an unseen class.  We address the more realistic problem of detecting categories that do not appear in the dataset in any form.  We denote such a category as an unfamiliar class; it is neither observed at train time# nor do we possess any knowledge regarding its relationships to attributes.  This problem is one that has received limited attention within the computer vision community.  In this work# we propose a novel approach to the unfamiliar class detection task that builds on attribute-based classification methods# and we empirically demonstrate how accuracy is impacted by attribute noise and dataset ""difficulty#"" as quantified by the separation of classes in the attribute space.  We also present a method for incorporating human users to overcome deficiencies in attribute detection and demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.",Catherine Wah*# UCSD; Serge Belongie# UCSD,cwah@cs.ucsd.edu; sjb@cs.ucsd.edu,07.02 Category recognition*,07.02 Category recognition*,07.01 Recognition,08.09 Statistical Methods and Learning,, ,Recognition,P 1C
2108,Object-Centric Anomaly Detection by Attribute-Based Reasoning,When describing images# humans tend not to talk about the obvious# but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and# consequently# meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.,Babak Saleh*# Rutgers University; Ali Farhadi# CMU; Ahmed Elgammal# ,babak.saleh@gmail.com; afarhadi@cs.cmu.edu; elgammal@cs.rutgers.edu,07.02 Category recognition*,07.02 Category recognition*,07.01 Recognition,07.05 Instance recognition,07.06 Object Detection, ,Recognition,P 1C
545,Learning Class-to-Image Distance with Object Matchings,We conduct image classification by learning a class-to-image distance function that matches objects. The set of objects in training images for an image class are treated as a collage. When presented with a test image# the best matching between this collage of training image objects and those in the test image is found. We validate the efficacy of the proposed model on the PASCAL 07 and SUN 09 datasets# showing that our model is effective for object classification and scene classification tasks. State-of-the-art image classification results are obtained# and qualitative results demonstrate that objects can be accurately matched.,Guang-Tong Zhou*# Simon Fraser University; Tian Lan# Simon Fraser University; Weilong Yang# ; Greg Mori# ,gza11@sfu.ca; taran.lan1986@gmail.com; wya16@sfu.ca; mori@cs.sfu.ca,07.02 Category recognition*,07.02 Category recognition*,07.03 Context and scene understanding,"08.01 Learning# statistics# and inference""",, ,Recognition,P 1C
1009,Sample Specific Late Fusion for Visual Category Recognition,Late fusion addresses the problem of combining the prediction scores of multiple classifiers# in which each classifier is trained with a specific feature or model. However# the existing methods generally use a fixed fusion weight for a classifier over all samples# and ignores the fact that each classifier may perform better or worse for certain subsets of the images (e.g.# images of different views or different qualities). In this paper# we propose a sample specific late fusion method to address this issue. Specifically# we cast the problem into an information propagation process which propagates the fusion weights learned on the labeled samples to the individual unlabeled samples# while enforcing the positive samples have higher fusion scores than the negative samples. Upon this process# we identify the optimal fusion weights for each sample and meanwhile push the positive samples toward the top positions in the fusion score rank list. We formulate our problem as a $\ell_{\infty}$ norm constrained optimization problem and apply the Alternating Direction Method of Multipliers (ADMM) for the optimization. Extensive experiment results on various visual categorization tasks show that the proposed method consistently and significantly beats the state-of-the-art late fusion methods. To the best knowledge# this is the first method supporting sample specific fusion weight learning.,Dong Liu*# Columbia University; Kuan-Ting Lai# National Taiwan University ; Guangnan Ye# Columbia University ; Ming-Syan Chen# National Taiwan University; Shih-Fu Chang# Columbia University,dongliu@ee.columbia.edu; kuantinglai@gmail.com; gy2179@columbia.edu; mschen@citi.sinica.edu.tw; sfchang@ee.columbia.edu,07.02 Category recognition*,07.02 Category recognition*,07.04 Image and Video Retrieval,08.09 Statistical Methods and Learning,, ,Recognition,P 1C
283,Efficient Object Detection and Segmentation for Fine-Grained Recognition,We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. As a result# we can  effectively `zoom in' on the object# i.e. center it# normalize it for scale# and discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm  leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition# e.g. birds species.  The proposed algorithm is much more efficient than other known methods in similar scenarios. Our method is also simpler and# unlike previous works which focus on a single class type# we apply it here to different classes of objects# e.g. birds# flowers# cats and dogs.  We tested the algorithm on a number of benchmark datasets# including the Oxford 102 Flowers# the Caltech-UCSD 200 Birds dataset# and the Oxford Cats and Dogs dataset. It outperforms all the known state-of-the-art methods on these datasets# sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%# consistently on all datasets. We also observed more than  a 4% improvement in recognition performance on a challenging large-scale flower dataset# containing 578 species of flowers and 250#000 images.,Anelia Angelova*# NEC Labs America; Shenghuo Zhu# NEC Labs America,anelia@nec-labs.com; anelia.angelova@gmail.com,07.02 Category recognition*,07.02 Category recognition*,07.06 Object Detection,07.07 Object Recognition,, ,Recognition,P 1C
395,Label-Embedding for Attribute-Based Classification,Attributes are an intermediate representation which enables sharing parameters between classes# a must when training data is scarce. We propose to cast the problem of attribute-based image classification as one of label embedding: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that# given an image# the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.,Zeynep Akata# XRCE; Florent Perronnin*# ; Zaid Harchaoui# INRIA; Cordelia Schmid# INRIA,zeynep.akata@xrce.xerox.com; florent.perronnin@xrce.xerox.com; zaid.harchaoui@inria.fr; cordelia.schmid@inria.fr,07.02 Category recognition*,07.02 Category recognition*,07.07 Object Recognition,,, ,Recognition,P 1C
766,Subcategory-aware Object Classification,In this paper# we introduce a subcategory-aware object classification framework to boost category level object classification performance. Motivated by the observation of considerable intra-class diversities and inter-class ambiguities in many current object classification datasets# we explicitly split data into subcategories by ambiguity guided subcategory mining. We then train an individual model for each subcategory rather than attempt to represent an object category with a monolithic model. More specifically# we build the instance affinity graph by combining both intra-class similarity and inter-class ambiguity. Visual subcategories# which correspond to the dense subgraphs# are detected by the graph shift algorithm and seamlessly integrated into the state-of-the-art detection assisted classification framework. Finally the responses from subcategory models are aggregated by subcategory-aware kernel regression. The extensive experiments over the PASCAL VOC 2007 and PASCAL VOC 2010 databases show the state-of-the-art performance from our framework.,Jian Dong*# NUS; Qiang Chen# NUS; Jiashi Feng# NUS; Wei Xia# NUS; Zhongyang Huang# PSL; Shuicheng YAN# NUS,a0068947@nus.edu.sg; chenqiang@nus.edu.sg; jiashi@nus.edu.sg; weixia@nus.edu.sg; ZhongYang.Huang@sg.panasonic.com; eleyans@nus.edu.sg,07.02 Category recognition*,07.02 Category recognition*,07.07 Object Recognition,,, ,Recognition,P 1C
1326,Vantage Feature Frames For Fine-Grained Categorization,We study fine-grained categorization# the task of distinguishing among (sub)categories of the same generic object class (e.g.# birds)# focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points# which is the approach taken by botanists# but using features dedicated the individual categories. Our implementation of the strategy is based on vantage feature frames# a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependent features computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category)# and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms# in particular those using more distributed representations# on standard databases of leaves.,ASMA REJEB SFAR*# INRIA; NOZHA BOUJEMAA# INRIA; DONALD GEMAN# JHU,asma.rejeb_sfar@inria.fr; nozha.boujemaa@inria.fr; geman@jhu.edu,07.02 Category recognition*,07.02 Category recognition*,07.07 Object Recognition,08.09 Statistical Methods and Learning,10.07 Additional applications, ,Recognition,P 1C
1232,Probabilistic Label Trees for Efficient Large Scale Image Classification,Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows.  The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically.  In this paper# we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy. ,Baoyuan Liu*# UCF; Fereshteh Sadeghi# UCF; Marshall Tappen# University of Central Florida; Ohad Shamir# Microsoft Research New England; Ce Liu# Microsoft Research New England,bliu@eecs.ucf.edu; fsadeghi@eecs.ucf.edu; mtappen@cs.ucf.edu; ohadsh@microsoft.com; celiu@microsoft.com,07.02 Category recognition*,07.02 Category recognition*,"08.01 Learning# statistics# and inference""",,, ,Recognition,P 1C
1520,From Words to Visual Concepts: Harvesting Mid-level Knowledge from Large-scale Internet Images,Obtaining effective mid-level representations has become increasingly important in computer vision. In this paper# we propose a fully automatic algorithm which learns visual concepts by harvesting a large number of Internet images (about half of million) using word-based queries. Existing approaches either rely on manual annotations or only learn image-level classifiers. Here# we take the advantage of having a large amount of well organized Google and Bing image data; visual concepts are automatically exploited based on word-based queries. Around 14# 000 visual concepts are learned from half a million images. Using the learned visual concepts# we show state-of-the-art performances on a variety of widely used benchmark datasets# which demonstrates the effectiveness of learned mid-level representations: being able to generalize well to general natural images. Our method shows significant improvement over the competing systems for image classification# including those using fully supervised data.,Quannan Li# Lab of Neuro Imaging# UCLA; Jiajun Wu# Tsinghua University; Zhuowen Tu*# ,quannan.li@gmail.com; jiajunwu.cs@gmail.com; zhuowen.tu@gmail.com,07.02 Category recognition*,07.02 Category recognition*,10.01 Aerial and outdoor image analysis and modeling,,, ,Recognition,P 1C
1548,Adaptive Active Learning for Image Classification,Recently active learning has attracted a lot of attention  in computer vision field# as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt the most uncertainty measure as the instance selection criterion. Although the most uncertainty query selection strategy is very effective  in many circumstances# it fails to take the information in the large amount of unlabeled instances into account and is prone to querying outliers. In this paper# we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision# object recognition and scene recognition# demonstrate the efficacy of the proposed  approach. ,Xin Li# Temple University; Yuhong Guo*# Temple University,xinli@temple.edu; yuhong@temple.edu,07.02 Category recognition*,07.02 Category recognition*,10.07 Additional applications,,, ,Recognition,P 1C
838,Cascaded and Supervised Laplacian Eigenmaps for Visual Object Category Recognition Based on Nearest Neighbors,Recognizing the category of a visual object remains a challenging computer vision problem. In this paper we develop a novel deep learning method that facilitates example-based visual object category recognition. Our deep learning architecture consists of multiple stacked layers and computes an intermediate representation that can be fed to a nearest-neighbor classifier. This intermediate representation is discriminative and structure-preserving. It is also capable of extracting essential characteristics shared by objects in the same category while filtering out nonessential differences among them. Each layer in our model is a nonlinear mapping# whose parameters are learned through two sequential steps that are designed to achieve the aforementioned properties. The first step computes a discrete mapping called supervised Laplacian eigenmap. The second step computes a continuous mapping from the discrete version through nonlinear regression. We have extensively tested our method and it achieves state-of-the-art recognition rates on a number of benchmark datasets.,Ruobing Wu# The University of Hong Kong; Yizhou Yu*# HKU/UIUC; Wenping Wang# HKU,rbwu@cs.hku.hk; yizhouy@acm.org; wenping@cs.hku.hk,07.02 Category recognition*,07.02 Category recognition*,,,, ,Recognition,P 1C
1541,Adopting Unseen Examples to a Category by Learned Attributes,We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition# we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool# and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset# ImageNet. ,Jonghyun Choi*# University of Maryland; Mohammad Rastegari# University of Maryland; Ali Farhadi# CMU; Larry Davis# ,jhchoi@umd.edu; mrastega@cs.umd.edu; afarhadi@cs.cmu.edu; lsd@umiacs.umd.edu,07.02 Category recognition*,07.02 Category recognition*,,,, ,Recognition,P 1C
1449,Visual Place Recognition with Repetitive Structures,Repeated structures such as building facades# fences or road markings   often   represent  a   significant   challenge  for   place recognition.   Firstly# repeated structures  are notoriously  hard for establishing correspondences using  multi-view geometry. Secondly# and even more  importantly# they violate the  feature independence assumed in  the  bag-of-visual-words   representation  which  often  leads  to over-counting  evidence  and   significant  degradation  of  retrieval performance.  In this work we  show that repeated structures are not a nuisance  but when  appropriately represented  they form  an important distinguishing feature  for many places. We  describe a representation of  repeated structures#  suitable  for scalable  retrieval# based  on robust  detection of  repeated structures  in the  image and  a simple modification  of  weights  in  the  bag-of-visual-word  model.   Place recognition results are shown on  a dataset of Street-View images from Pittsburgh demonstrating significant  gains in recognition performance compared to the standard bag-of-visual-words baseline and the recently proposed  burstiness   weights [12]#    which   explicitly downweight the effect of repeated structures.,Akihiko Torii*# Tokyo Institute of Technology; Josef Sivic# INRIA; Tomas Pajdla# CTU Prague; Masatoshi Okutomi# Tokyo Institute of Technology,torii@ctrl.titech.ac.jp; Josef.Sivic@ens.fr; pajdla@cmp.felk.cvut.cz; mxo@ctrl.titech.ac.jp,07.05 Instance recognition*,07.05 Instance recognition*,02.01 Feature extraction and matching,,, ,Recognition,P 1C
162,Cross-view Image Geolocalization,The recent availability of large amounts of geotagged imagery has inspired a number of data driven solutions to the image geolocalization problem. Existing approaches predict the location of a query image by matching it to a database of georeferenced photographs. While there are many geotagged images available on photo sharing and street view sites# most are clustered around landmarks and urban areas. The vast majority of the Earth's land area has no ground level reference photos available# which limits the applicability of all existing image geolocalization methods. On the other hand# there is no shortage of visual and geographic data which densely covers the Earth -- we examine overhead imagery and land cover survey data -- but the relationship between this data and ground level query photographs is complex. In this paper# we introduce a cross-view feature translation approach to greatly extend the reach of image geolocalization methods. We can often localize a query even if it has no corresponding ground-level images in the database.  A key idea is to learn the relationship between ground level appearance and overhead appearance and land cover attributes from sparsely available geotagged ground-level images. We perform experiments over a 1600 km^2 region containing a variety of scenes and land cover types. For each query# our algorithm produces a probability density over the region of interest. ,Tsung-Yi Lin*# University of California# San ; Serge Belongie# UCSD; James Hays# Brown University,tsl008@ucsd.edu; sjb@cs.ucsd.edu; hays@cs.brown.edu,07.05 Instance recognition*,07.05 Instance recognition*,02.05 Feature matching and indexing,07.01 Recognition,07.03 Context and scene understanding, ,Recognition,P 1C
1815,Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition,3D model-based object recognition has been a noticeable research trend in recent years. Common methods identify 2D-to-3D correspondences and make recognition decisions by RANSAC-based pose estimation# whose efficiency usually suffers from inaccurate correspondences caused by the increasing number of target objects for recognition. To overcome this scalability bottleneck# in this paper we propose an efficient 2D-to-3D correspondence filtering approach which combines a light-weight neighborhood-based step with a finer-grained pairwise step to remove spurious correspondences by 2D/3D geometric cues. On a dataset of 300 3D objects# the proposed solution achieves $\sim$10 times speed improvement over the baseline# with a comparable recognition accuracy. A non-GPU implementation achieves a speed of $\sim$3fps for 1280$\times$720 query images.,Qiang Hao*# Tianjin University; Rui Cai# Microsoft Research Asia; Zhiwei Li# Microsoft; Lei Zhang# Microsoft Research Asia; Yanwei Pang# Tianjin University; feng Wu# ; Yong Rui# Microsoft,haoq@live.com; ruicai@microsoft.com; zli@microsoft.com; leizhang@microsoft.com; pyw@tju.edu.cn; fengwu@microsoft.com; yongrui@microsoft.com,07.05 Instance recognition*,07.05 Instance recognition*,07.07 Object Recognition,,, ,Recognition,P 1C
2069,Learning and calibrating per-location classifiers for visual place recognition,The goal of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint# imaging conditions and the large size of the image database. The contribution of this work is two-fold. First# we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second# as only few positive training examples are available for each location# we develop a ranking based procedure to calibrate the outputs of the large number of individual per-location classifiers without the need for additional positive training data. Experiments are performed on a database of 25#000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work.,Petr GronÃ¡t*# INRIA; Josef Sivic# INRIA; Guillaume Obozinski# INRIA/ENS; Tomas Pajdla# CTU Prague,petr.gronat@inria.fr; Josef.Sivic@ens.fr; guillaume.obozinski@ens.fr; pajdla@cmp.felk.cvut.cz,07.05 Instance recognition*,07.05 Instance recognition*,08.09 Statistical Methods and Learning,,, ,Recognition,P 1C
1348,An approach to pose-based action recognition,We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely# we obtain the $K$-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the ``best'' one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations of body parts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable# compact# and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport# the Keck Gesture and the MSR-Action3D datasets.,CHUNYU WANG# Peking University; Yizhou Wang*# Peking University ; Alan Yuille# UCLA,wangchunyu@pku.edu.cn; yizhou.wang@pku.edu.cn; yuille@stat.ucla.edu,07.08 Part-based recognition*,07.08 Part-based recognition*,07.01 Recognition,09.01 Applications: humans,09.05 Gesture Analysis, ,Recognition,P 1C
1086,Blocks that Shout: Distinctive Parts for Scene Classification ,The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper# we propose a simple# efficient# and effective method to do so. We address this problem by learning parts gradually# starting from a single part occurrence with an Exemplar SVM. In this manner# additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a mean of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates.  We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost# compared to previous part learning methods such as Singh et al. We also show that a well constructed bag-of-word model can significantly outperform the state of the art performance on this data. ,Mayank Juneja*# IIIT-Hyderabad; Andrea Vedaldi# ; CV Jawahar# IIIT Hyderabad; Andrew Zisserman# Oxford,juneja@students.iiit.ac.in; vedaldi@robots.ox.ac.uk; jawahar@iiit.ac.in; az@robots.ox.ac.uk,07.08 Part-based recognition*,07.08 Part-based recognition*,07.02 Category recognition,07.03 Context and scene understanding,, ,Recognition,P 1C
1973,Part Discovery from Partial Correspondence,We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings# our approach can be used to discover parts that are hard to name# but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from pointwise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings# and show the utlity of the rich part library learned in this way for three tasks: object detection# category-specific saliency estimation# and fine-grained visual search. We perform extensive experiments and compare our method to current state of the art methods on these  tasks.,Subhransu Maji*# Toyota Technology Institute# at Chicago; Gregory Shakhnarovich# TTIC,smaji@ttic.edu; greg@ttic.edu,07.08 Part-based recognition*,07.08 Part-based recognition*,07.02 Category recognition,07.07 Object Recognition,, ,Recognition,P 1C
416,Learning Collections of Part Models for Object Recognition,We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations.  Part detectors can be trained and applied individually# which simplifies learning and extension to new features or categories.  We apply the parts to object category detection# pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring.  On PASCAL VOC 2010# we evaluate the part detectors' ability to discriminate and localize annotated keypoints.  Our detection system is competitive with the best-existing systems# outperforming other HOG-based detectors on the more deformable categories.,Ian Endres*# UIUC; Kevin Shih# University of Illinois; Johnston Jiaa# ; Derek Hoiem# UIUC,iendres2@uiuc.edu; kjshih2@illinois.edu; jiaa1@illinois.edu; dhoiem@illinois.edu,07.08 Part-based recognition*,07.08 Part-based recognition*,07.06 Object Detection,07.07 Object Recognition,, ,Recognition,P 1C
1194,Fast multiple-part based object detection using KD-Ferns,In this work we present a new part-based object detection algorithm with hundreds of parts performing real-time detection. Part-based models are currently state-of-the-art for object detection due to their ability to represent large appearance variations. However# due to their high computational demands such methods are limited to several parts only and are too slow for practical real-time implementation. Our algorithm is an accelerated version of the ``Feature Synthesis'' (FS) method [1]# which uses multiple object parts for detection and is among state-of-the-art methods on human detection benchmarks# but also ssuffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses several strategies for reducing the number of locations searched for each part. The first strategy uses a novel algorithm for approximate nearest neighbor search which we developed# termed ``KD-Ferns''# to compare each image location to only a subset of the model parts. Candidate part locations for a specific part are further reduced using spatial inhibition# and using an object-level ``coarse-to-fine'' strategy. In our empirical evaluation on pedestrian detection benchmarks# AFS maintains almost fully the accuracy performance of the original FS# while running more than X4 faster than existing part-based methods which use only several parts. AFS is to our best knowledge the first part-based object detection method achieving real-time running performance: nearly 10 frames per-second on 640X480 images on a regular CPU.,Dan Levi*# General Motors Research; Shai Silberstein# ; Aharon Bar-Hillel# GM,danmlevi@gmail.com; SHAI4998@netvision.net.il; aharon.barhillel@gm.com,07.08 Part-based recognition*,07.08 Part-based recognition*,07.06 Object Detection,09.07 Person detection and tracking,, ,Recognition,P 1C
107,Part-based Discriminators for Recognition and Verification,"From a set of images in a particular domain# labeled with part locations and class# we present a method to automatically learn a set of highly discriminative intermediate features that we call ""part-based discriminators."" Unlike previous work on feature learning# each of these discriminators takes a form specialized to re?ect discriminative information that is local to one of the parts. We demonstrate the usefulness of these discriminators with new state-of-the-art results on bird species identi?cation using the Caltech UCSD Birds dataset# and parity with the best existing results in face veri?cation on the Labeled Faces in the Wild (LFW) dataset. Finally# we also demonstrate the particular advantage of the discriminators when training data is scarce.  ",Thomas Berg*# ; Peter Belhumeur# ,tberg@cs.columbia.edu; belhumeur@cs.columbia.edu,07.08 Part-based recognition*,07.08 Part-based recognition*,09.04 Face recognition,,, ,Recognition,P 1C
739,Non-Parametric Filtering for Geometric Detail Extraction and Material Representation,Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling# but not accounted for in current intrinsic image works. In this work# we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as  albedo * (coarse-scale shading + shading detail). Our decomposition offers quantitative improvement in albedo recovery and in material classification.Our method also enables interesting image editing activities# including bump removal# geometric detail smoothing/enhancement and material transfer.,Zicheng Liao*# University of Illinois; Jason Rock# UIUC; David Forsyth# UIUC,liao17@illinois.edu; jjrock2@illinois.edu; daf@illinois.edu,01.01 Image processing*,01.01 Image processing*,01.03 Computational Photography and Video,01.08 Image enhancement# restoration# and denoising,01.13 Texture analysis and synthesis,Intrinsic images,Imaging,P 1D
1275,Learning the Change for Automatic Image Cropping,Image cropping is a common operation used to improve the visual quality of photographs. In this paper# we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content# and enhancement of overall composition. Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. In contrast to the many methods that exist for general assessment of image quality# ours specifically examines differences between the original and cropped photo in solving for the crop parameters. To this end# several novel image features are proposed to model the changes in image content and composition when a crop is applied. Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images.,Jianzhou Yan*# Chinese University of HK; Stephen Lin# ; Sing Bing Kang# Microsoft Research; Xiaoou Tang# ,yjz011@ie.cuhk.edu.hk; stevelin@microsoft.com; sbkang@microsoft.com; xtang@ie.cuhk.edu.hk,01.01 Image processing*,01.01 Image processing*,"01.08 Image enhancement# restoration# and denoising""",,,Cropping,Imaging,P 1D
1460,Statistical Textural Distinctiveness for Salient Region Detection in Natural Images,A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed.  Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image.  Based on the learnt sparse texture model# a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs.  Finally# the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms# their respective statistical textural distinctiveness based on the constructed graphical model# and general visual attentive constraints.  Experimental results using natural image data-sets and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.,Christian Scharfenberger*# University of Waterloo; Alexander Wong# University of Waterloo; Khalil Fergani# Unversity of Waterloo; David Clausi# ; John Zelek# ,c.n.scharfenberger@gmail.com; alex.s.wong@gmail.com; khalil.fergani@gmail.com; dclausi@uwaterloo.ca; jzelek@uwaterloo.ca,01.01 Image processing*,01.01 Image processing*,01.13 Texture analysis and synthesis,02.03 Feature descriptors,,Saliency# texture,Imaging,P 1D
1430,Real-time No-Reference Image Quality Assessment based on Filter Learning,This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal of developing a real-time# cross-domain model that can predict the quality of distorted images without any prior knowledge of non-distorted reference images and types of distortions present in these images. The contribution of our work is two-fold: first# the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems# therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second# the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception# yet# in other image domains# the final receiver of a digital image may not be a human.   The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regression model training independently# we propose a supervised method based on back-projection# which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters# the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained state-of-the-art results.,Peng Ye*# University of Maryland; Jayant Kumar# University of Maryland; Le Kang# UMD; David Doermann# University of Maryland,pengye@umiacs.umd.edu; jayant@umiacs.umd.edu; lekang@umiacs.umd.edu; doermann@umiacs.umd.edu,01.01 Image processing*,01.01 Image processing*,02.03 Feature descriptors,10.02 Document Analysis,10.07 Additional applications,Image quality,Imaging,P 1D
187,Learning without Human Scores for Blind Image Quality Assessment,General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in the fields of image processing# vision and machine learning. State of-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However# these methods need a large number of human scored images for training# and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches# and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image# and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning# but also has merits such as high linearity to human perception of image quality# real-time implementation and availability of image local quality map.,Wufeng Xue# Xi'an Jiaotong University; Lei Zhang*# The Hong Kong Polytechnic University; Xuanqin Mou# Xi'an Jiaotong University,xwolfs@hotmail.com; cslzhang@comp.polyu.edu.hk; xqmou@mail.xjtu.edu.cn,01.01 Image processing*,01.01 Image processing*,05.04 Image-based Modeling,08.09 Statistical Methods and Learning,,Image quality,Imaging,P 1D
89,The Variational Structure of Disparity and Regularization of 4D Light Fields,Unlike traditional images which do not offer information for different directions of incident light# a light field is defined on ray space# and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work# we analyze regularization of light fields in variational frameworks# and show that their variational structure is induced by disparity# which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore# we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way# we derive an efficient variational framework with convex priors# which can serve as a fundament for a large class of inverse problems on ray space. ,Bastian Goldluecke*# HCI; Sven Wanner# HCI,bastian.goldluecke@iwr.uni-heidelberg.de; sven.wanner@iwr.uni-heidelberg.de,01.03 Computational Photography and Video*,01.03 Computational Photography and Video*,01.01 Image processing,08.06 Optimization Methods,,Lightfield denoising,Imaging,P 1D
821,Globally Consistent Multi-Label Assignment on the Ray Space of 4D Light Fields,We present the first variational framework for multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images# features need to be extracted from the 2D projection of a three-dimensional scene. The associated loss of geometry information can cause severe problems# for example if different objects have a very similar visual appearance. In this work# we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems# but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution# we make all light fields available online with complete depth and segmentation ground truth data where available# and thus establish the first benchmark dataset for light field analysis to facilitate competitive further development of algorithms. ,Sven Wanner*# HCI; Christoph Straehle# Uni Heidelberg; Bastian Goldluecke# HCI,sven.wanner@iwr.uni-heidelberg.de; christoph.straehle@iwr.uni-heidelberg.de; bastian.goldluecke@iwr.uni-heidelberg.de,01.03 Computational Photography and Video*,01.03 Computational Photography and Video*,03.03 Image segmentation,08.06 Optimization Methods,,Lightfield segmentation# goes with 89,Imaging,P 1D
954,Principal observation ray calibration for tiled-lens-array integral imaging display,Integral imaging display (IID) is a promising technology to provide realistic 3D image without any glasses. To achieve a large display screen with a reasonable fabrication cost# a potential solution is a titled-lens-array IID (TLA-IID). However# TLA-IIDs are subject to 3D image artifacts when there are misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses and including them in a ray model used for rendering 3D image. Since the lens arrays are transparent and they change light propagation directions# this task is challenging for traditional calibration methods. In this paper we proposes a calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the cameraÂ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured pixels. Experiments show that lens array misalignments can be estimated with a standard deviation smaller than 50 um. Based on this# 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.,Weiming Li*# Samsung Adv. Inst. of Tech.; Mingcai Zhou# Samsung Adv. Inst. of Tech.; Shandong Wang# Samsung Adv. Inst. of Tech.; Haitao Wang# SAIT China,weiming.li@samsung.com; mingcai.zhou@samsung.com; sd.wang@samsung.com; ht.wang@samsung.com,01.03 Computational Photography and Video*,01.03 Computational Photography and Video*,04.03 Image stitching,10.06 Vision for Graphics,, ,Imaging,P 1D
1458,Decoding# Calibration and Rectification for Lenselet-Based Plenoptic Cameras,Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding# calibration and rectification procedure for lenselet-based plenoptic cameras appropriate for a range of computer vision applications.  We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space.  We further propose a five-parameter radial distortion model and a practical objective function based on ray reprojection.  Our model is of much lower dimensionality than camera array models# and more closely represents the physics of lenselet-based cameras.  Results include calibration of a commercially available camera using three calibration grid sizes over five datasets.  Typical RMS ray reprojection errors are 0.0628# 0.105 and 0.363 mm for 3.61# 7.22 and 35.1 mm calibration grids# respectively.  Rectification examples include calibration targets and real-world imagery.,Donald Dansereau*# ACFR; Oscar Pizarro# ACFR; Stefan Williams# University of Sydney,d.dansereau@acfr.usyd.edu.au; o.pizarro@acfr.usyd.edu.au; s.williams@acfr.usyd.edu.au,01.03 Computational Photography and Video*,01.03 Computational Photography and Video*,05.03 Calibration and pose estimation,,, ,Imaging,P 1D
1663,Adherent Raindrop Detection and Removal in Video,Raindrops appeared on windscreens or window glass can significantly degrade the visibility of the outside scenes. Being able to detect and remove raindrops will# therefore# benefit many computer vision applications# particularly outdoor surveillance systems and intelligent vehicle systems. In this paper# we introduce a method to detect and remove adherent raindrops automatically. We model the appearance of clear and blurred raindrops from its derivative properties. Based on it# we show that the motion inside raindrops# and the temporal derivatives of raindrop pixels are significantly different compared to the other areas# enabling us to distinguish them from the others. By further extending the analysis of blurred raindrops# we propose a raindrop removal method that solves a blending function with the clues from detection and intensity change in a few consecutive frames. Experimental results using various real videos show the effectiveness of our method.,Shaodi You*# The University of Tokyo; Rei Kawakami# ; Robby Tan# ; Katsushi Ikeuchi# Univerity of Tokyo,yousd@cvl.iis.u-tokyo.ac.jp; rei@cvl.iis.u-tokyo.ac.jp; tanrobby@gmail.com; ki@cvl.iis.u-tokyo.ac.jp,01.03 Computational Photography and Video*,01.03 Computational Photography and Video*,06.01 Illumination and Reflectance Modeling,10.01 Aerial and outdoor image analysis and modeling,, ,Imaging,P 1D
229,Stochastic Deconvolution,We present a novel stochastic framework for non-blind deconvolution based on point samples obtained from random walks. Unlike previous methods that must be tailored to specific regularization strategies# the new Stochastic Deconvolution method allows arbitrary priors# including non-convex and data-dependent regularizers to be introduced and tested with little effort. Stochastic Deconvolution is straightforward to implement# produces state-of-the-art results and directly leads to a natural boundary condition for image boundaries and saturated pixels.,James Gregson*# University of British Columbia; Felix Heide# University of British Columbia; Mushfiqur Rouf# University of British Columbia; Matthias Hullin# University of British Columbia; Wolfgang Heidrich# University of British Columbia,jgregson@cs.ubc.ca; fheide@cs.ubc.ca; nasarouf@cs.ubc.ca; hullin@cs.ubc.ca; heidrich@cs.ubc.ca,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,01.01 Image processing,,, ,Imaging,P 1D
482,Multi-Image Blind Deblurring via Shared-Latent Sparse Modeling,A multi-image blind deblurring approach using a shared latent sparse modeling of natural images is proposed in this paper. As opposed to previous approaches# our method ties the different observations together via a shared latent sparse representation and a special penalty function# coupling the image# blur kernels and noise levels together# thus enabling the estimation for all of them and leading to better restoration results. The coupled penalty function enjoys some nice properties such as shape-adaptiveness to blur and noise-level# which is in clear contrast with previous approaches.  The proposed method can recover a high quality image from a set of observations containing potentially both blurry and noisy images# without knowing \emph{a priori} the degradation type of each observation. Experimental results  on both synthetic and real-world test images compared with \emph{state-of-the-art} methods clearly demonstrate the superiority of the proposed method.,Haichao Zhang*# Northwestern Polytechnical Uni; David Wipf# ; Yanning Zhang# Northwestern Polytechnical University,hczhang1@gmail.com; davidwip@microsoft.com; ynzhang@nwpu.edu.cn,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,01.01 Image processing,"01.08 Image enhancement# restoration# and denoising""",, ,Imaging,P 1D
1038,Fast Image Super-resolution Based on In-place Example Regression,We propose a fast regression model for practical single image super-resolution based on in-place examples# by leveraging two fundamental super-resolution approaches---learning from an external database and learning from self-examples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples# a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and real-world images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details# while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore# our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications# where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts. ,"Jianchao Yang*# Adobe; Zhe Lin# """"""""""""""Adobe Systems# Inc.""""""""""""""; Scott Cohen# Adobe System Inc.",jiayang@adobe.com; zlin@adobe.com; scohen@adobe.com,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,01.01 Image processing,"01.08 Image enhancement# restoration# and denoising""",, ,Imaging,P 1D
1746,A machine learning approach for image deconvolution,Image deconvolution is the ill-posed problem of recovering a sharp image# given a blurry one generated by a convolution. In this work# we deal with space-invariant non-blind deconvolution. Currently# the most successful methods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise# and corrupts the image information. In a second (and arguably more difficult) step# one then needs to remove the colored noise# typically using a cleverly engineered algorithm. However# the methods based on this two-step approach do not properly address the fact that the image information has been corrupted. In this work# we also rely on a two-step procedure# but learn the second step on a large dataset of natural images# using a neural network. We will show that this approach outperforms the current state-of-the-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.,Christian Schuler*# MPI for Intelligent Systems; Harold Christopher Burger# MPI for Intelligent Systems; Stefan Harmeling# MPI for Intelligent Systems; Bernhard Schoelkopf# MPI for Intelligent Systems,christian.schuler@tuebingen.mpg.de; burger@tuebingen.mpg.de; stefan.harmeling@tuebingen.mpg.de; bs@tuebingen.mpg.de,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,01.01 Image processing,01.03 Computational Photography and Video,01.08 Image enhancement# restoration# and denoising, ,Imaging,P 1D
1819,Learning to Estimate and Remove Non-uniform Image Blur,This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say# horizontal) motion.  The estimation of the global (non-uniform) image blur is cast as a multi-label energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators# and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa's method by exploiting the natural order of discretized blur values for linear motions and defocus.  Once the blur has been estimated# the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images# and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al.,"Florent Couzinie-Devy*# Ecole Normale Superieure/INRIA; Jian Sun# Xi'an Jiaotong University# China""""""""""""""; Karteek Alahari# ENS-Willow; Jean Ponce# ENS",florent.couzinie-devy@ens.fr; jiansun@mail.xjtu.edu.cn; karteek.alahari@ens.fr; jean.ponce@ens.fr,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,01.01 Image processing,01.08 Image enhancement# restoration# and denoising,"03.04 Segmentation and Grouping""", ,Imaging,P 1D
2171,On a link between kernel mean maps and Fraunhofer diffraction# with an application to super-resolution beyond the diffraction limit,We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation# it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction# showing that imaging processes with arbitrarily small apertures can in principle be invertible# i.e.# do not lose information# provided the objects to be imaged satisfy a generic condition.  A real world experiment shows that we can super-resolve beyond the Rayleigh limit.,Stefan Harmeling*# MPI for Intelligent Systems; Michael Hirsch# ; Bernhard Schoelkopf# MPI for Intelligent Systems,stefan.harmeling@tuebingen.mpg.de; michael.hirsch@tuebingen.mpg.de; bs@tuebingen.mpg.de,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,01.01 Image processing,,, ,Imaging,P 1D
2083,Blur Processing Using Double Discrete Wavelet Transform,We propose a notion of double discrete wavelet transform (DDWT) that is designed to sparsify the blurred image and the blur kernel simultaneously. DDWT greatly enhances our ability to analyze# detect# and process blur kernels and blurry images---the proposed framework handles both global and spatially varying blur kernels seamlessly# and unifies the treatment of blur caused by object motion# optical defocus# and camera shake. To illustrate the potential of DDWT in computer vision and image processing# we develop example applications in blur kernel estimation# deblurring# and near-blur-invariant image feature extraction.,Yi Zhang*# University of Dayton; Keigo Hirakawa# ,zhangy3@udayton.edu; khirakawa1@udayton.edu,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,"01.08 Image enhancement# restoration# and denoising""",,, ,Imaging,P 1D
1387,Structured Face Hallucination,The goal of face hallucination is to generate high-resolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space# we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components# contours and smooth regions. We maintain the structure of each term by maintaining the gradients in the reconstructed high-resolution images. For facial components# we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours# we learn statistics-based priors to generate robust salient structures in high-resolution images. A patch matching method is utilized on the smooth regions and the image gradients are also preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability. ,Chih-Yuan Yang*# UC Merced; Sifei Liu# UC Merced; Ming-Hsuan Yang# UC Merced,cyang35@ucmerced.edu; sliu32@ucmerced.edu; mhyang@ucmerced.edu,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,09.01 Applications: humans,,, ,Imaging,P 1D
371,Unnatural Representation for Natural Image Deblurring,We first discuss in this paper that the success and efficiency of previous maximum a-posterior (MAP) based blur removal methods partly stem from their respective intermediate steps# which implicitly or explicitly create an unnatural representation that contains high-contrast image structures. We generalize this idea and propose a new effective framework for motion deblurring. Our system does not require extra filtering steps during optimization and demonstrates fast energy decreasing# making a small number of iterations enough for blur kernel estimation. It also provides a unified framework for both uniform and nonuniform motion deblurring. We validate our method extensively on benchmarking datasets and show comparison with other approaches with respect to convergence speed# running time# and result quality.,Li Xu*# CUHK; Shicheng Zheng# CUHK; Jiaya Jia# ,xuli@cse.cuhk.edu.hk; sczheng@cse.cuhk.edu.hk; leojia@cse.cuhk.edu.hk,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,,,, ,Imaging,P 1D
1788,Non-uniform motion deblurring for bilayer scenes,We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth# deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.,Paramanand Chandramouli*# IIT Madras; A. Rajagopalan# ,paramanand@gmail.com; raju@ee.iitm.ac.in,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,,,, ,Imaging,P 1D
1896,Depth Super Resolution by Rigid Body Self-Similarity in 3D,The task addressed in this work is to increase the spatial resolution and depth precision of a single# low-resolution# quantized and often noisy depth map. In contrast to previous work we do not utilize additional information# such as a high-resolution color image# multiple low-resolution depth images or a database of high resolution examples. Our approach increases the resolution of a single depth map by merging similar# albeit noisy# patches which come from the depth map itself. While this idea of exploiting image self-similarity has been successfully applied for color super resolution# we are the first to use it in the context of depth images. Our key contribution is to show that for depth images it is an advantage to move from 2D to 3D self-similarity# even with only approximate knowledge of the camera geometry# such that each 3D patch can undergo a 6 DoF rigid-body motion. Owing to the high-dimensional search space we introduce a new search algorithm# which is a 3D variant of the popular 2D PatchMatch technique. Another contribution is a simple# yet powerful upscaling algorithm# which predicts sharp object boundaries of the high resolution output depth map. We show that our results are highly competitive with those of alternative techniques leveraging even a higher resolution color image or an external depth patch database. ,Michael Hornacek*# TU Vienna; Christoph Rhemann# Microsoft Research Cambridge; Carsten Rother# ,hornacek@ims.tuwien.ac.at; chrheman@microsoft.com; carrot@microsoft.com,01.04 De-blurring and super-resolution*,01.04 De-blurring and super-resolution*,,,, ,Imaging,P 1D
189,Saliency Aggregation: A Data-driven Approach,A wide variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First# different methods have different performance in saliency analysis. Second# the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically# our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the individual contribution from individual saliency map but also the interaction between neighboring pixels. To account for the dependence of aggregation on an individual image# our approach first selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of methods that are aggregated.,Long Mai# Portland State University; Yuzhen Niu# Portland State University; Feng   Liu*# Portland State University,mtlong@cs.pdx.edu; yuzhen@cs.pdx.edu; fliu@cs.pdx.edu,01.05 Early and Biologically-inspired Vision*,01.05 Early and Biologically-inspired Vision*,,,,Saliency,Imaging,P 1D
845,What Makes a Patch Distinct? An Image Is More Than a Pile of Patches,What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns# others on the colors# and several add high-level cues and priors. We propose a simple# yet powerful# algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.,Ran Margolin*# Technion; Ayellet Tal# Technion; Lihi Zelnik-Manor# Technion,margolin@tx.technion.ac.il; ayellet@ee.technion.ac.il; lihi@ee.technion.ac.il,01.05 Early and Biologically-inspired Vision*,01.05 Early and Biologically-inspired Vision*,,,,Saliency,Imaging,P 1D
946,Learning video saliency from human gaze using candidate selection,During recent years remarkable progress has been made in visual saliency modeling. Our interest is in video saliency. Since videos are fundamentally different from still images# they are viewed differently by human observers. For example# the time each video frame is observed is a fraction of a second# while a still image can be viewed leisurely. Therefore#  video saliency estimation methods should differ substantially from image saliency methods. In this paper we propose a novel method for video saliency estimation# which is inspired by the way people watch videos. We explicitly model the continuity of the video by predicting the saliency map of a given frame# conditioned on the map from the previous frame. Furthermore# accuracy and computation speed are improved by restricting the salient locations to a carefully selected candidate set. We validate our method using two gaze-tracked video datasets and show we outperform the state-of-the-art. ,Dmitry Rudoy*# Technion; Dan Goldman# Adobe; Eli Shechtman# ; Lihi Zelnik-Manor# Technion,dmitry.rudoy@gmail.com; dgoldman@adobe.com; elishe@adobe.com; lihi@ee.technion.ac.il,01.05 Early and Biologically-inspired Vision*,01.05 Early and Biologically-inspired Vision*,,,,Saliency,Imaging,P 1D
961,Hierarchical Saliency Detection,When dealing with objects with complex patterns and texture# saliency detection confronts the critical problem that an image# in its salient foreground or background# contains small-size high contrast regions# which adversely affect saliency detection. This issue exists widely in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues from different layers and infer the final saliency map using a hierarchical model. Different from other multi-scale methods# which vary patches size or downscale images# our scale-based region handling is optimal to the objective using optimization. Our approach much improves saliency detection on common data# as well as many images that cannot be handled traditionally.,Qiong Yan*# CUHK; Li Xu# CUHK; Jianping Shi# CUHK; Jiaya Jia# ,qyan@cse.cuhk.edu.hk; xuli@cse.cuhk.edu.hk; jpshi@cse.cuhk.edu.hk; leojia@cse.cuhk.edu.hk,01.05 Early and Biologically-inspired Vision*,01.05 Early and Biologically-inspired Vision*,,,,Saliency,Imaging,P 1D
602,Consistent Exposure Stacks in the presence of Motion# Under-Exposure# and Saturation,We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously. Our method produces plausible results even on clipped pixels (that contain either too dark or bright values)# where accurate registration is impossible. We also show that our algorithm outperforms state-of-the-art approaches.,Jun Hu*# Duke University; Orazio Gallo# Nvidia Research; kari Pulli# Nvidia Research; Xiaobai Sun# Duke University,junhu@cs.duke.edu; orazio@soe.ucsc.edu; karip@nvidia.com; xiaobai@cs.duke.edu,01.06 High dynamic range imaging*,01.06 High dynamic range imaging*,01.01 Image processing,01.03 Computational Photography and Video,01.07 Image completion, ,Imaging,P 1D
625,FrameBreak:  Dramatic Image Extrapolation by Guided Shift-Maps,We significantly extrapolate the field of view of a photograph by learning from a roughly aligned# wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shift-map methods only support translations# this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation# scaling and reflection. To handle this increase in complexity# we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor# outdoor# natural# and man-made scenes.,Yinda Zhang# National University of Singapore; Jianxiong Xiao# Massachusetts Institute of Tec; James Hays# Brown University; Ping Tan*# ,zhangyinda@gmail.com; jxiao@csail.mit.edu; hays@cs.brown.edu; eletp@nus.edu.sg,01.07 Image completion*,01.07 Image completion*,04.03 Image stitching,,, ,Imaging,P 1D
1537,Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction,With the wide-spread of consumer 3D-TV technology#  stereoscopic video conference systems are emerging. However the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye region caused by these 3D glasses. More specifically we add polarized filters to the stereo camera so partial images of reflection can be captured. Then we develop a novel Bayesian model to describe the imaging process of the eye region including darkening and reflection# and infer the eye region based on Classification Expectation Maximization (EM). The recovered eye region under the glasses are brighter and with little reflections# leading to a more nature video conferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.,Mao Ye*# University of Kentucky; Cha Zhang# ; Ruigang Yang# University of Kentucky,mao.ye@uky.edu; chazhang@microsoft.com; ryang@cs.uky.edu,01.08 Image enhancement# restoration# and denoising*,01.08 Image enhancement# restoration# and denoising*,"01.01 Image processing""",,, ,Imaging,P 1D
554,Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras,The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems# however# is far from perfect. Some depth values can have significant errors# while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper# we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene# the missing depth value phenomenon# and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem# and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels# we propose a depth correction and completion algorithm that outperforms other techniques in the literature.,Ju Shen*# University of Kentucky; Sen-ching Cheung# University of Kentucky ,jushen.tom@gmail.com; cheung@engr.uky.edu,01.08 Image enhancement# restoration# and denoising*,01.08 Image enhancement# restoration# and denoising*,01.07 Image completion,01.12 Sensors,"08.05 Markov Random Fields""", ,Imaging,P 1D
529,Separating Signal from Noise  using  Patch Recurrence across Scales,Recurrence of small clean image patches across different scales of a natural image has been successfully used for solving ill-posed problems in clean images (e.g.# super-resolution from a single image). In this paper we show how this multi-scale property can be extended to solve ill-posed problems under noisy conditions# such as image denoising. While clean patches are obscured by severe noise in the original scale of a noisy image# noise levels drop dramatically at coarser image scales. This allows for the unknown hidden clean patches to ``naturally emerge'' in some coarser scale of the noisy image. We further show that patch recurrence across scales is strengthened when using directional pyramids (that blur and subsample only in one direction). Our statistical experiments show that for almost any noisy image patch (more than 99%)# there exists a ``good'' clean version of itself at the same relative image coordinates in some coarser scale of the image. This is a strong phenomenon of noise-contaminated natural images# which can serve as a strong prior for separating the signal from the noise. Finally# incorporating this multi-scale prior into a simple denoising algorithm yields state-of-the-art denoising results.,Maria Zontak*# Weizmann Institute; Michal Irani# ; Inbar Mosseri# Weizmann,maria.zontak@weizmann.ac.il; michal.irani@weizmann.ac.il; inbar.mosseri@weizmann.ac.il,01.08 Image enhancement# restoration# and denoising*,01.08 Image enhancement# restoration# and denoising*,"01.10 Multi-scale processing""",,, ,Imaging,P 1D
163,Texture-Preserved Image Denoising via Gradient Histogram Estimation and Matching,Image denoising is a classical yet fundamental problem in low level vision# as well as an ideal test bed to evaluate various statistical image modeling methods. One of the most challenging problems in image denoising is how to preserve the fine scale texture structures while removing noise. Various natural image priors# such as gradient based prior# nonlocal self-similarity prior# and sparsity prior# have been extensively exploited for noise removal. The denoising algorithms based on these priors# however# tend to smooth the detailed image textures# degrading the image visual quality. To address this problem# in this paper we propose a texture enhanced image denoising (TEID) method by enforcing the gradient distribution of the denoised image to be close to the estimated gradient distribution of the original image. A novel gradient histogram preservation (GHP) algorithm is developed to enhance the texture structures while removing noise. Our experimental results demonstrate that the proposed GHP based TEID can well preserve the texture features of the denoised images# making them look more natural.,Wangmeng Zuo# Harbin Institute of Technology; Lei Zhang*# The Hong Kong Polytechnic University; Chunwei Song# Harbin Institute of Technology; David Zhang# ,cswmzuo@gmail.com; cslzhang@comp.polyu.edu.hk; wolvesandme@gmail.com; csdzhang@comp.polyu.edu.hk,01.08 Image enhancement# restoration# and denoising*,01.08 Image enhancement# restoration# and denoising*,"05.04 Image-based Modeling""",,, ,Imaging,P 1D
1512,Fast Patch-based Denoising Using Approximated Patch Geodesic Paths,Patch-based methods such as Non-Local Means (NLM) and BM3D have become the defacto gold standard for image denoising. The core of these approaches is to use similar patches within the image as cues for denoising. The operation usually requires expensive pair-wise patch comparisons. In this paper# we present a novel fast patch-based denoising technique based on Patch Geodesic Paths (PatchGP). PatchGPs treat image patches as nodes and patch differences as edge weights for computing the shortest (geodesic) paths. The path lengths can then be used as weights of the smoothing/denoising kernel.  We first show that# for natural images# PatchGPs can be effectively approximated by minimum hop paths (MHPs) that generally correspond to Euclidean line paths connecting two patch nodes. To construct the denoising kernel# we further discretize the MHP search directions and use only patches along the search directions. Along each MHP# we apply a weight propagation scheme to robustly and efficiently compute the path distance. Finally# to handle noise at multiple scales# we conduct wavelet image decomposition and apply PatchGP scheme at each scale. Comprehensive experiments show that our approach achieves comparable quality as the state-of-the-art methods such as NLM# BM3D# and Field-of-Experts but is a few magnitudes faster. \end{abstract},Xiaogang Chen*# Shanghai Jiao Tong University; Sing Bing Kang# Microsoft Research; Jie Yang# Shanghai jiaotong university; Jingyi Yu# ,jhredblack@yahoo.com.cn; sbkang@microsoft.com; jieyang@sjtu.edu.cn; yu@eecis.udel.edu,01.08 Image enhancement# restoration# and denoising*,01.08 Image enhancement# restoration# and denoising*,,,, ,Imaging,P 1D
1605,A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems,In this work# we address the multi-label Mumford-Shah problem# i.e.# the problem of jointly estimating a partitioning of the domain of the image# and functions defined within regions of the partition. We create algorithms that are efficient# robust to undesirable local minima# and are easy-to-implement. Our algorithms are formulated by slightly modifying the underlying statistical model from which the multi-label Mumford-Shah functional is derived. The advantage of this statistical model is that the underlying variables: the labels and the functions are less coupled than in the original formulation# and the labels can be computed from the functions with more global updates. The resulting algorithms can be tuned to the desired level of locality of the solution: from fully global updates to more local updates. We demonstrate our algorithm on two applications: joint multi-label segmentation and denoising# and joint multi-label motion segmentation and flow estimation. We compare to the state-of-the-art in multi-label Mumford-Shah problems and show that we achieve more promising results.,Zhaojin  Lu# King Abdullah University of Science and Technology; Byung-Woo Hong# Chung-Ang University; Ganesh Sundaramoorthi*# King Abdullah University of Science and Technology,Zhaojin.Lu@kaust.edu.sa; hong@cau.ac.kr; ganesh.sundaramoorthi@kaust.edu.sa,01.11 PDEs and level-set methods*,01.11 PDEs and level-set methods*,03.01 Segmentation and 2D shape,03.03 Image segmentation,04.01 Motion estimation and alignment, ,Imaging,P 1D
1342,Computing Diffeomorphic Paths for Large Motion Interpolation,In this paper# we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms.  Direct computation of a geodesic path on the  space of diffeomorphisms $Diff(\Omega)$ is difficult# and it can be attributed mainly to the infinite dimensionality of  $Diff(\Omega)$.  Our proposed framework# to some degree# bypasses this difficulty using the quotient map  of $Diff(\Omega)$ to its quotient space $Diff(M)/Diff(M)_{\mu}$ by the subgroup of volume-preserving diffeomorphisms.   This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature# a space with well-known differential geometric properties.  Our framework leverages this recent result by computing the diffeomorphic path in two stages.     First# we project the given diffeomorphism pair onto this sphere and compute the geodesic path between the projected points. Second# we lift the geodesic on the sphere back to the space of diffeomerphisms# and for this step# we solve a quadratic programming problem with bilinear constraints using the augmented Lagrangian method with penalty. In this way# we can achieve the estimated path of diffeomorphisms# first# staying on the space of diffeomorphisms# and second# preserving shapes/volumes in the deformed images through the path as much as it can.  As an application# we have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences.  In the reported experiments# our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).,Dohyung Seo# University of Florida; Baba  Vemuri*# University of Florida; Jeffrey Ho# University of Florida,dhseo@ufl.edu; vemuri@cise.ufl.edu; jho@cise.ufl.edu,01.11 PDEs and level-set methods*,01.11 PDEs and level-set methods*,10.07 Additional applications,,, ,Imaging,P 1D
1233,Rotation# Scaling and Deformation Invariant Scattering for Texture Discrimination,An image representation for texture discrimination is constructed by cascading invariant scattering transforms. A joint roto-translation invariant scattering is introduced to preserve information on the joint spatial and rotation distribution of image structures. Invariance to scaling is obtained with a logarithmic non-linearity and a joint averaging in space and scale. A PCA classifier applied to this scattering representation yields state-of-the-art texture classifications on data bases of complex textures with deformations.,Laurent Sifre*# Ecole Polytechnique; Stephane Mallat# Echole Polytechnique,laurent.sifre@polytechnique.edu; mallat@cmap.polytechnique.fr,01.13 Texture analysis and synthesis*,01.13 Texture analysis and synthesis*,01.01 Image processing,01.10 Multi-scale processing,02.03 Feature descriptors, ,Imaging,P 1D
1943,Sensing and Recognizing Surface Textures Using a GelSight Sensor,Sensing surface textures by touch is a valuable capability for robotics.  Until recently it was difficult to build a compliant sensor with high sensitivity and high resolution. The GelSight sensor is compliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface textures. The GelSight sensor# when pressed against a surface# delivers a height map. This can be treated as an image# and processed using the tools of visual texture analysis. We have devised a gray-scale and rotationally invariant texture recognition system based on local binary patterns (LBP)# and enhanced it by the use of a multi-scale pyramid and a Hellinger distance metric. We built a database with 40 classes of tactile textures using materials such as fabrics# wood# and sandpaper. Our system can correctly categorize materials from this database with high accuracy. This suggests that the GelSight sensor can be useful for material recognition by robots.,Rui Li*# MIT; Edward Adelson# MIT,rui@mit.edu; adelson@csail.mit.edu,01.13 Texture analysis and synthesis*,01.13 Texture analysis and synthesis*,01.10 Multi-scale processing,01.12 Sensors,, ,Imaging,P 1D
2135,Enriching Texture Analysis with Semantic Data,We argue for the importance of explicit semantic modeling in human-centered texture analysis tasks such as retrieval# annotation# synthesis# and zero-shot learning.  To this end# low-level attributes are selected and used to define a semantic space of visual texture. Textures varied in subject# illumination# and rotation are positioned within this semantic space using a pairwise relative comparison procedure robust against cognitive bias. Existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures exhibiting disorder are shown to be poorly modeled by existing descriptors.   In a retrieval experiment semantic descriptors are shown to outperform a hybrid descriptor built from the most discriminative elements of the individual visual descriptors. Semantic modeling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.,Tim Matthews*# University of Southampton,tm1e10@soton.ac.uk,01.13 Texture analysis and synthesis*,01.13 Texture analysis and synthesis*,02.03 Feature descriptors,,, ,Imaging,P 1D
627,A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions,This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that can be efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and a putative vanishing point# and for computing the vanishing point defined by a subset of edges. Extensive experiments in both real and synthetic images show that our algorithms outperform the state-of-the-art methods while keeping computation tractable. In addition# we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.,Michel Antunes*# Inst. of Systems and Robotics; Joao Barreto# ,michel@isr.uc.pt; jpbar@isr.uc.pt,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,02.04 Feature detection,05.01 3D modeling and reconstruction,, ,Pose and Photometry,P 2A
315,Cloud Motion as a Calibration Cue,We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras.  This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion# focal length and geo-orientation.  Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions.  Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration.  We show results on real scenes that demonstrate the effectiveness of our proposed methods.   ,Nathan Jacobs*# University of Kentucky; Mohammad Islam# University of Kentucky; Scott Workman# University of Kentucky,jacobs@cs.uky.edu; mtis222@cs.uky.edu; smwork3@cs.uky.edu,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,04.01 Motion estimation and alignment,04.05 Object Tracking and Motion Analysis,04.06 Optical Flow, ,Pose and Photometry,P 2A
1408,SLAM++: Simultaneous Localisation and Mapping at the Level of Objects},We present the major advantages of a new real-time `object oriented' 3D SLAM paradigm# which takes full advantage in the loop of prior knowledge that the real-world tends to consist of repeated# domain-specific objects and structures.  As a hand-held depth camera browses a cluttered scene# real-time 3D object recognition and tracking provides a stream of 6DoF camera-object constraints which feed into an ever-growing graph of repetitive solid objects# continually optimised for metric consistency by an efficient pose-graph back end.  This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruction# but with a huge representation compression.   The object graph enables predictions for accurate iterative ICP-based camera to model tracking at each live frame# and efficient active search for new objects in currently undescribed image regions. We demonstrate robust real-time SLAM in large# cluttered environments# including loop closure# relocalisation and map adjustment in the case of moved objects. Naturally the object level description we recover also permits intelligent annotation of the scene with augmented reality.,Renato Salas-Moreno*# Imperial College London; Richard Newcombe# ICL; Paul Kelly# Imperial College London; Hauke Strasdat# Imperial College London; Andrew Davison# ,rfs09@doc.ic.ac.uk; rnewcomb@doc.ic.ac.uk; p.kelly@imperial.ac.uk; strasdat@gmail.com; ajd@doc.ic.ac.uk,05.03 Calibration and pose estimation*,05.01 3D modeling and reconstruction*,04.01 Motion estimation and alignment,04.04 Model-based reconstruction and tracking,05.02 Active rangefinding and depth sensors, ,Pose and Photometry,P 2A
311,Rolling Shutter Camera Calibration,Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devicesÂ—from smart-phones to high-end cameras. It is well known# that if a RS camera is used with a moving camera or scene# significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware and the ability to remove the cameraÂ’s lens. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.,Luc Oth*# ETH Zurich; Paul Furgale# ; Laurent Kneip# ETH Zurich; Roland Siegwart# ,othlu@student.ethz.ch; paul.furgale@mavt.ethz.ch; laurent.kneip@mavt.ethz.ch; rsiegwart@ethz.ch,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,05.10 Structure from Motion,,, ,Pose and Photometry,P 2A
1901,Radial Distortion Self-Calibration,In cameras with radial distortion straight lines in space are in general mapped to curves in the image. Although this is also true for epipolar geometry# special epipolar lines (those that go through the distortion center) remain straight. By finding these straight epipolar lines in camera pairs we obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove the idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this# we give comprehensice relations between different undistortion models and discuss advantages and drawbacks.,Jose Brito# Universidade do Minho# Guimaraes# Portugal; Kevin Koeser*# ; Roland Angst# Stanford University; Manuel JoÃ£o Ferreira# ; marc pollefeys# ETHZ,josehbrito@gmail.com; kevin.koeser@inf.ethz.ch; rangst@stanford.edu; mjf@dei.uminho.pt; marc.pollefeys@inf.ethz.ch,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,05.10 Structure from Motion,,, ,Pose and Photometry,P 2A
598,A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-made Environments,We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by novelly modeling the measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models# we formally derive a novel consistency measure# which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error# and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works# our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance# in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.,Yiliang Xu*# Kitware Inc.; Sangmin Oh# Kitware Inc.; Anthony Hoogs# ,yiliang.xu@kitware.com; sangmin.oh@kitware.com; anthony.hoogs@kitware.com,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,07.03 Context and scene understanding,,, ,Pose and Photometry,P 2A
45,Five Shades of Grey for Fast and Reliable Camera Pose Estimation,We introduce here an improved design for the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions# under a severe perspective# while heavily occluded# and under a strong motion blur.  Our marker field detection harnesses the fact that edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules.  The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time# and more importantly# our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where the planarity is assured and where the presented marker field design and detection algorithm provide a reliable and extremely fast solution.,Adam Herout*# Brno University of Technology; IstvÃ¡n SzentandrÃ¡si# Brno University of Technology; Michal ZachariÃ¡Âš# Brno University of Technology; MarkÃ©ta DubskÃ¡# Brno University of Technology; Rudolf Kajan# Brno University of Technology,herout@fit.vutbr.cz; iszent@fit.vutbr.cz; izacharias@fit.vutbr.cz; idubska@fit.vutbr.cz; ikajanr@fit.vutbr.cz,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,,,, ,Pose and Photometry,P 2A
975,Can a Fully Unconstrained Imaging Model be Applied Effectively to Central Cameras?,Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices# ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model#  and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it# resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasi-pinhole central cameras is supported by an extensive experimental validation.,Filippo Bergamasco# UniversitÃ  Ca Foscari Venezia; Andrea Albarelli# ; Emanuele Rodola# The University of Tokyo; Andrea Torsello*# ,fbergama@gmail.com; albarelli@unive.it; e.rodola@gmail.com; torsello@dsi.unive.it,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,,,, ,Pose and Photometry,P 2A
1758,Single Image Calibration of Multi-Axial Imaging Systems,Imaging systems consisting of a camera looking at multiple spherical mirrors (reflection) or multiple refractive spheres (refraction) have been used for wide-angle imaging applications. We describe such setups as multi-axial imaging systems# since a single sphere results in an axial system. Assuming an internally calibrated camera# calibration of such multi-axial systems involves estimating the sphere radii and locations in the camera coordinate system. However# previous calibration approaches require manual intervention or constrained setups. We present a fully automatic approach using a single photo of a 2D calibration grid. The pose of the calibration grid is assumed to be unknown and is also recovered. Our approach can handle unconstrained setups# where the mirrors/refractive balls can be arranged in any fashion# not necessarily on a grid.  The axial nature of rays allows us to compute the axis of each sphere separately. We then show that by choosing rays from two or more spheres# the unknown pose of the calibration grid can be obtained linearly and independently of sphere radii and locations. Knowing the pose# we derive analytical solutions for obtaining the sphere radius and location. This leads to an interesting result that $6$-DOF pose estimation of a multi-axial camera can be done without the knowledge of full calibration. Simulations and real experiments demonstrate the applicability of our algorithm.,Amit Agrawal*# MERL; Srikumar  Ramalingam# MERL,agrawal@merl.com; ramalingam@merl.com,05.03 Calibration and pose estimation*,05.03 Calibration and pose estimation*,,,, ,Pose and Photometry,P 2A
525,The Episolar Constraint: Monocular Shape from Shadow Correspondence,Shadows encode a powerful geometric cue: if one pixel casts a shadow onto another# then the two pixels are colinear with the lighting direction.  Given many images over many lighting directions# this constraint can be leveraged to recover the depth of a scene from a single viewpoint.  For outdoor scenes with solar illumination# we term this the episolar constraint# which provides a convex optimization to solve for the sparse depth of a scene from shadow correspondences# a method to reduce the search space when finding shadow correspondences# and a method to geometrically calibrate a camera using shadow constraints.  Our method constructs a dense network of nonlocal constraints which complements recent work on outdoor photometric stereo and cloud based cues for 3D. We demonstrate results across a variety of time-lapse sequences from webcams ``in the wild.''  ,Austin Abrams*# Washington University in St. L; Kylia Miskell# Washington University in St. Louis; Robert Pless# Washington University in St. Louis,abramsa@cse.wustl.edu; klm4@wustl.edu; pless@cse.wustl.edu,05.07 Shape from shading and specularities*,05.07 Shape from shading and specularities*,10.01 Aerial and outdoor image analysis and modeling,10.07 Additional applications,, ,Pose and Photometry,P 2A
476,Depth-assisted Shape-from-Shading,We present a depth-assisted shape-from-shading algorithm which uses a noisy# incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework# the partial depth information is used to overcome bas-relief ambiguity in normals estimation# as well as to assist in recovering relative albedos# which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.,Lap-Fai Yu*# UCLA; Sai-Kit Yeung# Singapore University of Technology and Design; Yu-Wing Tai# KAIST; Stephen Lin# ,craigyu@ucla.edu; saikit@sutd.edu.sg; yuwing@gmail.com; stevelin@microsoft.com,05.07 Shape from shading and specularities*,05.07 Shape from shading and specularities*,,,, ,Pose and Photometry,P 2A
502,Illumination Estimation based on Bilayer Sparse Coding,Computational color constancy is a very important topic in computer vision and has attracted many researchers' attention. Recently# lots of research has shown the effects of high level visual content information for illumination estimation. However# all of these existing methods are essentially combinational strategies in which image's content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper# we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose# the image's scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on two real-world image sets show that our algorithm is superior to other prevailing illumination estimation methods# even better than combinational methods.,Bing Li*# Institute of Automation; Weihua Xiong# ; Weiming Hu# ,bli@nlpr.ia.ac.cn; bjtulb@gmail.com; bjtulb@163.com,06.01 Illumination and Reflectance Modeling,01.02 Color processing and tone adjustment*,06.01 Illumination and Reflectance Modeling,,, ,Pose and Photometry,P 2A
890,Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions,We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials# such as wood# fabric# and granite. At appropriate scales# even ``uniform'' materials will also exhibit texture features that can be helpful for recognition# such as paper# metal# and ceramic. To cope with the high-dimensionality of BTFs# in this paper# we proposed to learn discriminative illumination patterns and texture filters# with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome# with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.,Chao Liu# Rochester Institute of Technology; Gefei Yang# Rochester Institute of Technology; Jinwei Gu*# Rochester Institute of Technology,bahatuestc@gmail.com; gxy2895@rit.edu; jwgu@cis.rit.edu,06.01 Illumination and Reflectance Modeling*,06.01 Illumination and Reflectance Modeling*,01.03 Computational Photography and Video,01.13 Texture analysis and synthesis,07.01 Recognition, ,Pose and Photometry,P 2A
1923,A Theory of Refractive Photo-Light-Path Triangulation ,3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack   appearance related visual cues and merely reflect and refract light from the surrounding   environment. Amongst several approaches to reconstruct such objects# the seminal work of   Light-Path triangulation is highly popular because of its general   applicability and analysis of minimal scenarios. A light-path is defined as the piece-wise linear   path taken by a ray of light as it passes from source# through the object and into the camera.   Transparent refractive objects not only affect the geometric configuration of light-paths but   also their radiometric properties.   In this paper# we describe a method that combines both geometric and   radiometric information to do reconstruction. We show two major consequences of    the addition of radiometric cues to the light-path setup.    Firstly# we extend the case of scenarios in which reconstruction is plausible# while reducing the minimal requirement of observations for a unique reconstruction. This happens   as a consequence of the fact that radiometric cues add an additional known variable to   the already existing system of equations.    Secondly# we present a \emph{simple} algorithm for reconstruction# owing to the nature   of the radiometric cue. We present several synthetic experiments to validate our   theories# and show high quality reconstructions in challenging scenarios.,Visesh Chari*# INRIA; Peter Sturm# INRIA,visesh@gmail.com; peter.sturm@inria.fr,06.01 Illumination and Reflectance Modeling*,06.01 Illumination and Reflectance Modeling*,05.07 Shape from shading and specularities,06.02 Photometric stereo,06.03 Photometric modeling, ,Pose and Photometry,P 2A
746,Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance Under Natural Illumination and Non-Lambertian Reflectance,"Appearance variation due to illumination changes is an inherent challenge in many vision tasks such as recognition. Inverse rendering tasks would benefit from a low-dimensional generative model which captures appearance variations w.r.t. illumination conditions and surface reflectance properties. However# conventional subspace construction approaches suffer from the need of ""large-enough"" image ensemble rendering numerical methods intractable. In this paper# we propose an analytic formulation for low-dimensional subspace construction in which shading cues lie while preserving the natural structure of an image sample. We take advantage of the frequency-space representation of the image irradiance equation where the process of finding such subspace can be cast as establishing a relation between its principal components and that of a deterministic set of basis functions# termed as irradiance harmonics. This resolves the issue of dimensionality since the source of randomness in the imaging process becomes the irradiance harmonics coefficients rather than the whole image realization. Representing images in their natural dimension# i.e. matrices# further lessen the number of parameters to be estimated to define a bilinear projection which maps the image sample to a lower-dimensional bilinear subspace. Results show significant impact on dimensionality reduction with minimal loss of information as well as robustness against noise.",Shireen Elhabian*# University of Louisville,shireen.youssef@gmail.com,06.01 Illumination and Reflectance Modeling*,06.01 Illumination and Reflectance Modeling*,06.03 Photometric modeling,08.09 Statistical Methods and Learning,, ,Pose and Photometry,P 2A
1105,Spectral Modeling and Relighting of Reflective-Fluorescent Scenes,Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination# which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper# we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally# we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.,Antony Lam*# Natl. Institute of Informatics; Imari Sato# ,antony@nii.ac.jp; imarik@nii.ac.jp,06.01 Illumination and Reflectance Modeling*,06.01 Illumination and Reflectance Modeling*,06.03 Photometric modeling,,, ,Pose and Photometry,P 2A
1805,Specular Reflection Separation using Dark Channel Priors,We present a novel method to separate specular reflection from a single image. Separating an image into diffuse and specular components is an ill-posed problem due to lack of observations. Existing methods rely on a specular-free image to detect and estimate specularity# which may confuse diffuse pixels with the same hue but a different saturation value as specular pixels. Our method is based on a novel observation that for most natural images the dark channel can provide an approximate specular-free image which can be used to resolve the saturation ambiguity. We propose a maximum a posteriori formulation which accurately recovers the specular reflection and chromaticity. We demonstrate the effectiveness of the proposed algorithm on real and synthetic examples. Experimental results show that our method significantly outperforms the state-of-the-art methods in separating specular reflection.,Hyeongwoo Kim*# KAIST; Hailin Jin# Adobe; Sunil Hadap# ,hyeongwoo.kim@kaist.ac.kr; hljin@adobe.com; hadap@adobe.com,06.01 Illumination and Reflectance Modeling*,06.01 Illumination and Reflectance Modeling*,06.03 Photometric modeling,,, ,Pose and Photometry,P 2A
590,BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition,In this paper we introduce unique publicly available dense BRDF data measurements. We use this dense data as a reference for performance evaluation of the proposed BRDF sparse angular sampling and interpolation approach. The method is based on sampling of BRDF subspaces at fixed elevations by means of several adaptively-represented perpendicular slices# which are distributed uniformly. Although the proposed method requires only a sparse sampling of the material the interpolation provides very accurate reconstruction visually and computationally comparable to densely measured reference. Due to the simple slices measurement and method's robustness it allows highly accurate acquisition of BRDFs# which is in comparison with standard uniform angular sampling considerably faster while using far less samples.,Jiri Filip*# UTIA AS CR; Radomir Vavra# Institute of Information Theory and Automation of the AS CR; Michal Haindl# UTIA AS CR; Vlastimil Havran# Czech Technical University in Prague; Pavel Zid# Institute of Information Theory and Automation of the AS CR; Mikulas Krupicka# Institute of Information Theory and Automation of the AS CR,filipj@utia.cas.cz; vavra@utia.cas.cz; haindl@utia.cas.cz; havran@fel.cvut.cz; zid@utia.cas.cz; krupimik@utia.cas.cz,06.01 Illumination and Reflectance Modeling*,06.01 Illumination and Reflectance Modeling*,,,, ,Pose and Photometry,P 2A
210,A New Perspective on Uncalibrated Photometric Stereo,We investigate the problem of reconstructing normals# albedo and lights of Lambertian surfaces in uncalibrated photometric stereo under the perspective projection model. Our analysis is based on establishing the integrability constraint. In the orthographic projection case# it is well-known that when such constraint is imposed# a solution can be identified only up to 3 parameters# the so-called generalized bas-relief (GBR) ambiguity. We show that in the perspective projection case the solution is unique. We also propose a closed-form solution which is simple# efficient and robust. We test our algorithm on synthetic data and publicly available real data. Our quantitative tests show that our method outperforms all prior work of uncalibrated photometric stereo under orthographic projection.,Thoma Papadhimitri*# University of Bern; Paolo Favaro# University of Bern,thoma.papadhimitri@iam.unibe.ch; paolo.favaro@gmail.com,06.02 Photometric stereo*,06.02 Photometric stereo*,05.01 3D modeling and reconstruction,05.07 Shape from shading and specularities,06.01 Illumination and Reflectance Modeling, ,Pose and Photometry,P 2A
418,Multi-view Photometric Stereo with Spatially Varying Isotropic Materials,We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple# which consists of only a digital camera and a handheld light source. From a single viewpoint# we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distribution function (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments# the captured shapes are accurate to $0.3$ millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9\%.,Zhenglong Zhou# National University of Singapore; Zhe Wu# National University of Singapore; Ping Tan*# ,zlzhou@nus.edu.sg; wuzhe@nus.edu.sg; eletp@nus.edu.sg,06.02 Photometric stereo*,06.02 Photometric stereo*,05.01 3D modeling and reconstruction,06.01 Illumination and Reflectance Modeling,10.06 Vision for Graphics, ,Pose and Photometry,P 2A
356,Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances,We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile# which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials# the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals# and that the intensity distribution of an intensity profile conveys information about the reflectance properties# when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations# we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.,Feng Lu*# The University of Tokyo; Yasuyuki Matsushita# MSRA; Imari Sato# ; Takahiro Okabe# ; Yoichi Sato# The University of Tokyo,lufeng@iis.u-tokyo.ac.jp; yasumat@microsoft.com; imarik@nii.ac.jp; takahiro@iis.u-tokyo.ac.jp; ysato@iis.u-tokyo.ac.jp,06.02 Photometric stereo*,06.02 Photometric stereo*,05.04 Image-based Modeling,05.11 Three-dimensional modeling and manipulation,06.03 Photometric modeling, ,Pose and Photometry,P 2A
1632,Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis,Under unknown directional lighting# the uncalibrated Lambertian photometric stereo algorithm recovers the shape of a smooth surface up to the generalized bas-relief (GBR) ambiguity. We resolve this ambiguity from the half-vector symmetry# which is observed in many isotropic materials. Under this symmetry# a 2D BRDF slice with low-rank structure can be obtained from an image# if the surface and lighting direction are recovered correctly. In general# this structure is destroyed by the GBR ambiguity. As a result# we can resolve the GBR ambiguity by restoring this structure. We develop a simple algorithm of auto-calibration from a separable homogeneous specular reflection of real images. Compared with previous methods on auto-calibration# this method takes a holistic approach to exploiting reflectance symmetry and produces superior results. ,Zhe Wu# NUS; Ping Tan*# ,wuzhe06@gmail.com; eletp@nus.edu.sg,06.02 Photometric stereo*,06.02 Photometric stereo*,,,, ,Pose and Photometry,P 2A
1880,Articulated and Restricted Motion Subspaces and Their Signatures,Articulated objects represent an important class of objects in our everyday environment.  Automatic detection of the type of articulated or otherwise restricted motion and extraction of the corresponding motion parameters are therefore of high value# \eg in order to augment an otherwise static 3D reconstruction with dynamic semantics# such as rotation axes and allowable translation directions for certain rigid parts or objects. Hence# in this paper# a novel theory to analyse relative transformations between two motion-restricted parts will be presented.  The analysis is based on linear subspaces spanned by relative transformations. Moreover# a signature for relative transformations will be introduced which uniquely specifies the type of restricted motion encoded in these relative transformations.  This theoretic framework enables the derivation of novel algebraic constraints# such as low-rank constraints for subsequent rotations around two fixed axes for example. Lastly# given the type of restricted motion as predicted by the signature# the paper shows how to extract all the motion parameters with matrix manipulations from linear algebra. Our theory is verified on several real data sets# such as a rotating blackboard or a wheel rolling on the floor amongst others.,Bastien Jacquet*# ETH Zurich; Roland Angst# Stanford University; marc pollefeys# ETHZ,bastien.jacquet@inf.ethz.ch; rangst@stanford.edu; marc.pollefeys@inf.ethz.ch,05.10 Structure from Motion*,05.10 Structure from Motion*,04.01 Motion estimation and alignment,05.01 3D modeling and reconstruction,05.11 Three-dimensional modeling and manipulation, ,Pose and Photometry,P 2A
1027,Template-Based Isometric Deformable 3D Reconstruction with Focal Length Self-Calibration,It has been shown that a surface deforming isometrically can be reconstructed from a single image and a template 3D shape. Methods from the literature solve this problem efficiently. However# they all assume that the camera model is calibrated# which drastically limits their applicability.  We propose {\em (i)} a general variational framework that applies to (calibrated and uncalibrated) general camera models and {\em (ii)} self-calibrating 3D reconstruction algorithms for the weak-perspective and full-perspective camera models. In the former case# our algorithm returns the normal field and camera's scale factor. In the latter case# our algorithm returns the normal field# depth and camera's focal length. Our algorithms are the first to achieve deformable 3D reconstruction including camera self-calibration. They apply to much more general setups than existing methods.  Experimental results on simulated and real data show that our algorithms give results with the same level of accuracy as existing methods using the true focal length on perspective images# and correctly find the normal field on affine images for which the tested existing methods fail.,Adrien Bartoli*# Universit?d'Auvergne; Toby Collins# ISIT,adrien.bartoli@gmail.com; Toby.Collins@gmail.com,05.10 Structure from Motion*,05.10 Structure from Motion*,05.01 3D modeling and reconstruction,05.03 Calibration and pose estimation,, ,Pose and Photometry,P 2A
1636,Monocular Template-Based 3D Reconstruction of Extensible Surfaces with Local Linear Elasticity,We propose a new approach for template-based extensible surface reconstruction from a single view. We extend henceforth the method of isometric surface reconstruction and more recently# conformal surface reconstruction. Our approach relies on the minimization of a proposed stretching energy formalized with respect to the Poisson's ratio parameter of the surface. We derive a patch-based formulation of this stretching energy by assuming local linear elasticity.~This formulation unifies geometrical and mechanical constraints in a single energy term.  We formally prove that we do not have global scale ambiguities and we prevent from local scale ambiguities by imposing a set of fixed boundary 3D points. We experimentally prove the necessary and sufficient condition of this set of boundary points. We demonstrate the effectiveness of our approach on different developable and non-developable surfaces with wide range of extensibility.,Abed Malti*# ALCoV-ISIT; Richard Hartley# ; Adrien Bartoli# Universit?d'Auvergne; Jae-Hak Kim# Australian National University,abed.malti@gmail.com; richard.hartley@anu.edu.au; adrien.bartoli@gmail.com; inyashio@gmail.com,05.10 Structure from Motion*,05.10 Structure from Motion*,05.01 3D modeling and reconstruction,,, ,Pose and Photometry,P 2A
2161,Non-Rigid Structure from Motion with Diffusion Maps Prior,In this paper# a novel approach based on a non-linear manifold learning technique is proposed to recover 3D non-rigid structures from 2D image sequences captured by a single camera. Most of the existing approaches assume that 3D shapes can be accurately modelled in a linear subspace. These techniques perform well when the deformations are relatively small or simple# but fail when more complex deformations need to be recovered. The non-linear deformations are often observed in highly flexible objects for which the use of the linear model is impractical.   A specific type of shape variations might be governed by only a small number of parameters# therefore can be well-represented in a low dimensional manifold. We learn a non-linear shape prior using diffusion maps method. The key contribution in this paper is the introduction of the shape prior term to constrain the reconstructed shapes to lie in the learned manifold. The proposed methodology has been validated quantitatively and qualitatively on 2D points sequences projected from the 3D motion capture data and real 2D video sequences. The comparisons of the proposed manifold based method against several state-of-the-art techniques are shown on different types of deformable objects. ,Lili Tao*# Uniersity of Central Lancashire; Bogdan Matuszewski# University of Central Lancashire,lltao@uclan.ac.uk; bmatuszewski1@uclan.ac.uk,05.10 Structure from Motion*,05.10 Structure from Motion*,05.01 3D modeling and reconstruction,,, ,Pose and Photometry,P 2A
1759,Joint Detection# Tracking and Mapping by Semantic Bundle Adjustment,In this paper we propose a novel Semantic Bundle Adjustment framework comprising a SLAM engine and an integrated object detection pipeline. The system builds on established tracking and mapping techniques to exploit incremental 3D reconstruction in order to validate hypotheses on the presence of sought objects. Then# detected objects are explicitly taken into account for a global semantic optimization of both the unknown camera and object poses. Thus# unlike all systems proposed so far# our approach allows for solving jointly the detection-SLAM problems# so that it can both detect objects as well as improve SLAM accuracy.,Nicola Fioraio*# DISI# University of Bologna; Luigi Di Stefano# DISI# University of Bologna,nicola.fioraio@unibo.it; luigi.distefano@unibo.it,05.10 Structure from Motion*,05.10 Structure from Motion*,05.03 Calibration and pose estimation,07.06 Object Detection,10.05 Robot vision, ,Pose and Photometry,P 2A
308,A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation,Due to its simplicity# the eight-point algorithm has been widely used in fundamental matrix estimation. Unfortunately# the rank-2 constraint of a fundamental matrix is enforced via a posterior rank correction step# thus leading to non-optimal solutions to the original problem. To address this drawback# existing algorithms need to solve either a very high order polynomial or a sequence of convex relaxation problems# both of which are computationally ineffective and numerically unstable. In this work# we present a new rank-2 constrained eight-point algorithm# which directly incorporates the rank-2 constraint in the minimization process. To avoid singularities# we propose to solve seven subproblems and retrieve their globally optimal solution by using tailored polynomial system solvers. Our proposed method is noniterative# computationally efficient and numerically stable. Experiment results have verified its superiority over existing algebraic error based algorithms in terms of accuracy# as well as its advantages when used to initialize geometric error based algorithms.,Yinqiang Zheng*# Tokyo Institute of Technology; Shigeki Sugimoto# Tokyo Institute of Technology; Masatoshi Okutomi# Tokyo Institute of Technology,zheng@ok.ctrl.titech.ac.jp; shige@ok.ctrl.titech.ac.jp; mxo@ctrl.titech.ac.jp,05.10 Structure from Motion*,05.10 Structure from Motion*,,,, ,Pose and Photometry,P 2A
1377,CLAM: Coupled Localization and Mapping with Efficient Outlier Handling,We describe a method to efficiently generate a model (map) of small-scale objects from video. The map encodes sparse geometry as well as coarse photometry# and could be used to initialize dense reconstruction schemes as well as to support recognition and localization of three-dimensional objects. Self-occlusions and predominance of outliers present a challenge to existing online Structure From Motion and Simultaneous Localization and Mapping systems. We propose a unified inference criterion that encompasses map building and localization (object detection) relative to it in a coupled fashion. We establish correspondence in a computationally efficient way without resorting to combinatorial matching or random-sampling techniques. Instead# we use a simpler M-estimator that exploits putative correspondence from tracking after photometric and topological validation. We have collected a new dataset to benchmark model building in the small scale# which will be  distributed upon completion of the blind review process. Although our system is significantly leaner than previous ones# it compares favorably to the state of the art in terms of accuracy and robustness.,Jonathan Balzer*# UCLA; Stefano Soatto# UCLA,balzer@cs.ucla.edu; soatto@cs.ucla.edu,05.10 Structure from Motion*,05.10 Structure from Motion*,,,, ,Pose and Photometry,P 2A
622,Inductive Hashing on Manifolds,   Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space.  Manifold learning techniques# in contrast# are better able to model the intrinsic structure  embedded in the original high-dimensional data.   The complexity of these models# and the problems with out-of-sample data# have previously rendered them unsuitable for application to large-scale embedding# however.      In this work# we consider how to learn compact binary embeddings on their  intrinsic manifolds. In order to address the above-mentioned difficulties#  we describe an efficient# inductive solution to the out-of-sample data problem# and a process by which non-parametric manifold may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE manifolds outperforms state-of-the-art hashing methods on large-scale benchmark datasets# and is very effective for image classification with very short code lengths. ,Fumin Shen*# NUST; Chunhua Shen# The University of Adelaide; Qinfeng Shi# University of Adelaide; Anton van den Hengel# The University of Adelaide; Zhenmin Tang# NJUST,fumin.shen@gmail.com; chunhua.shen@adelaide.edu.au; shiqinfeng@gmail.com; anton.vandenhengel@adelaide.edu.au; tang.zm@mail.njust.edu.cn,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,01.01 Image processing,02.03 Feature descriptors,, ,Methods and Retrieval,P 2B
284,Hash Bit Selection: a Unified Solution for Selection Problems in Hashing,Recent years have witnessed the attractive development of hashing techniques for nearest neighbor search.  However# to apply hashing techniques successfully# there are various kinds of selection problems in practice# e.g.# selecting features# hashing algorithms# parameter settings# kernels# etc. In this work# we unify all these selection problems into a hash bit selection framework# i.e.# selecting the most informative hash bits from a pool of candidate bits generated by different types of hashing methods using different feature spaces and/or parameter settings# etc. We represent the bit pool as a vertex- and edge-weighted graph with the candidate bits as vertices. The vertex weight represents the bit quality in terms of similarity preservation# and the edge weight reflects independence (non-redundancy) between bits. Then we formulate the bit selection problem as a quadratic programming on the graph# and solve it efficiently by replicator dynamics. Moreover# a theoretical study is provided to reveal the nature/intuition of our solution: the selected bits actually are the normalized dominant set of the candidate bit graph. Extensive experiments are conducted for three important selection scenarios when applying hash techniques# i.e.# hashing with multiple features# multiple hashing algorithms# and multiple bit hashing# on several large-scale benchmarks. We demonstrate that our bit selection approach can achieve superior performance over both naive selection methods and state-of-the-art hashing methods under each scenario# usually with significant accuracy gains from 10% to 50% relatively.,Xianglong Liu*# Beihang University; Junfeng He# Columbia University; Bo Lang# Beihang University; Shih-Fu Chang# Columbia University,xlliuchina@gmail.com; jh2700@columbia.edu; langbo@nlsde.buaa.edu.cn; sfchang@ee.columbia.edu,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,02.05 Feature matching and indexing,,, ,Methods and Retrieval,P 2B
312,All about VLAD,The objective of this paper is large scale object instance retrieval# given a query image. A starting point of such systems is feature detection and description# for example using SIFT.  The focus of this paper# however# is towards very large scale retrieval where# due to storage requirements# very compact image descriptors are required and no information about the original SIFT descriptors can be accessed directly at run time.  We start from VLAD# the state-of-the art compact descriptor introduced by Jegou et al. for this purpose# and make three novel contributions: first# we show that a simple change to the normalization method significantly improves retrieval performance; second# we show that vocabulary adaptation can substantially alleviate problems caused when images are added to the dataset after initial vocabulary learning. These two methods set a new state-of-the-art over all benchmarks investigated here: Oxford 5k and Holidays for both mid-dimensional (20k-D to 30k-D) and small (128-D) descriptors; and for the larger scale Oxford 105k and Holidays+Flickr1M benchmarks for small (128-D) descriptors.  Our third contribution is a multiple spatial VLAD representation# MultiVLAD# that allows the retrieval and localization of objects that only extend over a small part of an image (again without requiring use of the original image SIFT descriptors).,Relja Arandjelovic*# Oxford; Andrew Zisserman# Oxford,relja@robots.ox.ac.uk; az@robots.ox.ac.uk,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,02.05 Feature matching and indexing,,, ,Methods and Retrieval,P 2B
1949,Binary Code Ranking with Weighted Hamming Distance,Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods# the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The calculation of Hamming distance is efficient# however# in practice# there are often a large number of points sharing the same Hamming distance to a query# which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important.In this paper# we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of most existing binary hashing methods. By assigning different bit-level weights to different hash bits# the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weights for each hash bit. Evaluations on two large-scale image data sets containing up to one million points demonstrate the efficacy of our weighted Hamming distance for ranking.,Lei  Zhang*# ICT# CAS# China; Yongdong Zhang# ICT# CAS# China; Xiaoguang Gu# ICT# CAS# China; Qi Tian# University of Texas# San Antonio,zhanglei09@ict.ac.cn; zhyd@ict.ac.cn; xggu@ict.ac.cn; qitian@cs.utsa.edu,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,02.05 Feature matching and indexing,,, ,Methods and Retrieval,P 2B
33,Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds,"Propagating similarity information along the data manifold requires careful selection of local neighborhood. Selecting a ""good"" neighborhood in an unsupervised setting# given an affinity graph# has been a difficult task. The most common way to select a local neighborhood has been to use the k-nearest neighborhood (k-NN) selection criterion. However# it has the tendency to include noisy edges. In this paper# we propose a way to select a robust neighborhood using the consensus of multiple rounds of k-NNs. We explain how using consensus information can give better control over neighborhood selection. We also explain in detail the problems with another recently proposed neighborhood selection criteria# i.e.# Dominant Neighbors# and show that our method is immune to those problems. Finally# we show the results from experiments in which we compare our method to other neighborhood selection approaches. The results corroborate our claims that consensus of k-NNs does indeed help in selecting more robust and stable localities.",Vittal Premachandran*# NanyangTechnologicalUniversity; Ramakrishna Kakarala# NTU,vittalp@pmail.ntu.edu.sg; ramakrishna@ntu.edu.sg,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,03.05 Shape Representation and Matching,,, ,Methods and Retrieval,P 2B
2024,Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior,A topical video object refers to an object that is frequently highlighted in a video. It could be# e.g.# the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models# such as Latent Dirichelet Allocation (LDA)# for video object discovery often takes a bag-of-visual-words representation# which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior# which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale# view-point# color and lighting changes# or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.,gangqiang Zhao*# ; Junsong Yuan# Nanyang Technological University; Gang Hua# Stevens Institute of Technology,GQZHAO@NTU.EDU.SG; jsyuan@ntu.edu.sg; ganghua@gmail.com,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,07.05 Instance recognition,07.06 Object Detection,07.07 Object Recognition, ,Methods and Retrieval,P 2B
1280,Query Adaptive Similarity for Large Scale Object Retrieval,Many recent object retrieval systems rely on local features for describing an image. The similarity between a pair of images is measured by aggregating the similarity between their corresponding local features. In this paper we present a probabilistic framework for modeling the feature to feature similarity measure. We then derive a query adaptive distance which is appropriate for global similarity evaluation. Furthermore# we propose a function to score the individual contributions into an image to image similarity within the probabilistic framework. Experimental results show that our method improves the retrieval accuracy significantly and consistently. Moreover# our result compares favorably to the state-of-the-art.,Danfeng Qin*# ETH Zurich; Christian Wengert# Computer Vision Lab#ETH Zurich; Luc  Van Gool# Computer Vision Lab#ETH Zurich,qind@vision.ee.ethz.ch; wengert@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,07.07 Object Recognition,,, ,Methods and Retrieval,P 2B
1145,Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions,Though widely utilized for facilitating image management# user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images# resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper# we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column denoting a tag# LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity# considering image-image similarity# image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting unlabelled semantically related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the rationality and effectiveness of the proposed LSR.,Zijia Lin*# Tsinghua University; Guiguang Ding# Tsinghua University; Mingqing Hu# ,linzijia07@tsinghua.org.cn; dinggg@tsinghua.edu.cn; humingqing@ict.ac.cn,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,,,, ,Methods and Retrieval,P 2B
1315,Lp-norm IDF for Large Scale Image Search,   The Inverse Document Frequency (IDF) is prevalently utilized in the Bag-of-Words based image search. The basic idea is to assign less weight to terms with high frequency# and vice versa.  However# the estimation of visual word frequency is coarse and heuristic. Therefore# the effectiveness of the conventional IDF routine is marginal# and far from optimal. To tackle this problem# this paper introduces a novel IDF expression by the use of Lp-norm pooling technique. Carefully designed# the proposed IDF takes into account the term frequency# document frequency# the complexity of images# as well as the codebook information. Optimizing the IDF function towards optimal balancing between TF and pIDF weights yields the so-called Lp-norm IDF (pIDF). We show that the conventional IDF is a special case of our generalized version# and two novel IDFs# i.e. the average IDF and the max IDF# can also be derived from our formula. Further# by counting for the term-frequency in each image# the proposed Lp-norm IDF helps to alleviate the visual word burstiness phenomenon.     Our method is evaluated through extensive experiments on three benchmark datasets (Oxford 5K# Paris 6K and Flickr 1M). We report a performance improvement of as large as 27.1% over the baseline approach. Moreover# since the Lp-norm IDF is computed offline# no extra computation or memory cost is introduced to the system at all.,Liang Zheng# Tsinghua University; Shengjin Wang*# Tsinghua University; Ziqiong Liu# Tsinghua University; Qi Tian# University of Texas# San Antonio,zheng-l06@mails.tsinghua.edu.cn; wgsgj@tsinghua.edu.cn; liuziqiong@ocrserv.ee.tsinghua.edu.cn; qitian@cs.utsa.edu,07.04 Image and Video Retrieval*,07.04 Image and Video Retrieval*,,,, ,Methods and Retrieval,P 2B
156,Constraints as Features,In this paper# we introduce a new approach to constrained clustering which treats the constraints as features. Our method augments the original feature space with additional dimensions# each of which derived from a given Cannot-link constraints. The speci?ed Cannot-link pair gets extreme coordinates values# and the rest of the points get coordinate values that express their spatial in?uence from the speci?ed constrained pair. After augmenting all the new features# a standard unconstrained clustering algorithm can be performed# like k-means or spectral clustering. We demonstrate the ef?cacy of our method for active semi-supervised learning applied to image segmentation and compare it to alternative methods. We also evaluate the performance of our method on the four most commonly evaluated datasets from the UCI machine learning repository.,Shmuel Asafi*# Tel Aviv University; Daniel Cohen Or# Tel Aviv University,shmulikasafi@gmail.com; cohenor@gmail.com,08.01 Learning# statistics# and inference*,08.01 Learning# statistics# and inference*,"03.03 Image segmentation""",,, ,Methods and Retrieval,P 2B
1799,Learning Manifolds as Connected Linear Subspaces,  We present a novel method for manifold learning based upon multiple  model fitting. Unlike existing formulations# it directly  characterises the manifold as a set of connected linear subspaces in  the embedding space# without unwrapping it into a lower dimensional  space. This direct formulation of a manifold# allows us to learn  closed manifolds of interest to vision# such as those corresponding  to gait cycles or camera pose.  We report state of the art results  for manifold based nearest neighbour classification on vision  datasets# and show how the same techniques can be applied to the 3D  reconstruction of human motion from a single image.,Chris Russell*# ; Lourdes Agapito# ; Nikolaos Pitelis# qmul.ac.uk,chrisr@eecs.qmul.ac.uk; lourdes@dcs.qmul.ac.uk; nikolaos.pitelis@eecs.qmul.ac.uk,08.01 Learning# statistics# and inference*,08.01 Learning# statistics# and inference*,"05.01 3D modeling and reconstruction""",,, ,Methods and Retrieval,P 2B
888,Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video,We propose a novel approach to boost the performance of generic object detectors on videos by learning video-specific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features# which can be learned to build better detectors.  Unlike many supervised detector adaptation or detection-by-tracking methods# our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector# then iteratively learn new video-specific features and refine the detection scores.  In order to learn discriminative and compact features# we propose a new feature learning method using a deep neural network based on auto encoder. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously# which gives our features better discriminative ability; second# our learned features are more compact# while the unsupervised feature learning methods usually learn  a redundant set of over-complete features.  Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.,Yang Yang*# UCF; Guang Shu# University of Central Florida,yangyoung1@gmail.com; thesg2008@gmail.com,08.01 Learning# statistics# and inference*,08.01 Learning# statistics# and inference*,,,, ,Methods and Retrieval,P 2B
1620,Efficient# Fully-Connected CRF Inference with Data-Dependent# Non-Parametric Pairwise Potentials,Conditional Random Fields (CRFs) are used for diverse tasks# ranging from image denoising to object recognition. For images# they are commonly defined as a graph with nodes corresponding to individual pixels and pairwise links that connect nodes to their immediate neighbors. Recent work has shown that fully-connected CRFs# where each node is connected to every other node# can be solved efficiently under the restriction that the pairwise term is a Gaussian kernel over a Euclidean feature space.  In this paper# we generalize the pairwise terms to a non-linear dissimilarity measure that is not required to be a distance metric. To this end# we use an efficient embedding technique to estimate an approximate Euclidean feature space# in which the pairwise term can still be expressed as a Gaussian kernel. We demonstrate that the use of non-parametric models for the pairwise interactions# conditioned on the input data# greatly increases the expressive power whilst maintaining the efficient inference.,Neill Campbell*# UCL; Kartic Subr# University College London; Jan Kautz# UCL,n.campbell@cs.ucl.ac.uk; kartic@gmail.com; j.kautz@ucl.ac.uk,08.01 Learning# statistics# and inference*,08.01 Learning# statistics# and inference*,,,, ,Methods and Retrieval,P 2B
1650,Discriminative Sub-categorization,The objective of this work is to learn sub-categories. Rather than casting this simply as a problem of unsupervised clustering# we investigate a weakly supervised approach using both positive and negative samples of the category.  We make the following contributions: (i) we introduce a new model for discriminative sub-categorization which determines cluster membership for positive samples whilst simultaneously learning a max-margin classifier to separate each cluster from the negative samples; (ii) we show that this model does not suffer from the degenerate cluster problem that afflicts several competing methods (e.g.# Latent SVM (LSVM) and Max-Margin Clustering (MMC)); (iii) we show that the method is able to discover interpretable sub-categories in various datasets.  The model is evaluated experimentally over several UCI datasets# and its performance advantages over k-means and Latent SVM are demonstrated. We also stress test the model and show its resilience in discovering sub-categories as the parameters are varied.,Minh Hoai*# ; Andrew Zisserman# Oxford,minhhoai@robots.ox.ac.uk; az@robots.ox.ac.uk,08.01 Learning# statistics# and inference*,08.01 Learning# statistics# and inference*,,,, ,Methods and Retrieval,P 2B
1538,Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow,For problems over continuous random variables# MRFs with large cliques pose a challenge in probabilistic inference. Difficulties in performing optimization efficiently have limited the probabilistic models explored in computer vision and other fields. One inference technique that handles large cliques well is Expectation Propagation. EP offers run times independent of clique size# which instead depend only on the rank# or intrinsic dimensionality# of potentials. This property would be highly advantageous in computer vision. Unfortunately# for grid-shaped models common in vision# traditional Gaussian EP requires quadratic space and cubic time in the number of pixels.  Here# we propose a variation of EP that exploits regularities in natural scene statistics to achieve run times that are linear in both number of pixels and clique size. We test these methods on shape from shading# and we demonstrate strong performance not only for Lambertian surfaces# but also on arbitrary surface reflectance and lighting arrangements# which requires highly non-Gaussian potentials. Finally# we use large# non-local cliques to exploit cast shadow# which is traditionally ignored in shape from shading.,Brian Potetz*# ; Mohammadreza Hajiarb# University of Kansas,potetz@ittc.ku.edu; mhajiarb@ittc.ku.edu,08.03 Belief propagation*,08.03 Belief propagation*,05.07 Shape from shading and specularities,,, ,Methods and Retrieval,P 2B
234,Fast Approximate Energy Minimization with Learned State Filters,Pairwise discrete energies defined over graphs are ubiquitous in computer vision. Many algorithms have been proposed to minimize such energies# often concentrating on sparse graph topologies or specialized classes of pairwise potentials. However# when the graph is fully connected and the pairwise potentials are arbitrary# the complexity of even approximate minimization algorithms such as TRW-S grows quadratically both in the number of nodes and in the number of states a node can take. Moreover# recent applications are using more and more computationally expensive pairwise potentials. These factors make it very hard to employ fully connected models. In this paper we propose a novel# generic algorithm to approximately minimize any discrete pairwise energy function. Compared to existing methods# it efficiently handles fully connected graphs# with many states per node# and arbitrary pairwise potentials# which might be expensive to compute. We demonstrate experimentally on two applications that our algorithm is much more efficient than other generic minimization algorithms such as TRW-S# while returning essentially identical solutions.  ,Matthieu Guillaumin*# ETH Zurich; Luc Van Gool# KU Leuven; Vittorio Ferrari# University of Edinburgh,guillaumin@vision.ee.ethz.ch; Luc.VanGool@esat.kuleuven.be; ferrari@vision.ee.ethz.ch,08.05 Markov Random Fields*,08.05 Markov Random Fields*,03.03 Image segmentation,07.06 Object Detection,08.03 Belief propagation, ,Methods and Retrieval,P 2B
1362,Bilinear Programming for Human Activity Recognition with unknown MRF graphs,Markov Random Fields (MRFs) have been successfully applied to human activity modelling# largely due to their ability to model complex dependencies and deal with local uncertainty.  However# the underlying graph structure is often manually specified# or automatically constructed by heuristics.  We show# instead# that learning an MRF graph and performing MAP inference can be achieved simultaneously by solving a bilinear program. Equipped with the bilinear program based MAP inference for an unknown graph# we show how to estimate parameters efficiently and effectively with a latent structural SVM.  We apply our techniques to predict sport moves (such as \emph{serve}# \emph{volley} in tennis) and human activity in TV episodes (such as \emph{kiss}# \emph{hug} and \emph{Hi-Five}). Experimental results show the proposed method outperforms the state-of-the-art. ,Zhenhua Wang*# The University of Adelaide; Qinfeng Shi# University of Adelaide; Chunhua Shen# The University of Adelaide; Anton van den Hengel# The University of Adelaide,zhenhua.wang01@adelaide.edu.au; shiqinfeng@gmail.com; chunhua.shen@adelaide.edu.au; anton.vandenhengel@adelaide.edu.au,08.05 Markov Random Fields*,08.05 Markov Random Fields*,07.01 Recognition,08.01 Learning# statistics# and inference,"08.04 Graphical models""", ,Methods and Retrieval,P 2B
1227,A higher-order CRF model for road network extraction,The aim of this work is to extract the road network from aerial images. What makes the problem challenging is the complex structure of the prior: roads form a connected network of smooth# thin segments which meet at junctions and crossings. This type of a-priori knowledge is more difficult to turn into a tractable model than standard smoothness or co-occurrence assumptions. We develop a novel CRF formulation for road labeling# in which the prior is represented by higher-order cliques that connect sets of  superpixels along straight line segments. These long-range cliques have asymmetric robust P^N-potentials# which express a preference to assign all rather than just some of their constituent superpixels to the road class. Thus# the road likelihood is amplified for thin chains of superpixels# while the CRF is still amenable to optimization with graph cuts. Since the number of such  liques of arbitrary length is huge# we furthermore propose a sampling scheme which concentrates on those cliques which are most relevant for the optimization. In experiments on two different databases the model significantly improves both the per-pixel accuracy and the topological correctness of the extracted roads# and outperforms both a simple smoothness prior and heuristic completion based on the same rules as the clique sampling.,Jan  Wegner*# ETH Zurich; Javier Alexander Montoya Zegarra# ETHZ; Konrad Schindler# ,jan.wegner@geod.baug.ethz.ch; javier.montoya@geod.baug.ethz.ch; schindler@geod.baug.ethz.ch,08.05 Markov Random Fields*,10.01 Aerial and outdoor image analysis and modeling*,07.03 Context and scene understanding,,, ,Methods and Retrieval,P 2B
276,Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques,This paper introduces an efficient approach to integrating non-local statistics into the higher-order Markov Random Fields (MRFs) framework. Motivated by the observation that many non-local statistics (e.g.# shape priors# color distributions) can usually be represented by only a few parameters# we reformulate the higher-order MRF problem by introducing additional latent variables to represent the intrinsic dimensions of the higher-order cliques. The resulting new formulation automatically decomposes the energy function into less coupled terms# facilitating the design of efficient algorithms for maximum a posteriori (MAP) inference. Based on this novel modeling/inference framework# we achieve state-of-the-art solutions to the challenging problems of class-specific image segmentation and template-based 3D facial expression tracking.,Yun Zeng*# Harvard; Chaohui Wang# UCLA; Shing-Tung Yau# Department of Mathematics# Harvard University,yzeng@fas.harvard.edu; ch.wang@cs.ucla.edu; yau@math.harvard.edu,08.05 Markov Random Fields*,08.05 Markov Random Fields*,08.06 Optimization Methods,09.03 Face detection and head tracking,, ,Methods and Retrieval,P 2B
1843,Fast Trust Region for Segmentaiton,Trust region is a well-known general approach to optimization which   offers many advantages over standard gradient descent techniques. In   particular# it allows more accurate nonlinear approximation models.   In each iteration this approach computes a global optimum of a   suitable approximation model within a fixed radius around the current solution# a.k.a.~trust region.  In general# this approach can be used only when some efficient constrained   optimization algorithm is available for the selected non-linear   (more accurate) approximation model.    In this paper we propose a Fast Trust Region (FTR) approach for   optimization of segmentation energies with non-linear   regional terms# which are known to be challenging for existing   algorithms. These energies include# but are not limited to# KL   divergence and Bhattacharyya distance between the observed and the   target appearance distributions# volume constraint on segment size#   and shape prior constraint in a form of $L^2$ distance from target   shape moments. Our method is 10 to 400 times faster than the   existing state-of-the-art methods while converging to comparable or   better solutions.,Lena Gorelick*# University of West Ontario; Frank Schmidt# UniversitÃ© Paris Est; Yuri Boykov# ,lenagorelick@gmail.com; f.schmidt@esiee.fr; yuri@csd.uwo.ca,08.06 Optimization Methods*,08.06 Optimization Methods*,03.01 Segmentation and 2D shape,03.03 Image segmentation,, ,Methods and Retrieval,P 2B
712,Optimal Geometric Fitting Under the Truncated $L_2$-Norm,This paper is concerned with model fitting in the presence of noise and outliers. Previously it has been shown that the number of outliers can be minimized with polynomial complexity in the number of measurements. This paper improves on these results in two ways. First# it is shown that for a large class of problems# the statistically more desirable truncated $L_2$-norm can be optimized with the same complexity. Then# with the same methodology# it is shown how to transform multi-model fitting into a purely combinatorial problem---with worst-case complexity that is polynomial in the number of measurements# though exponential in the number of models.  We apply our framework to a series of hard registration and stitching problems demonstrating that the approach is not only of theoretical interest. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers for fitting problems with low-dimensional models.,Erik Ask*# Lund University; Olof Enqvist# Lund University; Fredrik Kahl# ,erikask@maths.lth.se; olofe@maths.lth.se; fredrik@maths.lth.se,08.06 Optimization Methods*,08.06 Optimization Methods*,04.02 Image alignment,04.03 Image stitching,, ,Methods and Retrieval,P 2B
550,In Defense of 3D-Label Stereo,It is commonly believed that higher order smoothness should be modeled using higher order interactions.  For example# 2nd order derivatives for deformable (active) contours are represented by triple cliques. Similarly# the 2nd order regularization methods in stereo predominately use MRF models with scalar (1D) disparity labels  and triple clique interactions. In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels# e.g. tangent planes.  This general paradigm has been criticized in the community due to perceived computational complexity  of optimization in higher-dimentional label space. Contrary to popular beliefs# we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization  problems with (nearly) submodular pariwise interactions. Our theoretical and experimental results demonstrates advantages over state-of-the-art methods for 2nd order smoothness stereo. ,Carl Olsson*# ; Johannes Ulen# Lund University; Yuri Boykov# ,calle@maths.lth.se; ulen@maths.lth.se; yuri@csd.uwo.ca,08.06 Optimization Methods*,08.06 Optimization Methods*,05.05 Multi-view stereo,,, ,Methods and Retrieval,P 2B
389,Universality of the Local Marginal Polytope,We show that solving the LP relaxation of a MAP inference problem in graphical models (also known as the min-sum problem# energy minimization# or weighted constraint satisfaction) is not easier that solving any LP. More precisely# any polytope is linear-time representable by a local marginal polytope and any LP can be reduced in linear time to a linear optimization (allowing infinite weights) over a local marginal polytope.,Daniel Prusa*# Czech Technical University; Tomas Werner# ,prusapa1@cmp.felk.cvut.cz; werner@cmp.felk.cvut.cz,08.06 Optimization Methods*,08.06 Optimization Methods*,08.04 Graphical models,08.05 Markov Random Fields,, ,Methods and Retrieval,P 2B
899,Continuous Inference in Graphical Models with Polynomial Energies,In this paper# we tackle the problem of  performing inference in graphical models whose energy is a polynomial function of continuous variables. Our energy minimization method follows a dual decomposition approach# where the global problem is split into subproblems defined over the graph cliques. The optimal solution to these subproblems is obtained by making use of a polynomial system solver. Our algorithm inherits the global convergence guarantees of dual decomposition. To speed up optimization# we also introduce a variant of this algorithm based on the augmented Lagrangian method. Our experiments illustrate the diversity of computer vision problems that can be expressed with polynomial energies# and demonstrate the benefits of our approach over existing continuous inference methods. ,Mathieu Salzmann*# NICTA,mathieu.salzmann@nicta.com.au,08.06 Optimization Methods*,08.06 Optimization Methods*,08.04 Graphical models,,, ,Methods and Retrieval,P 2B
969,Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization,Discrete graphical models (also known as discrete Markov random fields) are a major conceptual tool to model the structure of optimization problems in computer vision. While in the last decade research has focused on fast approximative methods# algorithms that provide globally optimal solutions have come more into the research focus in the last years. However# large scale computer vision problems seemed to be out of reach for such methods.  In this paper we introduce a promising way to bridge this gap based on roof duality and novel structural properties of the underlying problem factorization. Combining these preprocessing steps# we are able to solve grids of size 2048 x 2048 in less than 90 seconds. On the hitherto unsolvable Chinese character dataset of Nowozin et. al. we obtain provably optimal results in 56% of the instances and achieve competitive runtimes on other recent benchmark problems.,Joerg Kappes*# Heidelberg University; Markus Speth# ; Gerhard Reinelt# ; Christoph Schnoerr# U Heidelberg,kappes@math.uni-heidelberg.de; markus.speth@informatik.uni-heidelberg.de; gerhard.reinelt@informatik.uni-heidelberg.de; schnoerr@math.uni-heidelberg.de,08.06 Optimization Methods*,08.06 Optimization Methods*,08.04 Graphical models,08.05 Markov Random Fields,, ,Methods and Retrieval,P 2B
1944,Non-smooth Non-convex Optimization for Computer Vision,Natural image statistics indicate that we should use non-convex norms for most regularization tasks in image processing and computer vision. Still# they are rarely used in practice due to the challenge to optimize them. Recently# iteratively reweighed l1 minimization has been proposed as a way to tackle a class of non-convex functions by solving a sequence of convex l2-l1 problems. Here we extend the problem class to linearly constrained optimization of a Lipschitz continuous functions# which is the sum of a convex function and a concave# increasing function. This allows to apply the algorithm to many computer vision tasks.  We show the effect of non-convex regularizers on image denoising# deconvolution# optical flow# and depth map fusion. Non-convexity is particularly interesting in combination with total generalized variation and learned image priors. Efficient optimization is made possible by some important properties that are shown to hold.,Peter Ochs*# University of Freiburg; Alexey Dosovitskiy# University of Freiburg; Thomas Pock# ; Thomas Brox# University of Freiburg,ochs@cs.uni-freiburg.de; dosovits@cs.uni-freiburg.de; pock@icg.tugraz.at; brox@cs.uni-freiburg.de,08.06 Optimization Methods*,08.06 Optimization Methods*,08.07 Regularization,,, ,Methods and Retrieval,P 2B
2137,A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles,"In this paper we propose the first effective automated# Genetic Algorithm (GA)-based jigsaw puzzle solver. We introduce a novel procedure of merging two ""parent"" solutions to an improved ""child"" solution by detecting# extracting# and combining correctly assembled puzzle segments. The solver proposed exhibits state-of-the-art performance solving previously attempted puzzles faster and far more accurately# and also puzzles of size never before attempted. Other contributions include the creation of a benchmark of large images# previously unavailable. We share the data sets and all of our results for future testing and comparative evaluation of jigsaw puzzle solvers.",Dror Sholomon*# Bar-Ilan University; Omid David# Bar-Ilan University; Nathan Netanyahu# Bar-Ilan University,dror.sholomon@gmail.com; mail@omiddavid.com; nathan@cs.biu.ac.il,08.06 Optimization Methods*,10.07 Additional applications*,,,, ,Methods and Retrieval,P 2B
572,A convex regularizer for reducing color artifact in color image recovery,We propose a new convex regularizer# named the local color nuclear norm (LCNN)# for color image recovery. LCNN is designed to promote a specific feature# known as color lines# of color distribution in each local region of natural color images# and hence it is useful for reducing color artifact. Moreover# the proximity operator of LCNN is available with reasonable computational cost# so that we can efficiently incorporate LCNN into convex optimization problems formulated for various types of color image recovery. As an effective use of LCNN# we formulate a convex optimization problem for color image recovery# where LCNN and the color total variation are to be suppressed simultaneously# and it is solved by the proposed optimization scheme based on a primal-dual splitting algorithm. Numerical examples demonstrate that LCNN sufficiently reduces color artifact# resulting in recovery of high quality color images.,Shunsuke Ono*# Tokyo Institute of Technology; Isao Yamada# Tokyo Institute of Technology,ono@sp.ss.titech.ac.jp; isao@sp.ss.titech.ac.jp,08.07 Regularization*,08.07 Regularization*,01.04 De-blurring and super-resolution,01.08 Image enhancement# restoration# and denoising,"08.06 Optimization Methods""", ,Methods and Retrieval,P 2B
997,Extrinsic Classification for Riemannian Manifolds Using Kernel Learning,In computer vision applications# the features often lie on Riemannian manifolds. Popular learning algorithms such as discriminant analysis# partial least squares# support vector machines# etc.# are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence# classification is often performed in an extrinsic manner by embedding the manifolds in Euclidean spaces using kernels. However# for kernel based approaches# poor choice of kernel often results in reduced performance. In this paper# we address the issue of kernel-selection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically# for the SVM classifier# we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.,Raviteja Vemulapalli*# University of Maryland; Jai Shanker Pillai# University of Maryland# College Park; Rama Chellappa# UMD,raviteja@umd.edu; jsp@umiacs.umd.edu; rama@umiacs.umd.edu,08.07 Regularization*,08.07 Regularization*,07.01 Recognition,07.02 Category recognition,08.01 Learning# statistics# and inference, ,Methods and Retrieval,P 2B
971,Deep Learning Shape Priors for Object Segmentation,In this paper we introduce a new shape-driven approach for object segmentation. Given a training set of shapes# we first use deep Boltzmann machine to learn the hierarchical architecture of shape priors. This learned hierarchical architecture is then used to model shape variations of global and local structures in an energetic form. Finally# it is applied to data-driven variational methods to perform object extraction of corrupted data based on shape probabilistic representation. Experiments demonstrate that our model can be applied to dataset of arbitrary prior shapes# and can cope with image noise and clutter# as well as partial occlusions.,Fei Chen*# Zhejiang University; Huimin Yu# Zhejiang University; Roland Hu# Zhejiang University; Xunxun Zeng# Fuzhou University,chenfei314@zju.edu.cn; yhm2005@zju.edu.cn; haoji_hu@zju.edu.cn; xunxun@fzu.edu.cn,03.01 Segmentation and 2D shape*,03.01 Segmentation and 2D shape*,03.03 Image segmentation,,, ,Segmentation and Shape,P 2C
2031,Learning Ensemble of Local PDM-based Regressions,Statistical shape models# such as Active Shape Models# suffer from two major limitations: (i) inability to represent a large range of variations of a complex shape# and (ii) accounting for the errors in detection. In this paper# we propose a novel method that overcomes these limitations by using an ensemble of regression models of varying degree of locality to locate each point individually. The final location of a point is determined as a linear combination of the outputs of the regression models using a set of non-negative coefficients that are learned from the training set. Point distribution model was used as the regression model whose input is a set of selected reference points. The reference points are selected automatically from the training data based on their saliency and they may not all necessarily belong to the shape model. Thus# our method exploits additional appearance evidence while reducing the incorrect appearance guidance. Each model uses a different subset of reference points as input to provide varying degree of local shape constraints. The use of multiple models of different degree of locality improves the ability of our method to capture a larger range of shape variations. We demonstrate the superiority of our method compared to other approaches on a challenging problem of segmenting gene expression images of mouse brains.,Ioannis Kakadiaris*# ; Uday Kurkure# University of Houston; Yen Le# ,ioannisk@uh.edu; ukurkure@uh.edu; yenhle@cs.uh.edu,03.01 Segmentation and 2D shape*,03.01 Segmentation and 2D shape*,03.03 Image segmentation,,, ,Segmentation and Shape,P 2C
324,Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus ,We study the problem of multiple contour completion under side constraints. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired).  We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of nearly 1000 images# our experiments show that a small amount of side knowledge can give significant improvements over fully unsupervised contour completion methods. ,Jia Xu*# UW-Madison; Maxwell Collins# ; Vikas Singh# ,jiaxu@cs.wisc.edu; mcollins@cs.wisc.edu; vsingh@biostat.wisc.edu,03.01 Segmentation and 2D shape*,03.01 Segmentation and 2D shape*,03.04 Segmentation and Grouping,,, ,Segmentation and Shape,P 2C
1855,Recovering Line-networks in Images by Junction-Point Processes,The automatic extraction of line-networks from images is a well-known computer vision issue. Important efforts have been put on appearance and shape considerations to improve accuracy in presence of occlusions# shadows# and a wide variety of irrelevant objects. However most of existing works have ignored the structural aspect of the problem.  We present an original method which provides structurally-coherent solutions. Contrary to the pixel-based and object-based methods# our result is a graph in which each node represents either a connexion or an ending in the line-network. Based on stochastic geometry# we develop a new family of point processes consisting in sampling junction-points in the input image by using a Monte Carlo mechanism. The quality of a configuration is measured by a probability density which takes into account both image consistency and geometric priors. Our experiments on a variety of problems illustrate the potential of our approach in terms of accuracy# flexibility and efficiency with respect to state-of-the-art methods.,Dengfeng Chai# Zhejiang University; Wolfgang Forstner# University Bonn; Florent Lafarge*# ,chaidf@zju.edu.cn; wf@ipb.uni-bonn.de; florent.lafarge@inria.fr,03.01 Segmentation and 2D shape*,03.01 Segmentation and 2D shape*,07.06 Object Detection,08.05 Markov Random Fields,10.01 Aerial and outdoor image analysis and modeling, ,Segmentation and Shape,P 2C
327,Image Matting with Local and Nonlocal Smooth Priors,In this paper we propose a novel alpha matting method with local and nonlocal smooth priors. We observe the manifold preserving editing propagation [3] essentially introduced a nonlocal smooth prior on the alpha matte. This nonlocal smooth prior and the well known local smooth prior from matting Laplacian compliment each other. So we combine them with a simple data term from color sampling in a graph model for nature image matting. Our method has a closed-form solution and can be solved efficiently. Compared to the state-of-the-art methods# our method produces more accurate results according to the evaluation on standard benchmark datasets.,Xiaowu Chen# Beihang University; Dongqing Zou*# Beihang University; Ping Tan# ,chen@buaa.edu.cn; zoudq@vrlab.buaa.edu.cn; eletp@nus.edu.sg,03.03 Image segmentation*,03.03 Image segmentation*,01.01 Image processing,,, ,Segmentation and Shape,P 2C
1117,Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation,Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper# we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically# we first extrac- t graphlets from each image# which are small-sized graphs naturally extending the non-structural superpixel set homo- geneity. Then# a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter# we use GMM to learn the dis- tribution of the embedded graphlets. Finally# we propose a novel image segmentation algorithm# called graphlet cut# that leverages the learned graphlet distribution in measur- ing the homogeneity of spatially structured superpixel sets. Experimental results show that the proposed approach out- performs state-of-the-art weakly supervised image segmen- tation methods# and is competitive against fully supervised segmentation models.,Luming Zhang# ; Mingli Song*# Zhejiang University; Zicheng Liu# ; Xiao Liu# ; Jiajun Bu# ; Chun Chen# ,zglumg@zju.edu.cn; brooksong@zju.edu.cn; zliu@microsoft.com; ender_liux@zju.edu.cn; bjj@zju.edu.cn; chenc@zju.edu.cn,03.03 Image segmentation*,03.03 Image segmentation*,03.01 Segmentation and 2D shape,03.04 Segmentation and Grouping,, ,Segmentation and Shape,P 2C
1900,Towards Fast and Accurate Segmentation,  In this paper we explore approaches for accelerating segmentation   and edge detection algorithms based on the gPb framework. The   paper characterizes the performance of a simple but effective edge   detection scheme which can be computed rapidly and offers   performance that is competitive with the pB detector. The paper also   describes an approach for computing a reduced order normalized cut   that captures the essential features of the original problem but can   be computed in less than half a second on a standard computing   platform. ,Camillo Taylor*# ,cjtaylor@cis.upenn.edu,03.03 Image segmentation*,03.03 Image segmentation*,03.01 Segmentation and 2D shape,03.04 Segmentation and Grouping,, ,Segmentation and Shape,P 2C
99,Discriminative Re-Ranking of Diverse Segmentations,We introduce a hybrid# two-stage approach to semantic image segmentation. In the first stage a probabilistic CRF model generates a set of diverse hypothesized segmentations. In the second stage a discriminatively trained  reranking model selects the best segmentation from the set. The feed-forward reranking stage can use much more complex features than what could be efficiently used in the CRF# allowing a better exploration of solution space than possible with just the MAP inference from CRF. On VOC 2012 test segmentation data set our approach achieves accuracy of above 48\%# higher than any previously reported result. ,Payman Yadollahpour# TTI-Chicago; Dhruv Batra*# Virginia Tech; Gregory Shakhnarovich# TTIC,pyadolla@ttic.edu; dbatra@vt.edu; greg@ttic.edu,03.03 Image segmentation*,03.03 Image segmentation*,03.04 Segmentation and Grouping,08.01 Learning# statistics# and inference,08.04 Graphical models, ,Segmentation and Shape,P 2C
240,MsLRR: Segment Images via Internal Replication Prior, In this work# we present an efficient multi-scale low-rank representation (MsLRR) for image segmentation. Our method begins with  partitioning the input images into a set of superpixels# followed by seeking the optimal superpixel-pair affinity matrix# both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises#  we propose to infer the low-rank refined affinity matrix. The inference is guided  by two observations on natural images. First# looking into a single image# local small-size image patterns tend to recur frequently within the same semantic region# but may not appear in semantically different regions. We call this internal image statistics as \emph{replication prior}# and quantitatively justify it on real image databases. Second# the affinity matrices at different scales should be consistently solved# which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. The proposed representation can be used for both unsupervised or supervised image segmentation tasks. Our experiments demonstrate the presented method can substantially improve segmentation accuracy. ,Xiao Liu*# Hust.edu.cn,xbliu.cgcl@gmail.com,03.03 Image segmentation*,03.03 Image segmentation*,03.04 Segmentation and Grouping,,, ,Segmentation and Shape,P 2C
721,Unsupervised Joint Object Discovery and Segmentation in Internet Images,We present a new unsupervised algorithm to discover and segment out common objects from large and diverse image collections. In contrast to previous co-segmentation methods# our algorithm performs well even in the presence of significant amounts of noise images (not containing a common object)# as typical for datasets collected from internet search queries. The key insight to our algorithm is that common object patterns should be \emph{salient within each image}# while being \emph{sparse} with respect to smooth transformations \emph{across other images}. We propose to use dense correspondences between images to capture the sparsity and visual variability of the common object over the entire database# which enables us to ignore noise objects that may be salient within their own images but do not commonly occur in others. We performed extensive numerical ground truth evaluations. In addition to established co-segmentation datasets we tested our algorithm on several new datasets# generated using internet search# with significantly higher noise levels.,Michael Rubinstein*# MIT; Armand Joulin# Inria; Ce Liu# Microsoft Research New England; Johannes Kopf# ,mrub@mit.edu; armand.joulin@ens.fr; celiu@microsoft.com; kopf@microsoft.com,03.03 Image segmentation*,03.03 Image segmentation*,03.04 Segmentation and Grouping,04.02 Image alignment,, ,Segmentation and Shape,P 2C
1022,Ensemble Video Object Cut in Highly Dynamic Scenes,We consider video object cut as an ensemble of frame-level background-forground object classifiers which fuses information across frames and refine their segmentation results in a collaborative and iterative manner. Our approach addresses the challenging issues of modeling of background with dynamic textures and segmentation of foreground objects from cluttered scenes. We construct patch-level bag-of-words background models to effectively capture the background motion and texture dynamics. We propose a foreground salience graph (FSG)  to characterize the similarity of an image patch to the bag-of-words background models in the temporal domain and to neighboring image patches in the spatial domain. We incorporate this similarity information into a graph-cut energy minimization framework for foreground object segmentation. The background-foreground segmentation results at neighboring frames are fused together to construct a foreground probability map to update the graph weights. The resulting object shapes at neighboring frames are also used as contraints to guide the energy minimization process during graph cut. Our extensive experimental results and performance comparisons over a diverse set of challenging videos with dynamic scenes# including the new Change Detection Challenge Dataset#  demonstrate that the proposed ensemble video object cut method outperforms various state-of-the-art algorithms.,Xiaobo Ren# ; Tony Han# ; Zhihai He*# University of Missouri,xr7rf@mail.missouri.edu; hantx@missouri.edu; hezhi@missouri.edu,03.03 Image segmentation*,03.03 Image segmentation*,03.04 Segmentation and Grouping,09.08 Video Surveillance,, ,Segmentation and Shape,P 2C
1736,Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation,The proposed approach is based on standard graph transduction# semi-supervised learning (SSL) framework. Its key novelty is the integration of global connectivity constraints into this framework. Although connectivity leads to higher order constraints and their number is an exponential# finding the most violated connectivity constraint can be done efficiently in polynomial time. Moreover# each such constraint can be represented as a linear inequality. Based on this fact# we design a cutting-plane algorithm to solve the integrated problem. It iterates between solving a convex quadratic problem of label propagation with linear inequality constraints# and finding the most violated constraint. We demonstrate the benefits of the proposed approach on a realistic and very challenging problem of cosegmentation of multiple foreground objects in photo collections in which the foreground objects are not present in all photos. The obtained results not only demonstrate performance boost induced by the connectivity constraints# but also show a significant improvement over the state-of-the-art methods.,Tianyang Ma*# Temple University; Longin Jan Latecki# Temple University,ma.tianyang@gmail.com; latecki@temple.edu,03.03 Image segmentation*,03.03 Image segmentation*,03.04 Segmentation and Grouping,,, ,Segmentation and Shape,P 2C
1095,Top-down Segmentation of Non-rigid Visual Objects using Derivative-based Search on Sparse Manifolds,The solution for the top-down segmentation of non-rigid visual objects using machine learning techniques is generally regarded as too complex to be solved in its full generality given the large dimensionality of the search space of the explicit representation of the segmentation contour. In order to reduce this complexity# the problem is usually divided into two stages: rigid detection and non rigid segmentation. The rationale is based on the fact that the rigid detection can be run in a lower dimensionality space (i.e.# less complex and faster) than the original contour space# and its result is then used to constrain the non-rigid segmentation. In this paper# we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection search space of current state-of-the-art top-down segmentation methodologies. The main goals targeted by this smaller dimensionality search space are the decrease of the search running time complexity and the reduction of the training complexity of the rigid detector.  These goals are attainable given that both the search and training complexities are function of the dimensionality of the rigid search space. We test our approach in the segmentation of the left ventricle from ultrasound images and lips from frontal face images. Compared to the performance of state of-the-art non-rigid segmentation system# our experiments show that the use of sparse manifolds for the rigid detection leads to the two goals mentioned above.,Jacinto Nascimento# Instituto Superior Tecnico; Gustavo Carneiro*# ,jan@isr.ist.utl.pt; carneiro.gustavo@gmail.com,03.03 Image segmentation*,03.03 Image segmentation*,07.06 Object Detection,,, ,Segmentation and Shape,P 2C
2002,A Principled Deep Random Field Model for Image Segmentation,We propose a model for image segmentation that is able to overcome the short-boundary bias observed in standard pairwise random field based approaches. We show that a random field with multi-layered hidden units can encode boundary preserving higher order potentials such as the ones used in the cooperative cuts model of Jegelka & Bilmes while still allowing for fast and exact MAP inference. Exact inference allows our model to outperform previous image segmentation methods# and to see the true effect of coupling graph edges. Finally# our model can be easily extended to handle segmentation instances with multiple labels# for which it shows promising results.,Pushmeet Kohli*# MSR; Anton Osokin# Moscow State University; Stefanie Jegelka# ,pkohli@microsoft.com; anton.osokin@gmail.com; stefje@eecs.berkeley.edu,03.03 Image segmentation*,03.03 Image segmentation*,08.01 Learning# statistics# and inference,"08.05 Markov Random Fields""",, ,Segmentation and Shape,P 2C
1606,Background Modeling based on Bidirectional Analysis,Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes# the background model has been enhanced by introducing various forms of information including spatial consistency and temporal tendency. In this paper# we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling# and performs background subtraction based on bidirectional analysis; i.e.# analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is taken from a future period# our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore# the memory cost can be reduced by about 65% relative to typical background modeling. ,Atsushi Shimada*# Kyushu University; Hajime Nagahara# Kyushu University; RIn-ichiro Taniguchi# Kyushu University,atsushi@limu.ait.kyushu-u.ac.jp; nagahara@ait.kyushu-u.ac.jp; rin@ait.kyushu-u.ac.jp,03.03 Image segmentation*,03.03 Image segmentation*,09.08 Video Surveillance,,, ,Segmentation and Shape,P 2C
69,Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets,We propose a working set based approximate subgradient descent algorithm to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks# such as the structured SVM. We focus on the setting of general graphical models# such as loopy MRFs and CRFs commonly used in image segmentation# where exact inference is intractable and the most violated constraints can only be approximated# voiding the optimality guarantees of the structured SVMÂ’s cutting plane algorithm as well as reducing the robustness of existing subgradient based methods. We show that the proposed method obtains better subgradient estimates through the use of working sets# leading to improved convergence properties and increased reliability. Furthermore# our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts# which can be used to reduce learning time at only a small cost of performance. We demonstrate the strength of our method empirically on the segmentation of electron microscopic imagery as well as the popular MSRC data set and show state-of-the-art results. ,Aurelien Lucchi*# EPFL; Yunpeng Li# EPFL; Pascal Fua# EPFL,aurelien.lucchi@epfl.ch; yunpeng.li@epfl.ch; pascal.fua@epfl.ch,03.03 Image segmentation*,03.03 Image segmentation*,10.03 Medical Image Analysis,,, ,Segmentation and Shape,P 2C
1698,A Sentence is Worth a Thousand Pixels,We are interested in holistic scene understanding where images are accompanied with text in the form of complex sentential descriptions.   Such a scenario is typically available on the net# and in domains such as human-robot interaction. We propose a holistic conditional random field  model for semantic parsing  which reasons jointly about which objects  are present in the scene# their spatial extent as well as semantic segmentation# and employs text as well as image information as input. We automatically parse the sentences and extract objects and their relationships# and incorporate them into the model# both via potentials as well as by re-ranking  candidate detections. We demonstrate the effectiveness of our approach in the challenging UIUC sentences dataset  and show segmentation  improvements  of $12.5\%$ over the visual only model and detection improvements of $5\%$ AP over deformable part-based models.,Sanja Fidler# TTI chicago; Abhishek Sharma# University of Maryland; Raquel Urtasun*# TTI Chicago,fidler@ttic.edu; abhisharayiya@gmail.com; rurtasun@ttic.edu,03.03 Image segmentation*,03.03 Image segmentation*,,,, ,Segmentation and Shape,P 2C
905,Recurring Pattern Discovery ,We propose a novel unsupervised method for discovering recurring patterns in single or multiple images. Different from classic pairwise matching methods# a key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence.  We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and controlled synthesis image sets.,Yanxi Liu*# Penn State University; Jingchen Liu# Penn. State Univ.,yanxi@cse.psu.edu; jingchen@cse.psu.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,02.01 Feature extraction and matching,07.01 Recognition,07.06 Object Detection, ,Segmentation and Shape,P 2C
526,Image Segmentation by Cascaded Region Allglomeration,We propose a hierarchical segmentation algorithm that starts with a very fine oversegmentation and gradually merges regions using a cascade of boundary classifiers. This approach allows the weights of region and boundary features to adapt to the segmentation scale at which they are applied. The stages of the cascade are trained sequentially# with asymetric loss to maximize boundary recall. On six segmentation data sets# our algorithm achieves best performance under most region-quality measures# and does it with fewer segments than the prior work. Our algorithm is also highly competitive in a dense oversegmentation (superpixel) regime under boundary-based measures.,Zhile Ren# Zhejiang University; Gregory Shakhnarovich*# TTIC,jrenzhile@gmail.com; greg@ttic.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,02.02 Edge and contour detection,03.01 Segmentation and 2D shape,03.03 Image segmentation, ,Segmentation and Shape,P 2C
1674,Think Globally and Locally: Hybrid Models for Segmentation and Labeling,Conditional random fields are a powerful tool for building models to segment natural images. They are particularly appropriate for modeling local interactions among labels in a segmentation. Complementary to these are restricted Boltzmann machines (RBMs) and their extensions which have been used to model global shapes produced by segmentation models. In this work we present a new model that uses the combined power of these two types of networks to build a state-of-the-art labeler for the parts of complex face images. Specifically# we tackle the problem of segmenting the Labeled Faces in the Wild (LFW) data set into hair# skin and background. The CRF provides a good baseline labeler on its own# but we show how an RBM can be added to the architecture to provide a globlal shape bias that complements the local boundary modeling provided by the CRF. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high quality segmentation results for LFW# we demonstrate that the hidden unit activations in the RBM portion of our model can be interpreted as face attributes which have been learned without any attribute-specific training data.,Andrew Kae*# UMass Amherst; Kihyuk Sohn# University of Michigan# Ann Arbor; Honglak Lee# ; Erik Learned-Miller# UMass Amherst,akae@cs.umass.edu; kihyuks@umich.edu; honglak@eecs.umich.edu; elm@cs.umass.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,03.01 Segmentation and 2D shape,08.01 Learning# statistics# and inference,08.04 Graphical models, ,Segmentation and Shape,P 2C
724,Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds,Unsupervised over-segmentation of an image into regions of perceptually similar pixels# known as superpixels# is a widely used preprocessing step in segmentation algorithms. Superpixel methods reduce the number of regions that must be considered later by more computationally expensive algorithms# with a minimal loss of information. Nevertheless# as some information is inevitably lost# it is vital that superpixels not cross object boundaries# as such errors will propagate through later steps. Existing methods make use of projected color or depth information# but do not consider three dimensional geometric relationships between observed data points which can be used to prevent superpixels from crossing regions of empty space. We propose a novel over-segmentation algorithm which uses voxel relationships to produce over-segmentations which are fully consistent with the spatial geometry of the scene in three dimensional# rather than projective# space. Enforcing the constraint that segmented regions must have spatial connectivity prevents label flow across semantic object boundaries which might otherwise be violated. Additionally# as the algorithm works directly in 3D space# observations from several calibrated RGB+D cameras can be segmented jointly. Experiments on a large data set of human annotated RGB+D images demonstrate a significant reduction in occurrence of clusters crossing object boundaries# while maintaining speeds comparable to state-of-the-art 2D methods.,Jeremie Papon*# University of Goettingen; Alexey Abramov# Uni. Goettingen; Florentin Woergoetter# Uni. Goettingen; Markus Schoeler# Uni. Goettingen,jpapon@gmail.com; abramov@physik3.gwdg.de; worgott@physik3.gwdg.de; mschoeler@physik3.gwdg.de,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,03.03 Image segmentation,,, ,Segmentation and Shape,P 2C
1081,SCALPEL: Segmentation CAscades with Localized Priors and Efficient Learning ,In this work# we propose SCALPEL: a method for incorporating region features normally reserved for a re-ranker directly into the segmentation process. The key argument behind our approach is that a purely bottom-up attempt at greedy segmentation is doomed to failure; instead# we incorporate high-level information about the scale# probable layout# class of the object# and the current stage of the segmentation into the bottom-up procedure. We show how to jointly and efficiently train class- and scale-specific cascades of greedy segmentation models. Unlike a fixed model# our cascades are capable of learning to ignore boundaries early on in the process yet use them as a stopping criterion once the object has reached a certain size. When paired with a novel system that generates better localized shape priors than our competitors# our method leads to a concise# accurate set of roughly 650 proposals per image {\em without} re-ranking; these proposals are more accurate on the PASCAL VOC2010 dataset than state-of-the-art methods that use re-ranking to filter much larger bags of proposals.,David Weiss*# University of Pennsylvania; Ben Taskar# ,djweiss@cis.upenn.edu; taskar@cis.upenn.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,03.03 Image segmentation,07.07 Object Recognition,08.09 Statistical Methods and Learning, ,Segmentation and Shape,P 2C
1337,Submodular Salient Region Detection,The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function# which maximizes the total similarities (i.e.# total profits) between the hypothesized salient region centers (i.e.# facility locations) and their region elements (i.e.# clients)# and penalizes the number of potential salient regions (i.e.# the number of open facilities). The similarities are efficiently computed by finding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objective function# a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e ? 1)/e ? 0.632-approximation to the optimum. Experimental results demonstrate that our approach outperforms several recently proposed saliency detection approaches.,Zhuolin Jiang*# University of Maryland; Larry Davis# ,zhuolin@umiacs.umd.edu; lsd@umiacs.umd.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,03.03 Image segmentation,07.06 Object Detection,, ,Segmentation and Shape,P 2C
98,Moxels - A Movie Element Video Representation,In this paper# we develop a generative probabilistic model for temporally consistent super pixels in video sequences. Our method explicitly models the flow between frames and uses this information to partition the frames into a super pixel segmentation. While many algorithms must be tuned for particular video sequences# we only require the user to specify the approximate number of super pixels desired per frame. A new metric is introduced that measures how well super pixels are tracked.  Additionally# we demonstrate the capabilities of the algorithm by obtaining state-of-the-art results with a very simple object tracker built on top of the tracked super pixels.,Jason Chang*# MIT; Donglai Wei# ; John Fisher III# MIT,jchang7@csail.mit.edu; donglai@csail.mit.edu; fisher@csail.mit.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,04.01 Motion estimation and alignment,,, ,Segmentation and Shape,P 2C
1101,Pose from Flow and Flow from Pose,We develop an algorithm for estimating human body pose and dense body motion in monocular video. Human pose detectors# although successful in localizing faces and torsos of people# often fail in lower arms due to their surrounding clutter# or uncommon posture. Motion is a powerful cue for limb detection# however large frame to frame deformations and foreshortening# combined with clutter# of- ten render motion estimates inaccurate. We build a joint segmentation-detection algorithm that mediates the information between body parts recognition# and multi-frame motion grouping of point trajectories. Motion of body parts# though not accurate# is often sufficient to segment them from their backgrounds. Such segmentations are crucial for extracting hard to detect body parts out of their interior body clutter. By matching these segments to exemplars we obtain pose labeled body segments. The pose labeled segments and corresponding articulated joints are used to improve the motion flow fields by proposing kinematically constrained affine displacements on body parts. The pose based articulated motion model is shown to handle large limb rotations and displacements. Our algorithm can detect people under rare poses# frequently missed by pose detectors# showing the benefits of jointly reasoning about pose# segmentation and motion in videos.  ,Katerina Fragkiadaki*# University of Pennsylvania; Han Hu# Tsinghua University; jianbo shi# Upenn,katef@seas.upenn.edu; huh04@mails.tsinghua.edu.cn; jshi@seas.upenn.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,04.05 Object Tracking and Motion Analysis,,, ,Segmentation and Shape,P 2C
1282,Mesh Based Semantic Modelling for Indoor and Outdoor Scenes,Semantic reconstruction of a scene is important for a variety of applications such as 3D modelling# recognition and autonomous robotic navigation. However# most object labelling methods work in the image domain and fail to capture the information present in the 3D space. In this work we propose a principled way to generate object labelling in 3D. Our method builds a triangulated meshed representation of the scene from multiple depth estimates. We define a CRF over the mesh# which is able to capture the consistency of geometric properties of the objects present in the scene. In this framework# we are able to generate object hypotheses by combining information from multiple sources: geometric properties (from 3D mesh)# and appearance properties (from images). We demonstrate the robustness of our framework in both indoor and outdoor scenes. We create an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. For outdoor scenes# we create ground truth object labellings for the KITTI odometry dataset (stereo image sequence). We observe a significant speed-up in the inference stage by performing labelling on the mesh# and additionally achieve higher accuracies.,Sunando Sengupta*# Oxford Brookes University; Julien Valentin# Oxford Brookes University; Jonathan Warrell# Oxford Brookes Vision Group; Ali Shahrokni# 2d3 Sensing ; Philip Torr# Oxford Brookes University,ssengupta@brookes.ac.uk; julien.valentin-2010@brookes.ac.uk; jwarrell@brookes.ac.uk; ali.shahrokni@2d3sensing.co.uk; philiptorr@brookes.ac.uk,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,05.01 3D modeling and reconstruction,07.03 Context and scene understanding,07.07 Object Recognition, ,Segmentation and Shape,P 2C
915,Weakly-Supervised Bi-Clustering for Image Semantic Segmentation,In this paper# we propose a novel Weakly-Supervised Bi-Clustering (WSBC) approach for image semantic segmentation with image-level labels# i.e.# collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that pixels or superpixels belonging to an object class (e.g.# dog# tiger or sky) usually exist across multiple images and hence can be gathered via the idea of clustering. In WSBC# spectral clustering as an unsupervised form is adopted to cluster the superpixels obtained from a set of over segmented images. At the same time# a multi-class classifier as a kind of discriminative clustering is learned to exploit the discriminative information among different object classes. During the bi-clustering process# we reqiure the both clustering outputs to be consistent as much as possible. Besides# a weakly-supervised constraint from image-level tags is imposed to restrict the labeling information of superpixels. At last# the non-convex and non-smooth objective function can be efficiently optimized using an iterative CCCP procedure. Extensive experiments on MSRC and LabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.,Yang Liu*# NLPR#CASIA; Jing Liu# ; Zechao Li# ; Hanqing Lu# NLPR,liuyang6@nlpr.ia.ac.cn; jliu@nlpr.ia.ac.cn; zcli@nlpr.ia.ac.cn; luhq@nlpr.ia.ac.cn,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,07.04 Image and Video Retrieval,,, ,Segmentation and Shape,P 2C
575,Salient object detection: a discriminative regional feature integration approach,Salient object detection has been attracting a lot of interest# and recently various heuristic computational models have been designed. In this paper# we regard saliency map computation as a regression problem. Our method# which is based on multi-level image segmentation# uses the supervised learning approach to map the regional feature vector to a saliency score# and finally fuses the saliency scores across multiple levels# yielding the saliency map. The contributions lie in two-fold. One is that we introduce a new regional feature vector# backgroundness# to characterize the background# which can be regarded as a counterpart of the objectness descriptor [2]. The other is that we show our approach# which integrates the regional contrast# regional property and regional backgroundness descriptors together to form the master saliency map# is able to produce superior saliency maps to most existing algorithms which combine saliency maps heuristically computed from different types of features. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.,Huaizu Jiang# Xi'an Jiaotong University; Jingdong Wang*# Microsoft Research Asia; Zejian Yuan# Xi'an Jiaotong University; Yang Wu# Academic Center for Computing and Media Studies# Kyoto University; Nanning Zheng# Xi'an Jiaotong University,jianghuaizu@gmail.com; jingdw@microsoft.com; yuan.ze.jian@mail.xjtu.edu.cn; yangwu@mm.media.kyoto-u.ac.jp; nnzheng@mail.xjtu.edu.cn,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,07.06 Object Detection,,, ,Segmentation and Shape,P 2C
541,Revisiting Depth Layers from Occlusions,In this work# we consider images of a scene with a moving object captured by a static camera. As the object (human or otherwise) moves about the scene# it reveals pairwise depth-ordering or occlusion cues. The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. In this paper# we cast the problem of depth-layer segmentation as a discrete labeling problem on a spatio-temporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently# and using a naive combination of the features.,Adarsh Kowdle*# Cornell University; Andrew Gallagher# ; Tsuhan Chen# ,apk64@cornell.edu; andrew.c.gallagher@gmail.com; tsuhan@ece.cornell.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,08.05 Markov Random Fields,09.09 Video Analysis and Event Recognition,, ,Segmentation and Shape,P 2C
1171,Hierarchical Video Representation with Trajectory Binary Partition Tree,As early stage of video processing# we introduce an iterative trajectory merging algorithm that produces a region-based and hierarchical representation of the video sequence# called the Trajectory Binary Partition Tree (BPT). From this representation# many analysis and graph cut techniques can be used to extract partitions or objects that are useful in the context of specific applications.   In order to define trajectories and to create a precise merging algorithm# color and motion cues have to be used. Both types of informations are very useful to characterize objects but present strong differences of behavior in the spatial and the temporal dimensions. On the one hand# scenes and objects are rich in their spatial color distributions# but these distributions are rather stable over time. Object motion# on the other hand# presents simple structures and low spatial variability but may change from frame to frame. The proposed algorithm takes into account this key difference and relies on different models and associated metrics to deal with color and motion information. We show that the proposed algorithm outperforms existing hierarchical video segmentation algorithms and provides more stable and precise regions.,Guillem Palou*# Technical University of Catalo; Philippe Salembier# Technical University of Catalonia,guillem.palou@upc.edu; philippe.salembier@upc.edu,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,09.09 Video Analysis and Event Recognition,,, ,Segmentation and Shape,P 2C
426,Discriminative Subspace Clustering,   We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces# called  Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data# and their results are combined together in an ensemble# in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness# and also of low computational complexity.,Vasileios Zografos*# Linkoping University; Liam Ellis# ; Rudolf Mester# ,zografos@isy.liu.se; liam.ellis@liu.se; mester@vsi.cs.uni-frankfurt.de,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,,,, ,Segmentation and Shape,P 2C
519,PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors,Driven by recent vision and graphics applications such as image segmentation and object recognition# assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often# such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these# we propose a generic and fast computational framework called PISA -- Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment# our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping# feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures# which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner# the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post-relaxation# PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA's superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.,Keyang Shi# Sun Yat-Sen University; Keze Wang# Sun Yat-Sen University; Jiangbo Lu# Advanced Digital Sciences Cent; Liang Lin*# Sun Yat-Sen University,shiky1989@hotmail.com; kezewang@gmail.com; jiangbo.lu@adsc.com.sg; linliang@ieee.org,03.04 Segmentation and Grouping*,03.04 Segmentation and Grouping*,,,, ,Segmentation and Shape,P 2C
1089,Boundary detection benchmarking: beyond F-Measures,For an ill-posed problem like boundary detection# human labeled datasets play a critical role.  Compared to the active research on finding a better boundary detector to refresh the performance record# there is surprisingly little discussion on the boundary detection benchmark itself.  The goal of this paper is to identify the potential pitfalls of today's most popular boundary benchmark# BSDS 300.  In the paper# we first introduce a psychophysical experiment to show that many of the ``weak'' boundary labels are unreliable and may contaminate the benchmark.  Then we analyze the benchmarking protocol that encourages an algorithm to bias towards those problematic ``weak'' boundary labels.  With this evidence# we focus on a new problem of detecting strong boundaries as one alternative.  Finally# we assess the performances of 9 major algorithms on different ways of utilizing the dataset# suggesting new directions for improvements.,Xiaodi Hou*# Caltech; Alan Yuille# UCLA; Christof Koch# Caltech,xiaodi.hou@gmail.com; yuille@stat.ucla.edu; koch.christof@gmail.com,10.04 Performance Evaluation and Databases*,10.04 Performance Evaluation and Databases*,02.02 Edge and contour detection,03.03 Image segmentation,03.04 Segmentation and Grouping, ,Segmentation and Shape,P 2C
1796,Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation,This paper tackles the supervised evaluation of image segmentation algorithms. First# it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures# it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion# this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.,Jordi Pont-Tuset*# UPC; Ferran Marques# UPC,jordi.pont@upc.edu; ferran.marques@upc.edu,10.04 Performance Evaluation and Databases*,10.04 Performance Evaluation and Databases*,03.03 Image segmentation,,, ,Segmentation and Shape,P 2C
1872,Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problem ,The analysis of 3-D shape meshes is a fundamental problem in computer vision# graphics# and medical imaging. Frequently# the needs of the application require that our analysis  take a multi-resolution view of the shape's local and global topology# and that the solution is consistent across multiple scales. Unfortunately# the preferred mathematical construct which offers this behavior in classical image/signal processing# Wavelets# is no longer applicable in this  general setting (data with non-uniform topology). In particular# the traditional definition does not allow writing out an expansion for graphs that do not correspond to the uniformly sampled lattice (e.g.# images). In this paper# we adapt recent results in harmonic analysis# to derive Non-Euclidean Wavelets based algorithms for a range of shape analysis problems in vision and medical imaging. We show how descriptors derived from the dual domain representation offer native multi-resolution behavior for characterize local/global topology around vertices. With only minor modifications# the framework yields a method for extracting interest/key points from shapes# a surprisingly simple algorithm for 3-D shape segmentation (competitive with state of the art)# and a method for surface alignment (without landmarks). We give an extensive set of comparison results on a large shape segmentation benchmark and derive a uniqueness theorem for the surface alignment problem.,Won Hwa Kim*# University of Wisconsin ; Moo K. Chung# ; Vikas Singh# ,wonhwa@cs.wisc.edu; mkchung@wisc.edu; vsingh@biostat.wisc.edu,03.05 Shape Representation and Matching*,03.05 Shape Representation and Matching*,01.10 Multi-scale processing,05.11 Three-dimensional modeling and manipulation,10.03 Medical Image Analysis, ,Segmentation and Shape,P 2C
688,Robust Estimation of Nonrigid Transformation for Point Set Registration,We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration# feature descriptors such as shape context are used to establish rough correspondence. In the second step# we estimate the transformation using a robust estimator called $L_2E$. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space# more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-of-the-art methods# particularly when there are a large number of outliers. Moreover# our method of robustly estimating transformations from correspondences is general and has many other applications.,Jiayi Ma*# Huazhong Univ. of Sci. & Tech.; Ji Zhao# CMU; Alan Yuille# UCLA; Zhuowen Tu# ,jyma2010@gmail.com; zhaoji84@gmail.com; yuille@stat.ucla.edu; zhuowen.tu@gmail.com,03.05 Shape Representation and Matching*,03.05 Shape Representation and Matching*,02.05 Feature matching and indexing,,, ,Segmentation and Shape,P 2C
463,Efficient Computation of Shortest Path-Concavity for 3D Meshes,In the context of shape segmentation and retrieval object-wide distributions of measures are needed to accurately evaluate and compare local regions of shapes. Lien et al. [14] proposed two point-wise concavity measures in the context of Approximate Convex Decompositions of polygons measuring the distance from a point to the polygonÂ’s convex hull: an accurate Shortest Path-Concavity (SPC) measure and a Straight Line-Concavity (SLC) approximation of the same. While both are practicable on 2D shapes# the exponential costs of SPC in 3D makes it inhibitively expensive for a generalization to meshes [12]. In this paper we propose an efficient and straight forward approximation of the Shortest Path-Concavity measure to 3D meshes. Our approximation is based on discretizing the space between mesh and convex hull# thereby reducing the continuous Shortest Path search to an efficiently solvable graph problem. Our approach works out-of-the-box on complex mesh topologies and requires no complicated handling of genus. Besides presenting a rigorous evaluation of our method on a variety of input meshes# we also define an SPC-based Shape Descriptor and show its superior retrieval and runtime performance compared with the recently presented results on the Convexity Distribution [16] by Rosin et al. ,Henrik Zimmer*# RWTH Aachen University; Marcel Campen# ; Leif Kobbelt# RWTH Aachen University,zimmer@informatik.rwth-aachen.de; campen@informatik.rwth-aachen.de; kobbelt@informatik.rwth-aachen.de,03.05 Shape Representation and Matching*,03.05 Shape Representation and Matching*,03.04 Segmentation and Grouping,,, ,Segmentation and Shape,P 2C
504,Boundary Cues for 3D Object Shape Recovery,"Early work in computer vision considered a host of geometric cues for both shape reconstruction and recognition. However# since then# the vision community has focused heavily on shading cues for reconstruction# and moved towards data-driven approaches for recognition. In this paper# we reconsider these perhaps overlooked ""boundary"" cues (such as self occlusions and folds in a surface)# as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks# we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction# such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection# Lambertian surfaces).",Kevin Karsch*# UIUC; Zicheng Liao# University of Illinois; Jason Rock# UIUC; Jonathan Barron# UC Berkeley; Derek Hoiem# UIUC,karsch1@uiuc.edu; liao17@illinois.edu; jjrock2@illinois.edu; jonbarron@gmail.com; dhoiem@illinois.edu,03.05 Shape Representation and Matching*,03.05 Shape Representation and Matching*,07.07 Object Recognition,,, ,Segmentation and Shape,P 2C
869,A Linear Approach to Matching Cuboids in RGBD Images,We propose a novel linear method to match cuboids in indoor scenes using RGBD images from Kinect. Beyond depth maps# these cuboids reveal important structures of a scene. Instead of directly fitting cuboids to 3D data# we first construct cuboid candidates using superpixel pairs on a RGBD image# and then we optimize the configuration of the cuboids to satisfy the global structure constraints. The optimal configuration has low local matching costs# small object intersection and occlusion# and the cuboids tend to project to a large region in the image; the number of cuboids is optimized simultaneously. We formulate the multiple cuboid matching problem as a mixed integer linear program and solve the optimization efficiently with a branch and bound method. The optimization guarantees the global optimal solution. Our experiments on the Kinect RGBD images of a variety of indoor scenes show that our proposed method is efficient# accurate and robust against object appearance variations# occlusions and strong clutter.,Hao Jiang*# Boston College; Jianxiong Xiao# Massachusetts Institute of Tec,hjiang@cs.bc.edu; jxiao@csail.mit.edu,03.05 Shape Representation and Matching*,03.05 Shape Representation and Matching*,08.06 Optimization Methods,,, ,Segmentation and Shape,P 2C
1014,Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF),With volumetric data from widefield fluorescence microscopy# many emerging questions in biological and bio-medical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy# its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However# the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper# we present a blind deconvolution method that improves results of state-of-the-art deconvolution methods on widefield data by exploiting the properties of the widefield OTF.,Margret Keuper*# University of Freiburg; Thorsten Schmidt# University of Freiburg; Jan Padeken# MPI of Immunobiology ; Maja Temerinac-Ott# ; Patrick Heun# MPI of Immunobiology; Olaf Ronneberger# University of Freiburg; Thomas Brox# University of Freiburg,keuper@cs.uni-freiburg.de; tscmidt@cs.uni-freiburg.de; padeken@immunbio.mpg.de; temerina@cs.uni-freiburg.de; heun@ie-freiburg.mpg.de; ronneber@cs.uni-freiburg.de; brox@cs.uni-freiburg.de,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,01.04 De-blurring and super-resolution,"01.08 Image enhancement# restoration# and denoising""",, ,Motion and Medical Imaging,P 2D
741,Image Understanding from Experts' Eyes by Modeling Perceptual Skills of Diagnostic Reasoning Processes,Eliciting and representing experts' remarkable perceptual capability of locating# identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper# we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding segments of the experts' eye movement sequences. We further specify a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns during diagnosis# we are able to categorize the images from the perspective of experts' viewing strategies. In each category# images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians' diagnostic viewing behaviors informs about medical images' understanding to correct diagnosis.,Rui Li*# Rochester Institute of Tech.,lr8032@gmail.com,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,01.05 Early and Biologically-inspired Vision,07.02 Category recognition,07.03 Context and scene understanding, ,Motion and Medical Imaging,P 2D
153,Adaptive Compressed Tomography Sensing,One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction# with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections# the adaptive model produces higher image quality# when compared with known limited angle# non-adaptive acquisition algorithms.,Oren Barkan*# Tel Aviv University; Amir Averbuch# Tel Aviv University; Shai Dekel# GE healthcare; Jonathan Weill# Tel Aviv University; Yaniv Tenzer# GE healthcare,barkanoren@gmail.com; amir@math.tau.ac.il; shai.dekel@ge.com; jw666jw666@gmail.com; yaniv.tenzer@ge.com,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,02.02 Edge and contour detection,08.08 Sparse coding and dictionaries,, ,Motion and Medical Imaging,P 2D
1963,Classification of Tumor Histology via Morphometric Context,Image-based classification of tissue histology# in terms of different components (e.g.# normal signature# categories of abberrant signatures)# provides a series of indices for tumor composition. Subsequently# aggregation of these indices in each whole slide image (WSI) from a large cohort can pro- vide predictive models of clinical outcome. However# the performance of the existing techniques is hindered as a re- sult of large technical and biological variations that are al- ways present in a large cohort. In this paper# we propose two algorithms for classification of tissue histology based on robust representations of morphometric context# which are built upon nuclear level morphometric features at vari- ous locations and scales within the spatial pyramid match- ing (SPM) framework. These methods have been evaluated on two distinct datasets of different tumor types collected from The Cancer Genome Atlas (TCGA)# and the experi- mental results indicate that our methods are (i) extensible to different tumor types; (ii) robust in the presence of wide technical and biological variations; (iii) invariant to differ- ent nuclear segmentation strategies; and (iv) scalable with varying training sample size. In addition# our experiments suggest that enforcing sparsity# during the construction of morphometric context# further improves the performance of the system.,Hang Chang*# Lawrence Berkeley National Lab; Alexander Borowsky# UC Davis; Paul Spellman# OHSU# Portland# Oregon; Bahram Parvin# Lawrence Berkeley National Lab,hchang@lbl.gov; adborowsky@ucdavis.edu; spellmap@ohsu.edu; b_parvin@lbl.gov,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,02.03 Feature descriptors,07.01 Recognition,07.02 Category recognition, ,Motion and Medical Imaging,P 2D
1155,Efficient 3D Endfiring TRUS Prostate Segmentation with Globally Optimized Rotational Symmetry,Segmenting 3D endfiring transrectal ultrasound (TRUS) prostate images efficiently and accurately is of utmost importance for the planning and guiding 3D TRUS guided prostate biopsy. Poor image quality and imaging artifacts of 3D TRUS images often introduce a challenging task in computation to directly extract the 3D prostate surface. In this work# we propose a novel global optimization approach to delineate 3D prostate boundaries using its rotational resliced images around a specified axis# which properly enforces the inherent rotational symmetry of prostate shapes  to jointly adjust a series of 2D slicewise segmentations in the global 3D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard# we propose a novel coupled continuous max-flow model# which not only provides a powerful mathematical tool to analyze the proposed optimization problem but also amounts to a new and efficient duality-based algorithm. Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-art methods in terms of efficiency# accuracy# reliability and less user-interactions# and reduces the execution time by a factor of $100$.,Jing Yuan*# Robarts Research Institute; Wu Qiu# Robarts Research Institute; Martin Rajchl# Robarts Research Institute; eranga Ukwatta# Robarts Research Institute; Xue-Cheng Tai# Math Department# Univ. Bergen; Aaron Fenster# Robarts Research Institute,cn.yuanjing@gmail.com; qiu.wu.ch@gmail.com; mrajchl@robarts.ca; eukwatta@imaging.robarts.ca; tai@math.uib.no; afenster@robarts.ca,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,03.01 Segmentation and 2D shape,03.03 Image segmentation,, ,Motion and Medical Imaging,P 2D
1394,Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation,In this paper# we propose a graph-based method for 3D vascular tree structure segmentation based on a new tubularity markov tree model TMT# which works as both new energy function and graph construction method. With the help of power-watershed implementation [7]# a global optimal segmentation can be obtained with low computational cost. Different with all the other graph-based vessel segmentation methods# the proposed method does not depend on any skeleton and ROI extraction method. The classical issues of the graph-based method# such as shrinking bias and the sensitivity to seed point location# can be solved with the proposed method thanks to the vessel data fidelity energy obtained with TMT. The proposed method is compared with some classical graph-based image segmentation methods and two up to date 3D vessel segmentation methods# and is demonstrated to be more accurate than these methods for 3D vessel tree segmentation. The computational cost for the proposed method is low (within 20 seconds for 256*256*144 image) although the whole 3D image is considered.,Ning ZHU*# HKUST; Albert Chung# HKUST,nzhu@cse.ust.hk; achung@cse.ust.hk,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,03.03 Image segmentation,,, ,Motion and Medical Imaging,P 2D
1508,Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso (SCOTO),Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. In this paper# a novel semi-automated prostate segmentation method is presented. Specifically# to segment the prostate in the current treatment image# the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. Then# the prostate is segmented automatically by the proposed two steps: (i) The first step of prostate-likelihood estimation to predict the prostate likelihood for each voxel in the current treatment image# aiming to generate the 3-D prostate-likelihood map by the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) The second step of multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from the planning and previous treatment images. The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset# consisting of 24 patients with 330 images. Moreover# it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification of the first and last slices of the prostate.,Yinghuan Shi*# Nanjing University.; Shu Liao# ; Yaozong Gao# ; Daoqiang Zhang# ; Yang Gao# ; Dinggang Shen# ,yinghuan.shi@gmail.com; liaoshu.cse@gmail.com; yzgao@cs.unc.edu; daoqiangz@gmail.com; gaoy@nju.edu.cn; dgshen@med.unc.edu,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,03.03 Image segmentation,,, ,Motion and Medical Imaging,P 2D
1485,Area Preserving Brain Mapping,Brain mapping transforms the brain cortical surface to canonical planar domains# which plays a fundamental role in morphlogical study. Most existing brain mapping methods are based on angle preserving maps# which may introduce large area distortion. This work proposes an area-preserving brain mapping method based on Monge-Brenier theory. The brain mapping is intrinsic to the Riemannian metric# unique# and diffeomorphic. The computation is equivalent to convex energy minimization and power Voronoi diagram construction. Comparing to the existing approaches based on Monge-Kantorovich theory# the proposed one greatly reduces the complexity (from n^2 unknowns to n )# and improves the simplicity and efficiency.  Experimental results on caudate nucleus surface mapping and cortical surface mapping demonstrate the efficiency and efficacy of the current method. Conventional methods for caudate nucleus surface mapping suffers from numerical instability; in contrast# current method produces diffeomorpic mappings stably. In the study of cortical surface classification for recognition of Alzheimer's Disease (AD) disease# the proposed method  outperforms other statistical methods.,Zhengyu Su*# Stony Brook University; Wei Zeng# School of Computing & Information Sciences# Florida International University; Rui Shi# Department of Computer Science# Stony Brook University; Yalin Wang# Computer Science and Engineering# Arizona State University; Jian Sun# Tsinghua University ; Xianfeng Gu# Department of Computer Science# Stony Brook University,zhsu@cs.stonybrook.edu; wzeng@cs.fiu.edu; rshi@cs.stonybrook.edu; Yalin.Wang@asu.edu; sunjian0813@gmail.com; gu@cs.stonybrook.edu,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,03.05 Shape Representation and Matching,,, ,Motion and Medical Imaging,P 2D
925,Discriminative Brain Effective Connectivity Analysis for AlzheimerÂ’s Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network,Analyzing brain network from neuroimages is becoming a promising approach in identifying novel connectivity based biomarkers for the AlzheimerÂ’s disease (AD). In this regard# brain Â“effective connectivityÂ” analysis# which studies the causal relationship among brain regions# is highly challenging and of many research opportunities. Most of the existing works in this field use generative methods. Despite their success in data representation and other important merits# generative methods are not necessarily discriminative# which may cause the ignorance of subtle but critical disease-induced changes. In this paper# we propose a learning-based approach that integrates the benefits of generative and discriminative methods to recover effective connectivity. In particular# we employ Fisher kernel to bridge the generative models of sparse Bayesian network (SBN)and the discriminative classifiers of SVMs# and convert the SBN parameter learning to Fisher kernel learning via minimizing a generalization error bound of SVMs. Our method is able to simultaneously boost the discrimination power of both the generative SBN models and the SBN-induced SVM classifiers via Fisher kernel. The proposed method is tested on analyzing brain effective connectivity for AD from ADNI data. It demonstrates significant improvements over the state-of-the-art: classification accuracy increased above 10% by our SBN models# and above 16% by our SBN-induced SVM classifiers with a simple feature selection.,Luping  Zhou*# University of Wollongong; Lei Wang# University of Wollongong; lingqiao Liu# Australian National University; Philip Ogunbona# University of Wollongong; Dinggang Shen# ,luping.zhou.jane@gmail.com; leiw@uow.edu.au; lingqiao.liu@cecs.anu.edu.au; philipo@uow.edu.au; dgshen@med.unc.edu,10.03 Medical Image Analysis*,10.03 Medical Image Analysis*,,,, ,Motion and Medical Imaging,P 2D
722,Compressible Motion Fields,Traditional video compression methods obtain a compact representation for image frames by computing coarse motion fields defined on patches of pixels called blocks. This piecewise constant flow approximation reduces the size of the motion field but introduces block artifacts in the warped image frame.   In this paper# we address the problem of estimating dense motion fields that# while accurately predicting one frame from a given reference frame by warping it with the field# are also compressible. We introduce a representation for motion fields based on wavelet bases# and approximate the compressibility of their coefficients with a piecewise smooth surrogate function that yields an objective function similar to classical optical flow formulations. We then show how to quantize and encode such coefficients with adaptive precision.    We demonstrate the effectiveness of our approach by comparing its performance with a state-of-the-art wavelet video encoder. Experimental results on a number of standard flow and video datasets reveal that our method significantly outperforms both block-based and optical-flow-based motion compensation algorithms.  ,Giuseppe Ottaviano*# UniversitÃ  di pisa; Pushmeet Kohli# MSR,ottavian@di.unipi.it; pkohli@microsoft.com,04.01 Motion estimation and alignment*,04.01 Motion estimation and alignment*,01.10 Multi-scale processing,04.06 Optical Flow,08.06 Optimization Methods, ,Motion and Medical Imaging,P 2D
534,Fast Rigid Motion Segmentation via Incrementally-Complex Local Models,The problem of rigid motion segmentation of trajectory data under orthography has been long solved for non-degenerate motions in the absence of noise. But because real trajectory data often incorporates noise# motion degeneracies and motion dependencies# recently proposed motion segmentation methods resort to non-trivial representations to achieve state of the art accuracies# at the expense of a large computational cost. This paper proposes a method that dramatically reduces this cost (by two orders of magnitude) with minimal segmentation accuracy loss (from 98.8\% achieved by the state of the art# to 96.2\% achieved by our method on the standard Hopkins 155 dataset). Computational efficiency comes from the use of a simple but powerful representation of motion that explicitly incorporates mechanisms to deal with noise# outliers and motion degeneracies. Subsets of motion models with the best balance between prediction accuracy and model complexity are chosen from a pool of candidates# which are then used for segmentation. ,Fernando Flores-Mangas*# University of Toronto; Allan Jepson# ,mangas@cs.toronto.edu; jepson@cs.toronto.edu,04.01 Motion estimation and alignment*,04.01 Motion estimation and alignment*,04.05 Object Tracking and Motion Analysis,05.10 Structure from Motion,, ,Motion and Medical Imaging,P 2D
1070,Determining Motion Directly from Normal Flows upon the use of a Spherical Eye Platform,We address the problem of recovering 3D motion from video data# which is not from the full flow interpolated from the apparent flow (normal flow) but from the apparent flow itself directly. We increase the visual field of the imaging system by fixating a number of cameras together to form an approximate spherical eye. With a substantially widened visual field# we discover that estimating the directions of translation and rotation components of the motion separately are possible and particularly efficient. This reduces the complicated 5D motion estimation into two simple sub-problems. In addition# the inherent ambiguities between translation and rotation also disappear. Experimental results on synthetic and real image data are provided. The results not only show that the accuracy of motion estimation is comparable to those of the state-of-the-art methods that require to use explicit feature correspondences or to interpolate the full optical flows# but also a faster computation time.,Tak-Wai Hui*# The Chinese University of HK; Ronald Chung# The Chinese University of HK,twhui1@mae.cuhk.edu.hk; rchung@mae.cuhk.edu.hk,04.01 Motion estimation and alignment*,04.01 Motion estimation and alignment*,10.05 Robot vision,,, ,Motion and Medical Imaging,P 2D
461,Correspondence-Less Non-Rigid Registration of Triangular Surface Meshes,A novel correspondence-less approach is proposed to find a thin plate spline map between a pair of deformable 3D objects represented by triangular surface meshes. The proposed method works without landmark extraction and feature correspondences. The aligning transformation is found simply by solving a system of nonlinear equations. Each equation is generated by integrating a nonlinear function over the object's domains. We derive recursive formulas for the efficient computation of these integrals. Based on a series of comparative tests on a large synthetic dataset# our triangular mesh-based algorithm outperforms state of the art methods both in terms of computing time and accuracy. The applicability of the proposed approach has been demonstrated on the registration of 3D lung CT volumes. ,Zsolt Santa# University of Szeged; Zoltan Kato*# ,santazs@inf.u-szeged.hu; kato@inf.u-szeged.hu,04.02 Image alignment*,04.02 Image alignment*,01.01 Image processing,03.05 Shape Representation and Matching,05.11 Three-dimensional modeling and manipulation, ,Motion and Medical Imaging,P 2D
687,Video Editing with Temporal# Spatial and Appearance Consistency,Given an area of interest in a video sequence# one may want to manipulate or edit the area# e.g. remove occlusions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency# spatial pose# and visual realism. The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment# pose rectification# as well as precise recovery of the occlusion. To make our method applicable to long video sequences# we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames# and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust alignment process# we automatically construct a trimap of the region for each frame# which is used as the input to alpha matting methods to extract the occluding foreground. Extensive experimental results on both simulated and real data demonstrate the accurate and robust performance of our method.,Xiaojie Guo*# Tianjin University; Xiaochun Cao # Institute of Information Engineering# CAS? China; Yi Ma# MSRA,xguo@tju.edu.cn; caoxiaochun@iie.ac.cn; mayi@microsoft.com,04.02 Image alignment*,10.07 Additional applications*,01.01 Image processing,,, ,Motion and Medical Imaging,P 2D
1047,Correlation Filters for Improved Object Alignment,Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model# it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models# there has been relatively less progress in designing robust landmark detection models. In this paper we present an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to overall better shape alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.,Vishnu Naresh Boddeti*# Carnegie Mellon University; Takeo Kanade# Carnegie Mellon University; Vijayakumar Bhagavatula# Carnegie Mellon University,naresh@cmu.edu; tk@cs.cmu.edu; kumar@ece.cmu.edu,04.02 Image alignment*,04.02 Image alignment*,03.01 Segmentation and 2D shape,03.05 Shape Representation and Matching,04.01 Motion estimation and alignment, ,Motion and Medical Imaging,P 2D
1403,Plane-Based Content-Preserving Warps for Video Stabilization,Recently# a new image deformation technique called content-preserving warping (CPW) has been successfully employed to produce the state-of-the-art video stabilization results in many challenging cases. The key insight of CPW is that the true image deformation due to viewpoint change can be well approximated by a carefully constructed warp using a set of sparsely constructed 3D points only. However# since CPW solely relies on the tracked feature points to guide the warping# it works poorly in large textureless regions# such as ground and building interiors. To overcome this limitation# in this paper we present a hybrid approach for novel view synthesis# observing that the textureless regions often correspond to large planar surfaces in the scene. Particularly# given a jittery video# we first segment each frame into piecewise planar regions as well as regions labeled as non-planar using Markov random fields. Then# a new warp is computed by estimating a single homography for regions belong to the same plane# while inheriting results from CPW in the non-planar regions. We demonstrate how the segmentation information can be efficiently obtained and seamlessly integrated into the stabilization framework. Experimental results on a variety of real video sequences verify the effectiveness of our method.,Zihan Zhou*# University of Illinois; Hailin Jin# Adobe; Yi Ma# MSRA,zzhou7@illinois.edu; hljin@adobe.com; mayi@microsoft.com,04.02 Image alignment*,05.04 Image-based Modeling*,04.01 Motion estimation and alignment,04.02 Image alignment,, ,Motion and Medical Imaging,P 2D
603,Deformable Spatial Pyramid Matching for Fast Dense Correspondences,"We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences.  Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels.  Whereas the prevailing approaches operate at the pixel level# we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents---ranging from an entire image# to coarse grid cells# to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations# while the ""deformable"" aspect of our model overcomes the strict rigidity of traditional spatial pyramids.  Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow and PatchMatch)# both in terms of accuracy and run time.",Jaechul Kim*# University of Texas at Austin; Ce Liu# Microsoft Research New England; Fei Sha# University of Southern California; Kristen Grauman# Utexas,jaechul@cs.utexas.edu; celiu@microsoft.com; feisha@usc.edu; grauman@cs.utexas.edu,04.02 Image alignment*,04.02 Image alignment*,07.02 Category recognition,,, ,Motion and Medical Imaging,P 2D
700,The Generalized Laplacian Distance and its Applications for Visual Matching,The graph Laplacian operator# originated in graph theory# is commonly used for learning applications such as spectral clustering and embedding. In this paper we explore the Laplacian distance# a distance function related to the graph Laplacian# and use it for visual search. We show that previous techniques such as Matching by Tone Mapping (MTM) are particular cases of the Laplacian distance. Generalizing the Laplacian distance results in distance measures which are invariant or robust to various visual distortions. A novel# SVD based algorithm makes it possible to compute these generalized distances efficiently by convolution operations. The proposed approach is demonstrated for tone mapping invariant# outlier robust#occlusion robust and multimodal template matching.,Elhanan Elboher*# HUJI; Michael Werman# ; Yacov Hel-Or# idc.ac.il,elhanan.elboher@mail.huji.ac.il; werman@cs.huji.ac.il; toky@idc.ac.il,04.02 Image alignment*,04.02 Image alignment*,07.04 Image and Video Retrieval,,, ,Motion and Medical Imaging,P 2D
439,Groupwise Registration via Graph Shrinkage on the Image Manifold,Recently# groupwise registration has been investigated for simultaneous alignment of all images without selecting any individual image as the template# thus avoiding the potential bias in image registration. However# none of current groupwise registration method fully utilizes the image distribution to guide the registration. Thus# the registration performance usually suffers from large inter-subject variations across individual images. To solve this issue# we propose a novel groupwise registration algorithm for large population dataset# guided by the image distribution on the manifold. Specifically# we first use a graph to model the distribution of all image data sitting on the image manifold# with each node representing an image and each edge representing the geodesic pathway between two nodes (or images). Then# the procedure of warping all images to their population center turns to the dynamic shrinking of the graph nodes along their graph edges until all graph nodes become close to each other. Thus# the topology of image distribution on the image manifold is always preserved during the groupwise registration. More importantly# by modeling the distribution of all images via a graph# we can potentially reduce registration error since every time each image is warped only according to its nearby images with similar structures in the graph. We have evaluated our proposed groupwise registration method on both synthetic and real datasets# with comparison to the two state-of-the-art groupwise registration methods. All experimental results show that our proposed method achieves the best performance in terms of registration accuracy and robustness.,Shihui Ying*# UNC at Chapel Hill; Guorong Wu# ; Qian Wang# University of North Carolina at Chapel Hill; Dinggang Shen# ,Yingshihui@gmail.com; grwu@med.unc.edu; qianwang@cs.unc.edu; dgshen@med.unc.edu,04.02 Image alignment*,04.02 Image alignment*,10.03 Medical Image Analysis,,, ,Motion and Medical Imaging,P 2D
1311,FAsT-Match: Fast Affine Template Matching,FAsT-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation# we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth# the result is a practical affine template matching algorithm with approximation guarantees# that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets# and report very good results. To the best of our knowledge# this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.,Simon Korman*# Tel-Aviv University; Daniel Reichman# Weizmann Institute of Science; Gilad Tsur# Weizmann Institute of Science; Shai Avidan# Tel-Aviv University,simon.korman@gmail.com; daniel.reichman@gmail.com; gilad.tsur@gmail.com; avidan@eng.tau.ac.il,04.02 Image alignment*,04.02 Image alignment*,,,, ,Motion and Medical Imaging,P 2D
927,As-Projective-As-Possible Image Stitching with Moving DLT,We investigate projective estimation under model inadequacies# i.e.# when the underpinning assumptions of the projective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp ---a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice# and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps# i.e.# warps that aim to be globally projective# yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT)# our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching# with significantly reduced ghosting effects# thus lowering the dependency on post hoc deghosting.,Julio Zaragoza*# The University of Adelaide; Tat-Jun Chin# The University of Adelaide; Michael Brown# NUS; David Suter# ,jzaragoza@cs.adelaide.edu.au; tjchin@cs.adelaide.edu.au; brown@comp.nus.edu.sg; dsuter@cs.adelaide.edu.au,04.03 Image stitching*,04.03 Image stitching*,01.01 Image processing,04.02 Image alignment,, ,Motion and Medical Imaging,P 2D
1307,Real-time Model-based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues,We propose a novel model-based method for estimating and tracking the six-degrees-of-freedom (6DOF) pose of rigid objects of arbitrary shapes in real-time. By combining dense motion and stereo cues with sparse keypoint correspondences# and by feeding back information from the model to the cue extraction level# the method is both highly accurate and robust to noise and occlusions. A tight integration of the graphical and computational capability of Graphics Processing Units (GPUs) results in pose updates at framerates exceeding 60 Hz. Since a benchmark dataset that enables the evaluation of stereo-vision-based pose estimators in complex scenarios is currently missing in the literature# we have introduced a novel synthetic benchmark dataset with varying objects# background motion# noise and occlusions. Using this dataset and a novel evaluation methodology# we show that the proposed method greatly outperforms state-of-the-art methods. Finally# we demonstrate excellent performance on challenging real-world sequences involving object manipulation. ,Karl Pauwels*# University of Granada; Leonardo Rubio# University of Granada; Javier DÃ­az# University of Granada; Eduardo Ros# University of Granada,kpauwels@atc.ugr.es; lrubio@atc.ugr.es; jdiaz@atc.ugr.es; eduardo@atc.ugr.es,04.05 Object Tracking and Motion Analysis,04.04 Model-based reconstruction and tracking*,04.05 Object Tracking and Motion Analysis,04.06 Optical Flow,05.09 Stereo correspondence, ,Motion and Medical Imaging,P 2D
1399,Minimum Uncertainty Gap for Robust Visual Tracking,We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds# our method finds the confident and reliable state of the target. In the paper# the state that gives  the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only# especially when there are severe illumination changes# occlusions# and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally# an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.,Junseok Kwon# Seoul National university; Kyoung Mu Lee*# Seoul National University,paradis0@snu.ac.kr; kyoungmu@snu.ac.kr,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,04.01 Motion estimation and alignment,08.02 Bayesian modeling and inference,, ,Motion and Medical Imaging,P 2D
829,Part-based Visual Tracking with Online Latent Structural Learning,Despite many advances made in the area# deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets# but applying this approach to a part-based target model is complicated by the need to model the relationships between parts# and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e.# predicting the location of the bounding boxes) with latent part variables. To better estimate the parts# and to avoid over-fitting caused by the extra model complexity/capacity introduced by the parts# we propose a two-stage training process# based on the primal rather than the dual form. We then show that the method outperforms the state-of-art (linear and non-linear kernel) trackers.,Rui Yao*# Northwestern Polytechnic Univ.; Qinfeng Shi# University of Adelaide; Chunhua Shen# The University of Adelaide; Yanning Zhang# Northwestern Polytechnical University; Anton van den Hengel# The University of Adelaide,yaorui@mail.nwpu.edu.cn; shiqinfeng@gmail.com; chunhua.shen@adelaide.edu.au; ynzhang@nwpu.edu.cn; anton.vandenhengel@adelaide.edu.au,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,04.04 Model-based reconstruction and tracking,09.03 Face detection and head tracking,09.08 Video Surveillance, ,Motion and Medical Imaging,P 2D
1426,Least Soft-thresold Squares Tracking,In this paper# we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods# the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution# which can be solved efficiently. Based on maximum joint likelihood of parameters# we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods# the proposed metric is more effective in dealing with outliers. In addition# we present an update scheme to explain the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.,Dong Wang# DLUT# china; Huchuan Lu*# Dalian University of Technolog; Ming-Hsuan Yang# UC Merced,wangdong.ice@gmail.com; lhchuan@dlut.edu.cn; mhyang@ucmerced.edu,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,04.04 Model-based reconstruction and tracking,,, ,Motion and Medical Imaging,P 2D
1018,Self-paced learning for long-term tracking,We address the problem of long-term object tracking# where the object may become occluded or leave-the-view. In this setting# we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the Â“rightÂ” frames# and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates# demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online algorithm that approaches real-time performance. Our models significantly outperform prior art#  reducing the average error on benchmark videos by a factor of 4. ,James Supancic III*# UC Irvine; Deva Ramanan# UCI,jsupanci@uci.edu; dramanan@ics.uci.edu,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,04.06 Optical Flow,07.05 Instance recognition,07.06 Object Detection, ,Motion and Medical Imaging,P 2D
1649,Multi-target Tracking by Rank-1 Tensor Approximation,In this paper we formulate the multi-target tracking (MTT) as a rank-1 tensor approximation problem and propose an L-1 norm tensor power iteration solution.  In particular# a high order tensor is constructed based on trajectories in the time window# with each tensor element be the affinity of the corresponding trajectory. The assignment variables are the L-1 normalized vectors# which are used to approximate the rank-1 tensor. Our approach provides a flexible and effective formulation where both pairwise and high-order association energies can be used expediently. We also show the close relation between our formulation with the multi-dimensional assignment (MDA) model. To solve the optimization in rank-1 tensor approximation# we propose an algorithm that iteratively powers the intermediate solution followed by an L-1 tensor normalization. Aside from effectively capturing high-order motion information# the proposed solver runs efficiently with proved convergence. The experimental validation are conducted on two challenging datasets and our method demonstrates promising performances on both of them.,Xinchu Shi*# NLPR; Haibin Ling# ; Junliang Xing# Institute of Automation# Chine; Weiming Hu# ,xcshi@nlpr.ia.ac.cn; hbling@temple.edu; jlxing@nlpr.ia.ac.cn; wmhu@nlpr.ia.ac.cn,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,09.07 Person detection and tracking,09.08 Video Surveillance,10.01 Aerial and outdoor image analysis and modeling, ,Motion and Medical Imaging,P 2D
477,Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities,Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object  tracking approaches. This simplification of given geometric information produces projection artifacts and constrains most approaches to objects moving on a common 2D ground-plane. To overcome these limitations# we introduce the concept of an occupancy volume - exploiting the full geometry and the objects' center of mass - and develop an efficient algorithm for 3D object tracking# which is not restricted to the common ground-plane assumption. Individual objects are tracked using the local mass density scores within a particle filter based approach# constrained by a Voronoi partitioning between nearby trackers. We benefit from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand# when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-the-art methods# while reaching near real-time performance.   ,Horst Possegger*# Graz University of Technology; Sabine Sternig# Graz University of Technology; Thomas Mauthner# ; Peter Roth# ; Horst Bischof# ,possegger@icg.tugraz.at; sternig@icg.tugraz.at; mauthner@icg.tugraz.at; pmroth@icg.tugraz.at; bischof@icg.tugraz.at,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,09.08 Video Surveillance,,, ,Motion and Medical Imaging,P 2D
618,Information Consensus for Distributed Multi-Target Tracking,"Due to their high fault-tolerance# ease of installation and scalability to large networks# consensus-based distributed algorithms have recently gained immense popularity in the sensor networks community# and of late in computer vision. Multi-target tracking in a camera network is one of the fundamental problems in this domain. Distributed estimation algorithms work by exchanging information between sensors that are communication neighbors. Since most cameras are directional sensors# it is often the case that neighboring sensors may not be sensing the same target. Such sensors that do not have information about a target are termed as ""naive"" with respect to that target. In this paper# we propose consensus-based distributed multi-target tracking algorithms in a camera network that are specifically aware of this issue of naivety. The estimation errors in tracking and data association# as well as the effect of naivety# are combined together leading to the development of an information-weighted consensus algorithm# which we term as the Multi-target Information Consensus (MTIC) algorithm. Experimental analysis is provided to back up the theoretical results.",Ahmed Kamal*# UCR; Amit Roy-Chowdhury# ; Jay Farrell# ,akamal@ee.ucr.edu; amitrc@ee.ucr.edu; farrell@ee.ucr.edu,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,09.08 Video Surveillance,,, ,Motion and Medical Imaging,P 2D
558,Online Object Tracking: A Benchmark,Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets# it is of great importance to develop a library and benchmark to gauge the state of the art. In this work# we review recent advances of online object tracking and annotate image sequences with attributes for performance evaluation and analysis. We carry out large scale experiments (29 tracking algorithms with analysis of more than 660#000 frames) with different evaluation criteria to understand how these algorithms perform. By analyzing quantitative results# we identify the critical components for a robust tracker and provide potential future research directions in this field.,Yi Wu*# UC Merced; Jongwoo Lim# ; Ming-Hsuan Yang# UC Merced,ywu.china@gmail.com; jongwoo.lim@gmail.com; mhyang@ucmerced.edu,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,,,, ,Motion and Medical Imaging,P 2D
668,Learning Compact Binary Codes for Visual Tracking,A key problem in visual tracking is to represent the appearance of an object in a way that is robust to visual changes. To attain this robustness# increasingly complex models are used to capture appearance variations. However# such models can be difficult to maintain accurately and efficiently. In this paper# we propose a visual tracker in which objects are represented by compact and discriminative binary codes. This representation can be processed very efficiently# and is capable of effectively fusing information from multiple cues. An incremental discriminative learner is then used to construct an appearance model that optimally separates the object from its surrounds. Furthermore# we design a hypergraph propagation method to capture the contextual information on samples# which further improves the tracking accuracy. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.,Xi Li*# University of Adelaide; Chunhua Shen# The University of Adelaide; Anthony Dick# ; Anton van den Hengel# The University of Adelaide,xi.li03@adelaide.edu.au; chunhua.shen@adelaide.edu.au; ard@cs.adelaide.edu.au; anton.vandenhengel@adelaide.edu.au,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,,,, ,Motion and Medical Imaging,P 2D
1121,Visual Tracking via Locality Sensitive Histograms,This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin# a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away are rejected automatically by assigning a very low weight. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed# which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outperforms the state-of-the-art methods in challenging scenarios# especially when the illumination changes dramatically.,Shengfeng He*# City University of Hong Kong; Qingxiong Yang# City University of Hong Kong; Rynson Lau# City University of Hong Kong; Jiang Wang# City University of Hong Kong; Ming-Hsuan Yang# UC Merced,shengfeng_he@yahoo.com; liiton.research@gmail.com; rynson.lau@cityu.edu.hk; wj11hk@gmail.com; mhyang@ucmerced.edu,04.05 Object Tracking and Motion Analysis*,04.05 Object Tracking and Motion Analysis*,,,, ,Motion and Medical Imaging,P 2D
11,Optical Flow Estimation using Laplacian Mesh Energy,In this paper we present a novel non-rigid optical flow algorithm for dense image correspondence and non-rigid registration. The algorithm uses a unique Laplacian Mesh Energy term to encourage local smoothness whilst simultaneously preserving non-rigid deformation. Laplacian deformation approaches have become popular in graphics research as they enable mesh deformations to preserve local surface shape. In this work we propose a novel Laplacian Mesh Energy formula to ensure such sensible local deformations between image pairs. We express this wholly within the optical flow optimization# and show its application in a novel coarse-to-fine pyramidal approach. Our algorithm achieves the state-of-the-art performance in all trials on the Garg et al. dataset# and top tier performance on the Middlebury evaluation.,Wenbin Li*# University of Bath; Darren Cosker# University of Bath; Matthew Brown# University of Bath,w.li@bath.ac.uk; dpc@cs.bath.ac.uk; m.brown@bath.ac.uk,04.06 Optical Flow*,04.06 Optical Flow*,04.01 Motion estimation and alignment,,, ,Motion and Medical Imaging,P 2D
377,Large Displacement Optical Flow from Nearest Neighbor Fields,We present an optical flow algorithm for large displacement motions. Most existing optical flow methods use the standard coarse-to-fine framework to deal with large displacement motions which has intrinsic limitations. Instead# we formulate the motion estimation problem as a motion segmentation problem. We use approximate nearest neighbor fields to compute an initial motion field and use a robust algorithm to compute a set of similarity transformations as the motion candidates for segmentation. To account for deviations from similarity transformations# we add local deformations in the segmentation process. We also observe that small objects can be better recovered using translations as the motion candidates. We fuse the motion results obtained under similarity transformations and under translations together before a final refinement. Experimental validation shows that our method can successfully handle large displacement motions. Although we particularly focus on large displacement motions in this work# we make no sacrifice in terms of overall performance. In particular# our method ranks at the top of the Middlebury benchmark.,"Zhuoyuan Chen*# Northwestern University; Hailin Jin# Adobe; Zhe Lin# """"""""""""""Adobe Systems# Inc.""""""""""""""; Scott Cohen# Adobe System Inc.; Ying Wu# ",zhuoyuanchen2014@u.northwestern.edu; hljin@adobe.com; zlin@adobe.com; scohen@adobe.com; yingwu@eecs.northwestern.edu,04.06 Optical Flow*,04.06 Optical Flow*,04.01 Motion estimation and alignment,04.05 Object Tracking and Motion Analysis,08.05 Markov Random Fields, ,Motion and Medical Imaging,P 2D
1653,A Fully Connected Layered Model of Foreground and Background Flow,Layered models allow scene segmentation and motion estimation to be formulated together and to inform one another. Traditional layered motion methods# however# employ fairly weak models of scene structure# relying on locally connected Ising/Potts models which have limited ability to capture long-range correlations in natural scenes. To address this# we formulate a fully connected layered model that enables global reasoning about the complicated segmentations of real objects. Optimization with fully connected graphical models is challenging# and our inference algorithm leverages recent work on efficient mean field updates for fully connected conditional random fields. These methods can be implemented efficiently using high-dimensional Gaussian filtering. We combine these ideas with a layered flow model# and find that the long-range connections greatly improve segmentation into figure-ground layers when compared with locally connected MRF models. Experiments on several benchmark datasets show that the method can recover fine structures and large occlusion regions# with good flow accuracy and much lower computational cost than previous locally connected layered models.,Deqing Sun*# ; Jonas Wulff# MPI Tuebingen; Erik Sudderth# Brown University; Michael Black# Max Planck Institute; Hanspeter Pfister# Harvard,dqsun@seas.harvard.edu; jonas.wulff@tuebingen.mpg.de; sudderth@cs.brown.edu; black@tuebingen.mpg.de; pfister@seas.harvard.edu,04.06 Optical Flow*,04.06 Optical Flow*,04.01 Motion estimation and alignment,08.04 Graphical models,08.05 Markov Random Fields, ,Motion and Medical Imaging,P 2D
357,Crossing the Line: Crowd Counting by Integer Programming with Local Features,In this paper# we propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a monocular video sequence. Through a line sampling process# the video is first converted into a temporal slice image. Next# the number of people is estimated in a set of overlapping sliding windows on the temporal slice image# using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval# an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting# our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.,Zheng Ma*# City University of Hong Kong; Antoni  Chan# City University of Hong Kong,zhengma2@student.cityu.edu.hk; abchan@cs.cityu.edu.hk,09.08 Video Surveillance*,09.08 Video Surveillance*,09.09 Video Analysis and Event Recognition,,, ,Video Analysis,P 3A
1727,Multi-Source Multi-Scale Counting in Extremely Dense Crowd Images,We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a still image. Due to problems including perspective# occlusion# clutter# and few pixels per person# counting by human detection in such images is almost impossible. Instead# in our approach# we rely on low confidence head detections# repetition of texture elements# and Fourier analysis to compute several maps which indicate the confidence in observing individuals in an image region. Secondly# we employ a global constraint on spatial and multi-scale individual count consistency using Markov Random Field# which enforces consistency in counts in local neighborhoods of the image arising from perspective distortion. We tested our method on a dataset of fifty crowd images containing 64K annotated humans# with the head counts ranging from 94 to 4543. This is in stark contrast to datasets used for existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.,Haroon Idrees*# UCF; Imran Saleemi# UCF; Mubarak Shah# University of Central Flrida,haroonidrees@gmail.com; imran@cs.ucf.edu; shah@cs.ucf.edu,09.08 Video Surveillance*,09.08 Video Surveillance*,10.01 Aerial and outdoor image analysis and modeling,,, ,Video Analysis,P 3A
877,Better exploiting motion for better action recognition,"Several recent works on action recognition have attested the importance of explicitly integrating motion characteristics in the video description. This paper establishes that adequately decomposing visual motion into camera and scene motions greatly facilitates action recognition tasks. In particular# we introduce camera motion compensation# based on a real-time robust technique# both in the extraction of the space-time trajectories and in the computation of existing descriptors (so-called ""compensated descriptors"")# which proves to be extremely effective in boosting the performance of these descriptors. Second# we design a new motion descriptor# the DCS descriptor# based on differential motion scalar quantities# divergence# curl and shear features. It captures additional information on the local motion patterns enhancing results. Finally# applying the recent VLAD coding technique proposed in image retrieval provides a substantial improvement for action recognition. These novel contributions complement one another and lead to outperform all reported results by a significant margin on three challenging datasets# namely Hollywood 2# HMDB51 and Olympic Sports.",Mihir Jain# INRIA; Herve Jegou*# ; Patrick Bouthemy# INRIA,mihir.jain@inria.fr; herve.jegou@inria.fr; patrick.bouthemy@inria.fr,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,04.05 Object Tracking and Motion Analysis,07.01 Recognition,07.02 Category recognition, ,Video Analysis,P 3A
155,Detection of Manipulation Action Consequences (MAC),The problem of action recognition and human activity has been an active research area in Computer Vision and Robotics. While full-body motions can be characterized by movement and change of posture# no characterization# that holds invariance# has yet been proposed for the description of manipulation actions. We propose that a fundamental concept  in understanding such actions# are the \textbf{consequences of actions}. There is a small set of fundamental primitive action consequences that provides a systematic high-level classification of manipulation actions. In this paper a technique is developed to recognize these action consequences. A novel active tracking and segmentation method  monitors the appearance and the topological structure of the manipulated object. Then  a visual semantic graph (VSG) based procedure applied to the time sequence of the monitored object recognizes the action consequence. We provide a new dataset# called \textbf{M}anipulation \textbf{A}ction \textbf{C}onsequences (MAC 1.0)# which can  serve as testbed for other studies on this topic. Several experiments are conducted on this dataset# demonstrate that our method can robustly track objects and detect their deformations and division during the manipulation. Quantitative tests prove the effectiveness and efficiency of the method.,Yezhou Yang*# University of Maryland; Cornelia Fermuller# ; Yiannis Aloimonos# ,yzyang@cs.umd.edu; fer@umiacs.umd.edu; yiannis@cs.umd.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.01 Recognition,,, ,Video Analysis,P 3A
183,Representing Videos using Mid-level Discriminative Patches,How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action# a semantic object# or perhaps a random but informative spatio-temporal patch in the video. What defines these spatio-temporal patches is their discriminative and representative properties.  We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore# these patches can be used as a discriminative vocabulary for action classification where they demonstrate state-of-the-art performance on a subset of the UCF50 and Olympics datasets.,Arpit Jain*# University of Maryland; Abhinav Gupta# ; Mikel Rodriguez# MITRE; Larry Davis# ,ajain@umd.edu; abhinavg@cs.cmu.edu; rodriguez.mikel@gmail.com; lsd@umiacs.umd.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.01 Recognition,,, ,Video Analysis,P 3A
344,Modeling Actions through State Changes,In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships# but differ in their purpose and meanings. The key to differentiate these actions is being able to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned# we run them at each frame of the videos and pool their outputs to detect actions. We introduce methods that leverage the changes in the state of the environment to recognize actions and segment activities. Our results outperform state-of-the-art action recognition and activity segmentation results. ,Alireza Fathi*# Georgia Tech; James Rehg# Georgia Tech,afathi3@gatech.edu; rehg@gatech.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.01 Recognition,07.07 Object Recognition,, ,Video Analysis,P 3A
422,Recognizing Activities via Bags of Words for Attribute Dynamics,A novel video representation for activity recognition is proposed. It combines an attribute-based video representation# and the modeling of video dynamics. The video sequence is decomposed into short-term segments# which are characterized by the dynamics of their attributes. These are modeled by a dictionary of attribute dynamics templates# which are implemented with a recently introduced generative model# the binary dynamic system (BDS). Methods are proposed for learning  dictionaries of such binary systems# from a training corpus# and for quantizing the sequence of attribute scores extracted from a video sequence with these dictionaries. This leads to the representation of the video as a histogram of assignments of video segments to BDSs# which is denoted the bag-of-words for attribute dynamics (BoWAD). An extensive experimental evaluation reveals that this representation has performance superior to the state-of-the-art in complex activity recognition.,Weixin Li*# UC San Diego; Qian Yu# Sarnoff; Nuno Vasconcelos# ,wel017@ucsd.edu; qian.yu@sri.com; nuno@ece.ucsd.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.01 Recognition,07.04 Image and Video Retrieval,, ,Video Analysis,P 3A
1093,Sampling Strategies for Real-time Action Recognition,Local spatio-temporal features and bag-of-features representation have become popular for action recognition. A recent trend is to use dense sampling for better performance. While many methods claimed to use dense feature sets# most of them are just denser than approaches based on sparse interest point detectors. In this paper# we explore sampling with high density on action recognition. We also investigate the impact of random sampling over dense grid for computational efficiency. We present a real-time action recognition system which integrates fast random sampling method with semantic meaningful local spatiotemporal features from Local Part Model. We evaluate our approach on three popular action recognition datasets. Our technique achieves high accuracy on simple KTH dataset# and outperforms state-of-the-art on two very challenging datasets# namely# 93% on KTH# 66.6% on UCF50 and 31.74% on HMDB51.,Feng Shi*# University of Ottawa; Emil Petriu# University of Ottawa; Robert LaganiÃ¨re# University of Ottawa,fshi098@site.uottawa.ca; petriu@site.uottawa.ca; laganier@uottawa.ca,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.01 Recognition,,, ,Video Analysis,P 3A
1844,Dynamic Scene Classification: Learning Motion Descriptors with Slow Features Analysis,In this paper# we address the challenging problem of categorizing video sequences composed of dynamic natural scenes. Contrarily to previous methods that rely on handcrafted descriptors# we propose here to represent videos using unsupervised learning of motion features. Our method encompasses three main contributions: 1) Based on the Slow Feature Analysis principle# we introduce a learned local motion descriptor which represents the principal and more stable motion components of training videos. 2) We integrate our local motion feature into a global coding/pooling architecture in order to provide an effective signature for each video sequence. 3) We report state of the art classification performances on two challenging natural scenes data sets. In particular# an outstanding improvement of 11% in classification score is reached on a data set introduced in 2012.,Christian Theriault*# LIP6; Nicolas Thome# ; Matthieu Cord# ,theriaultchristian@gmail.com; nicolas.thome@lip6.fr; Matthieu.Cord@lip6.fr,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.02 Category recognition,07.03 Context and scene understanding,08.09 Statistical Methods and Learning, ,Video Analysis,P 3A
1269,Online Video Parsing,We present a novel approach for video parsing and simultaneous online learning of dominant and anomalous behaviors in surveillance videos. Dominant behaviours are those occurring frequently in videos and hence# usually do not attract much  attention. They can be characterized by different complexities in space and time# ranging from a scene background to human activities. In contrast# an anomalous behaviour is defined as having a low likelihood of occurrence. We do not employ any models of the entities in the scene in order to detect these two kinds of behaviors. In this paper# video events are learnt at each pixel without supervision using densely constructed spatio-temporal video volumes. Furthermore# the volumes are organized into large contextual graphs. These compositions are employed  to construct a hierarchical codebook model for the dominant behaviours. By decomposing spatio-temporal contextual information into unique spatial and temporal contexts# the proposed framework learns the models of the dominant spatial and temporal events. Thus# it is ultimately capable of simultaneously modeling high-level behaviours as well as low-level spatial# temporal and spatio-temporal pixel level changes. Experiments have been performed on different video datasets and the qualitative and quantitative comparisons of the results indicates that the proposed method has superior performance to the alternative approaches.,Mehrsan Javan Roshtkhari*# McGill University; Martin Levine# McGill University,javan@cim.mcgill.ca; levine@cim.mcgill.ca,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.03 Context and scene understanding,07.04 Image and Video Retrieval,09.08 Video Surveillance, ,Video Analysis,P 3A
122,Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition,We present data-driven techniques to augment Bag of Words (BoW) models# which allow for more robust modeling and recognition of complex activities# especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches# which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition# we also propose the use of randomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.,Vinay Kumar Bettadapura*# Georgia Tech; Grant Schindler# Georgia Institute of Technology; Thomas  Ploetz# ; Irfan Essa# GaTech,vinay@gatech.edu; schindler@cc.gatech.edu; thomas.ploetz@newcastle.ac.uk; irfan@cc.gatech.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.04 Image and Video Retrieval,08.09 Statistical Methods and Learning,09.08 Video Surveillance, ,Video Analysis,P 3A
771,Complex Event Detection via Multi-Source Video Attributes,In this paper# we focus on complex event detection which is an emerging area of research. Since complex events essentially include human# scenes# objects and actions that can be summarized by visual attributes# leveraging relevant attributes properly could be helpful for event detection. In literature# many works have used visual attributes at image level to facilitate recognition or detection tasks. However# attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence# we propose to leverage attributes at video level# i.e.# the semantic labels of external videos are used as attributes# more particularly named as video attributes in this work. Compared to complex event videos# these external videos contain simple contents such as objects# scenes and actions which are the basic elements of complex events. Specifically# building upon a correlation vector which correlates the attributes and the complex event# we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos in a joint framework. Extensive experiments on complex event detection using a real-world large-scale dataset validate the efficacy of the proposed approach.,Zhigang Ma*# University of Trento; Yi Yang# Carnegie Mellon University; Zhongwen Xu# Carnegie Mellon University; Shuicheng YAN# NUS; Nicu Sebe# University of Trento; Alexander Hauptmann# Carnegie Mellon University,ma@disi.unitn.it; yiyang@cs.cmu.edu; zhongwen@cs.cmu.edu; eleyans@nus.edu.sg; sebe@disi.unitn.it; alex@cs.cmu.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.04 Image and Video Retrieval,,, ,Video Analysis,P 3A
1330,A Thousand Frames in Just a Few Words:  Lingual Description of Videos through Latent Topics and Sparse Object Stitching,The problem of describing images through natural language has gained importance in the  computer vision community.  The solution has focused on either a top-down approach of generating natural language through combinations of object detections and language models derived from external knowledge bases or the bottom-up way of propagating keyword tags from training images to test images through probabilistic or nearest neighbor techniques.  However# describing videos by generating high level lingual descriptions directly is a less studied problem.  In this paper# we combine the best of both worlds to generate lingual descriptions of real life videos  that capture the most relevant contents of a video.  We propose an efficient hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation# an intermediate level of concept detectors and a final high level module to produce lingual descriptions.  We compare the results of our system to human synopses in both short and long forms on two datasets. We empirically show that our system covers the information need significantly better than those covered by the lingual descriptions from intermediate levels. ,Pradipto Das# SUNY at Buffalo; Chenliang Xu# SUNY at Buffalo; Richard Doell# SUNY at Buffalo; Jason Corso*# Buffalo,pdas3@buffalo.edu; chenlian@buffalo.edu; rfdoell@buffalo.edu; jcorso@buffalo.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.04 Image and Video Retrieval,,, ,Video Analysis,P 3A
1063,Spatiotemporal Deformable Part Models for Action Detection,Deformable part models have achieved impressive performance for object detection# even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model# the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned.  By focusing on the most distinctive parts of each action# our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.,Yicong Tian# University of Central Florida; Rahul Sukthankar*# Google Research and CMU; Mubarak Shah# ,tyc.cong@gmail.com; rahuls@cs.cmu.edu; shah@eecs.ucf.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.08 Part-based recognition,,, ,Video Analysis,P 3A
1299,Poselet Key-framing: A Model for Human Activity Recognition,In this paper# we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes -- collections of partial key-poses of the actor(s)# depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework# where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and mine for hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show  classification performance that is competitive with the state of the art  on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.,Michalis Raptis*# Disney Research Pittsburgh; Leonid Sigal# ,mraptis@disneyresearch.com; lsigal@disneyresearch.com,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,07.08 Part-based recognition,08.01 Learning# statistics# and inference,"08.09 Statistical Methods and Learning""", ,Video Analysis,P 3A
995,Recognize Human Activities from Partially Observed Videos,Recognizing human activities in partially observed videos is a challenging problem and has many practical applications. When the unobserved subsequence is at the end of the video# the problem is reduced to activity prediction from unfinished activity streaming# which has been studied by many researchers. However# in the general case# an unobserved subsequence may occur at any time by yielding a temporal gap in the video. In this paper# we propose a new method that can recognize human activities from partially observed videos in the general case. Specifically# we formulate the problem into a probabilistic framework: 1) dividing each activity into multiple ordered temporal segments# 2) using spatiotemporal features of the training video samples in each segment as bases and applying sparse coding (SC) to derive the activity likelihood of the test video sample at each segment# and 3) finally combining the likelihood at each segment to achieve a global posterior for the activities. We further extend the proposed method to include more bases that correspond to a mixture of segments with different temporal lengths (MSSC)# which can better represent the activities with large intra-class variations. We evaluate the proposed methods (SC and MSSC) on various real videos. We also evaluate the proposed methods on two special cases: 1) activity prediction where the unobserved subsequence is at the end of the video# and 2) human activity recognition on fully observed videos. Experimental results show that the proposed methods outperform existing state-of-the-art comparison methods.,Yu Cao*# University of South Carolina; Daniel Barrett# Purdue University; Andrei Barbu# Purdue University; Siddharth Narayanaswamy# Purdue University; Haonan Yu# Purdue University; Aaron Michaux# Purdue University; Yuewei Lin# University of South Carolina; Sven Dickinson# University of Toronto; Jeffrey Siskind# Purdue University; Song Wang# University of South Carolina,cao@cec.sc.edu; dpbarret@purdue.edu; andrei@0xab.com; siddharth@iffsid.com; yu239@purdue.edu; aaron@pageofswords.net; ywlin.cq@gmail.com; sven@cs.toronto.edu; qobi@purdue.edu; songwang@cec.sc.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,08.02 Bayesian modeling and inference,08.08 Sparse coding and dictionaries,, ,Video Analysis,P 3A
1235,Event Recognition in Videos by Learning From Heterogeneous Web Sources,In this work# we propose to leverage a large number of loosely labeled web videos (e.g.# from YouTube) and web images (e.g.# from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources# in which the samples from different source domains can be represented by different types of features with different dimensions (e.g.# the SIFT features from web images and space-time (ST) features from web videos) while the target domain samples are with multiple types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain# we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier by simultaneously seeking for the optimal weights for different source domains with different types of features as well as inferring the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of our method MDA-HS for event recognition in consumer videos.,Lin Chen*# NTU; Lixin Duan# ; Dong Xu# ,chen0631@ntu.edu.sg; lxduan@gmail.com; dongxu@ntu.edu.sg,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,08.09 Statistical Methods and Learning,,, ,Video Analysis,P 3A
1223,Motionlets: Mid-Level 3D Parts for Human Motion Recognition,This paper proposes \emph{motionlet}# a mid-level and spatiotemporal part# for  human motion recognition. Motionlet can be seen as a tight cluster in motion and appearance space# corresponding to the moving process of different body parts. We postulate three key properties of motionlet for action recognition: high motion saliency# multiple scale representation# and representative-discriminative ability. Towards this goal# we develop a data-driven approach to learn motionlets from training videos. First# we extract 3D regions with high motion saliency. Then we cluster these regions and preserve the centers as candidate templates for motionlet. Finally# we examine the representative and discriminative power of the candidates# and introduce a greedy method to select effective candidates. With motionlets# we present a mid-level representation for video# called \emph{motionlet activation vector}. We conduct experiments on three datasets# KTH# HMDB51# and UCF50. The results show that the proposed methods significantly outperform state-of-the-art methods.,LiMin Wang*# CUHK; Qiao Yu# SIAT# China; Xiaoou Tang# ,07wanglimin@gmail.com; yu.qiao@siat.ac.cn; xtang@ie.cuhk.edu.hk,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,09.01 Applications: humans,,, ,Video Analysis,P 3A
102,Multi-Agent Event Detection: Localization and Role Assignment,We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically# to detect multi-agent events from video# our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations# we formulate the joint problem as two efficient optimization subproblems---quadratic programming for role assignment followed by linear programming for event localization. Additionally# we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos# which contain multiple target events and nonparticipating agents.,Suha Kwak*# POSTECH; Bohyung Han# POSTECH; Joon Han# POSTECH,mercury3@postech.ac.kr; bhhan@postech.ac.kr; joonhan@postech.ac.kr,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,09.08 Video Surveillance,,, ,Video Analysis,P 3A
817,Cross-View Action Recognition via a Continuous Virtual Path,In this paper# we propose a novel method for cross-view action recognition via a continuous virtual path# which connects the source view and the target view. Each point on this virtual path is a virtual view# which is obtained by a linear transformation of the action descriptor. All the virtual views are concatenated into an infinite-dimensional feature to characterize continuous changes from the source to the target view. However# these infinite-dimensional features cannot be used directly. Thus# we propose a virtual view kernel to compute the value of similarity between two infinite-dimensional features# which can be readily used to construct any kernelized classifiers. In addition# there are a lot of unlabeled samples from the target view# which can be utilized to improve the performance of classifiers. Thus# we present  a constraint strategy to explore the information contained in the unlabeled samples.  The rationality behind the constraint is that any action video belongs to only one class. Our method is verified on the IXMAS dataset# and the experimental results demonstrate that our method achieves better performance than the state-of-the-art methods.,Zhong Zhang*# CASIA,zhong.zhang@ia.ac.cn,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,09.08 Video Surveillance,,, ,Video Analysis,P 3A
135,Large-Scale Video Summarization Using Web-Image Priors,Given the enormous growth in user-generated videos# it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality# summarization methods designed for well-produced videos do not generalize to them. To address this challenge# we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work# we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover# to automatically evaluate summarization algorithms on a large scale# we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that of multiple human evaluators. Finally# we present results for our framework tested on hundreds of user-generated videos.,Aditya Khosla*# MIT; Raffay Hamid# eBay; Chih-Jen Lin# National Taiwan University; Neel Sundaresan# eBay Research Labs,khosla@mit.edu; raffay@gmail.com; cjlin@csie.ntu.edu.tw; nsundaresan@ebay.com,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,,,, ,Video Analysis,P 3A
298,Representing and Discovering Adversarial Team Behaviors using Player Roles,In this paper# we describe a method to represent and discover adversarial group behavior in a continuous domain. In comparison to other types of behavior# adversarial behavior is heavily structured as the location of an player (or agent) is dependent both on their teammates and adversaries# in addition to the tactics or strategies of the team. We propose a method which can exploit this relationship through the use of a spatiotemporal basis model. As players constantly change roles during a match# we show that employing a role-based representation instead of one based on player identity can best exploit the playing structure. As vision-based systems currently do not provide perfect detection/tracking (e.g. missed or false detections)# we show that our approach can denoise the signal and assign player roles which enables analysis of team formation and plays in continuous sports occur. Finding a compact representation allows significant temporal analysis to occur# which was previously prohibitive due to the dimensionality of the signal. To evaluate our approach# we used a fully instrumented field-hockey pitch with 8 fixed high-definition (HD) cameras and evaluated our approach on approximately 200k frames of data from a state-of-the-art real-time player detector and compared it to manually labelled data. ,Patrick Lucey*# Disney Research Pittsburgh; Alina Bialkowski# ; Peter Carr# Disney Research; Iain Matthews # ; Yaser Sheikh# ,patrick.lucey@disneyresearch.com; alina.bialkowski@disneyresearch.com; peter.carr@disneyresearch.com; iainm@disneyresearch.com; yaser@cs.cmu.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,,,, ,Video Analysis,P 3A
1257,Story-Driven Summarization for Egocentric Video,"We present a video summarization approach that discovers the story of an egocentric video.  Given a long input video# our method selects a short chain of video subshots depicting the essential events.  Inspired by work in text analysis that links news articles over time# we define a random-walk based metric of influence between subshots that reflects how visual objects contribute to the progression of events.  Using this influence metric# we define an objective for the optimal $k$-subshot summary.  Whereas traditional methods optimize a summary's diversity or representativeness# ours explicitly accounts for how one sub-event ``leads to"" another---which# critically# captures event connectivity beyond simple object co-occurrence.  As a result# our summaries provide a better sense of story.  We apply our approach to over 22 hours of daily activity video taken from 23 unique camera wearers# and systematically evaluate its quality compared to multiple baselines with 38 human subjects.",Zheng Lu*# The University of Texas at Aus; Kristen Grauman# Utexas,luzheng@cs.utexas.edu; grauman@cs.utexas.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,,,, ,Video Analysis,P 3A
1332,Finding Group Interactions in Social Clutter,We consider the problem of finding distinctive social interactions involving groups of agents embedded within larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos# and a long input video of a large gathering (with approximately-tracked agents)# we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery of exemplars is annotated with group-interaction categories# each detected interaction is  classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions# and (b) pair-wise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly# the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections# two that involve humans and one that involves mice.,Ruonan Li*# ; Todd Zickler# ,ruonanli@seas.harvard.edu; zickler@seas.harvard.edu,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,,,, ,Video Analysis,P 3A
1396,First-Person Activity Recognition: Recognizing Behaviors Targeted to Observer from Its Viewpoint,This paper discusses the problem of recognizing interaction-level human activities from a first-person viewpoint. The goal is to enable an observer (e.g.# a robot or a wearable computer) to understand `what activity a human is performing to itself' from continuous video inputs. These include friendly interactions such as `hugging the observer' as well as hostile interactions like `punching the observer' or `throwing objects to the observer'# whose videos involve a large amount of camera ego-motion caused by physical interactions. The paper investigates multi-channel kernels to integrate global and local motion information in a first-person video# and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. In experiments# we not only show classification results with segmented videos# but also confirm that our new approach is able to detect activities from continuous videos reliably.,Michael Ryoo*# ; Larry Matthies# JPL,mryoo@jpl.nasa.gov; lhm@jpl.nasa.gov,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,,,, ,Video Analysis,P 3A
1822,Joint Sparsity-based Representation and Analysis of Unconstrained Video Events,While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied# we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers# we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components# across other videos in the system# to perform subsequent video analysis. We demonstrate the efficacy of our approach for event classification and clustering by reporting competitive results on standard datasets such as# HMDB# UCF-50# Olympic Sports and KTH.,Raghuraman Gopalan*# AT&T Research,raghuram@research.att.com,09.09 Video Analysis and Event Recognition*,09.09 Video Analysis and Event Recognition*,,,, ,Video Analysis,P 3A
1912,Motion Estimation for Self-Driving Cars With a Generalized Camera ,In this paper# we present a visual ego-motion estimation algorithm for a self-driving car equipped with a close- to-market multi-camera system. By modeling the multi- camera system as a generalized camera and applying the non-holonomic motion constraint of a car# we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including absolute scale can be obtained. We provide the analytical solutions for the general case and a special case with only intra-camera correspondences# and show that up to a maximum of 6 solutions exist for both cases. We also show the existence of degeneracy when the car undergoes straight motion with intra-camera correspondences where the absolute scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth. ,Gim Hee Lee*# ETHZ; Friedrich Fraundorfer# ; marc pollefeys# ETHZ,glee@student.ethz.ch; fraundorfer@inf.ethz.ch; marc.pollefeys@inf.ethz.ch,10.05 Robot vision*,10.05 Robot vision*,05.03 Calibration and pose estimation,05.10 Structure from Motion,, ,Video Analysis,P 3A
43,Learning Separable Filters,Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately# these filters are usually both numerous and non-separable# making their use computationally expensive. In this paper# we show that such filters can be computed as linear combinations of a smaller number of separable ones# thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes# and we show that we significantly outperform state-of-the-art methods on the linear structure extraction task# in terms of both accuracy and speed. Moreover# our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions. ,Roberto Rigamonti*# EPFL; Amos Sironi# EPFL; Vincent Lepetit# EPFL; Pascal Fua# EPFL,roberto.rigamonti@epfl.ch; amos.sironi@epfl.ch; vincent.lepetit@epfl.ch; pascal.fua@epfl.ch,02.01 Feature extraction and matching*,02.01 Feature extraction and matching*,01.01 Image processing,08.08 Sparse coding and dictionaries,, ,Features and Contours,P 3B
569,Robust Feature Matching with Alternate Hough and Inverted Hough Transforms,We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences# and enhances the performance of matching in both precision and recall. Inspired by the fact that locally grouped features on the same object share coherent homographies# we investigate this property# and cast the task of feature matching as a density estimation problem in Hough space. Specifically# we identify correct correspondences by projecting the corresponding homographies into the parametric Hough space. It activates mutual verification of relevant correspondences by voting# and boosts the precision of matching. On the other hand# the correspondence enrichment is achieved by inferring the concerted homographies propagated from the grouped features. The recall is hence increased. The two processes are tightly coupled. Through an iterative optimization process# more true correspondences are detected while plausible enrichments are gradually revealed. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.,Hsin-I Chen# National Taiwan University; Yen-Yu Lin*# Academia Sinica; Bing-Yu Chen# National Taiwan University,fensichen@gmail.com; yylin@citi.sinica.edu.tw; robin@ntu.edu.tw,02.01 Feature extraction and matching*,02.01 Feature extraction and matching*,02.05 Feature matching and indexing,,, ,Features and Contours,P 3B
2017,SWIGS: A Swift Guided Sampling Method,We present SWIGS# a Swift and efficient Guided Sampling method for improving the speed of a robust model estimation from image feature correspondences. Our method does not require parameters to tune nor an offline stage to learn the inlier/outlier distributions. SWIGS leverages the accuracy of our new confidence measure (MR-Rayleigh)# which assigns a correctness-confidence to a putative correspondence. MR-Rayleigh is inspired by Meta-Recognition (MR)# an algorithm based on Extreme Value Theory that aims to predict when a classifier's outcome is correct.  We demonstrate that by using a Rayleigh distribution to approximate the mode of the outlier distribution# the prediction accuracy of MR can be improved considerably. Our experiments show that MR-Rayleigh tends to predict better than the often-used Lowe's ratio and the standard MR under a range of imaging conditions. Furthermore# our homography estimation experiment demonstrates that SWIGS performs similarly or better than other well known guided sampling methods while requiring fewer iterations# leading to fast and accurate model estimates.,Victor Fragoso*# UCSB; Matthew Turk# ,vfragoso@cs.ucsb.edu; mturk@cs.ucsb.edu,02.01 Feature extraction and matching*,02.01 Feature extraction and matching*,05.10 Structure from Motion,07.07 Object Recognition,, ,Features and Contours,P 3B
1083,Learning Multiple Non-Linear Sub-Spaces using K-RBMs,The overall complexity in building descriptive or discriminative models is shared between the features derived from raw data and the models that use these features as inputs. Simple features require complex models while more sophisticated features require simpler models to achieve the same level of model quality. Learning semantically richer features is# therefore# the key to building simpler# more interpretable# and more accurate models. In domains such as images# where the data (image patches) might lie in multiple non-linear manifolds# feature learning becomes even more important. In this paper# we propose a framework that uses K Restricted Boltzmann Machines (K-RBMS) to learn non-linear manifolds in the raw image space. We solve the coupled problem of ?nding the right non-linear manifolds in the input space and associating image patches with those manifolds in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classi?cation datasets show that such a framework outperforms the traditional feature representations such as the SIFT based Bagof-Words (BoW) and convolutional deep belief networks.,Siddhartha Chandra*# CVIT# IIIT Hyderabad; Shailesh Kumar# ; CV Jawahar# IIIT Hyderabad,robinchandra19@gmail.com; shkumar@google.com; jawahar@iiit.ac.in,02.01 Feature extraction and matching*,02.01 Feature extraction and matching*,07.01 Recognition,08.09 Statistical Methods and Learning,, ,Features and Contours,P 3B
1075,Light Field Distortion Feature for Transparent Object Recognition,Current object recognition algorithms use local features# such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF)# for visually learning to recognize objects. However# these approaches cannot apply to transparent objects made of glass or plastic# as such objects do not have visual features of their own and the appearance of such objects drastically varies with scene background. Indeed# in transmitting light# transparent objects have the unique characteristic of distorting the background by refraction. In this paper# we use a single-shot light field image as an input and model the distortion of the light field caused by the refractive property of a transparent object. We propose a new feature# called light field distortion (LFD) feature# for identifying a transparent object.  Our proposed method uses this LFD feature incorporated into the bag-of-features approach for recognizing transparent objects.  We evaluated the performance in a laboratory setups and real experiments.,Kazuki Maeno# Kyushu University; Hajime Nagahara*# Kyushu University; Atsushi Shimada# Kyushu University; RIn-ichiro Taniguchi# Kyushu University,maeno@limu.ait.kyushu-u.ac.jp; nagahara@ait.kyushu-u.ac.jp; atsushi@limu.ait.kyushu-u.ac.jp; rin@ait.kyushu-u.ac.jp,02.01 Feature extraction and matching*,02.01 Feature extraction and matching*,07.07 Object Recognition,,, ,Features and Contours,P 3B
1940,From Local Similarity to Global Coding; An Application to Image Classification,Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently# methods that code a descriptor based on nearby bases have proven to be efficacious. These methods take into account the nonlinear structure of descriptors# since local similarities are a good approximation of global similarities. However# they confine their usage of the global similarities to nearby bases. In this paper# we propose a coding scheme that brings into focus the manifold structure of descriptors# and devise a method to compute the global similarities of descriptors to the bases. In this context# given a local similarity measure between bases# a global measure is computed.  Exploiting the local similarity of a descriptor and its nearby bases# a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods# the proposed coding varies smoothly with respect to the underlying manifold. Experiments on the benchmarked image classification datasets validate the superiority of the proposed method over its locality and sparsity based rivals.,Amirreza Shaban# Sharif University of Tech; Hamid Rabiee*# Sharif University of Tech; Mehrdad Farajtabar# Gatech; Marjan Ghazvininejad# EPFL,shaban@ce.sharif.edu; rabiee@sharif.edu; mehrdad@gatech.edu; marjan.ghazvininejad@epfl.ch,02.01 Feature extraction and matching*,02.01 Feature extraction and matching*,"08.01 Learning# statistics# and inference""",,, ,Features and Contours,P 3B
1701,Joint Spectral Correspondence for Disparate Image Matching,We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night)# age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixel-level descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art. ,Mayank Bansal*# University of Pennsylvania; Kostas Danilidiis# Upenn,mayankb@cis.upenn.edu; kostas@cis.upenn.edu,02.01 Feature extraction and matching*,02.01 Feature extraction and matching*,,,, ,Features and Contours,P 3B
1056,Efficient Color Boundary Detection with Color-opponent Mechanisms,Color information plays important role in understanding of natural scenes by at least facilitating evaluating boundaries of objects or areas. In this study# we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model# which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). We demonstrate that our framework has the excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes. ,Kaifu Yang# UESTC; Yongjie Li*# UESTC; Shaobing Gao# UESTC,yang_kf@163.com; liyj@uestc.edu.cn; gao_shaobing@163.com,02.02 Edge and contour detection,01.05 Early and Biologically-inspired Vision*,02.02 Edge and contour detection,,, ,Features and Contours,P 3B
578,Winding Number and Salient Contour Extraction,This paper aims to extract salient closed contours which usually corresponds to the boundary of foreground objects. A set of contour priors such as local contrast# good continuity and closure has been used to extract clean contours against various noise. However# regional cues can also help to separate the figural region from the background. To make use of both regional cues and contour cues# this paper propose a model that has explicit representation of regional labels and contour labels. To ensure the regioncontour consistency# region labels are constrained to be the same as the winding number induced by the contours. By the virtue of the fast winding number computation method# the region labels are related to the edge labels by a linear transformation. After the transformation# the consistency of the regional labels and edge labels are guaranteed. As an example of our method# the ratio cut objective function is translated into the function based on edges. The improvements from incorporating both regional and contour cues are demonstrated in the experiments.,"Yansheng Ming*# Australian National University; Hongdong Li# ; Xuming He# """"""""""""""NICTA# Australia""""""""""""""",yansheng.ming@anu.edu.au; hongdong.li@anu.edu.au; xuming.he@nicta.com.au,02.02 Edge and contour detection*,02.02 Edge and contour detection*,03.03 Image segmentation,,, ,Features and Contours,P 3B
1494,Supervised Semantic Gradient Extraction Using Linear Optimization,This paper proposes a new supervised semantic edge and gradient extraction approach# which allows the user to roughly scribble over the desired region to extract semantically-dominant and coherent edges in it. Our approach first extracts low-level edgelets (small edge clusters) from the input image as primitives and builds a graph upon them# by jointly considering both the geometric and appearance compatibility of edgelets. Given the characteristics of the graph# it cannot be effectively optimized by commonly-used energy minimization tools such as graph cuts. We thus propose an efficient linear algorithm for precise graph optimization# by taking advantage of the special structure of the graph. Objective evaluations show that the proposed method significantly outperforms previous semantic edge detection algorithms. Finally# we demonstrate the effectiveness of the system in various image editing tasks.,Shulin Yang*# University of washington; Jue Wang# ; Linda Shapiro# ,yang@cs.washington.edu; juewang@adobe.com; shapiro@cs.washington.edu,02.02 Edge and contour detection*,02.02 Edge and contour detection*,08.06 Optimization Methods,,, ,Features and Contours,P 3B
912,Spatio-Temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera,Local spatio-temporal interest points (STIP) and the resulting features from RGB videos often reflect interesting events that can be used for compact representation of videos as well as for interpreting spatio-temporal events. In this paper# we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPs from depth videos (called DSTIP) that effectively suppress the noisy measurements. Further# we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using our own dataset and the MSRAction3D and MSRDailyActivity3D public datasets. Experimental evaluations show that the proposed approach outperforms state-of-the-art activity recognition algorithms on depth videos# and this framework is more widely applicable than existing methods. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications. ,Lu Xia*# The university of texas at aus; Jake Aggarwal# ,xialu@utexas.edu; aggarwaljk@mail.utexas.edu,02.03 Feature descriptors*,02.03 Feature descriptors*,02.01 Feature extraction and matching,02.04 Feature detection,, ,Features and Contours,P 3B
1988,Sparse Quantization for Patch Description,The representation of local image patches is crucial for the good performance and efficiency of many vision tasks. Patch descriptors have been designed to generalize towards diverse variations# depending on the application# as well as the desired compromise between accuracy and efficiency. We present a novel formulation of patch description# that serves such issues well. Sparse quantization lies at its heart. This allows for efficient encodings# leading to powerful# novel binary descriptors# yet also to the generalization of existing descriptors like SIFT or BRIEF. We demonstrate the capabilities of our formulation for both keypoint matching and image classification. Our binary descriptors achieve state-of-the-art results for two keypoint matching benchmarks# namely those by Brown and Mikolajczyk. For image classification# we propose new descriptors# that perform better on Caltech101 and PASCAL VOC07 than SIFT. ,Xavier Boix# ETH; Gemma Roig*# ETH Zurich; Michael Gygli# ETH; Luc  Van Gool# Computer Vision Lab#ETH Zurich,boxavier@vision.ee.ethz.ch; gemmar@vision.ee.ethz.ch; gyglim@student.ethz.ch; vangool@vision.ee.ethz.ch,02.03 Feature descriptors*,02.03 Feature descriptors*,02.01 Feature extraction and matching,,, ,Features and Contours,P 3B
332,Evaluation of Color STIPs for Human Action Recognition,This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). All existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this# these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Also# valuable information might be neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs extend existing intensity-based STIP detectors and descriptors in a multi-channel reformulations derived from the opponent color space. These chromatic representations are characterized by different levels of photometric invariance and discriminative power. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports# UCF11 and UCF50 action recognition benchmarks. Moreover# the results show that color STIPs are the single best low-level feature choice for STIP-based approaches to human action recognition.,Ivo Everts*# University of Amsterdam; Jan van Gemert# University of Amsterdam; Theo Gevers# ,ivoeverts@gmail.com; J.C.vanGemert@uva.nl; th.gevers@uva.nl,02.03 Feature descriptors*,02.03 Feature descriptors*,02.04 Feature detection,07.01 Recognition,, ,Features and Contours,P 3B
260,Supervised Kernel Descriptors for Visual Recognition,In visual recognition tasks# the design of low level image feature representation is fundamental. The advent of local patch features from pixel attributes such as SIFT# LBP have precipitated dramatic progresses. Recently# a kernel view of these features# called kernel descriptors (KDES)# generalizes the feature design in an unsupervised fashion and yields impressive results. In this paper# we present a supervised framework to embed the class label information into the design of kernel descriptors# which we call supervised kernel descriptors (SKDES). Specifically# we adopt the broadly applied image classification pipeline and a large margin criterion to learn the low-level image patch representation# which makes the patch feature much more compact and achieve better discriminative ability comparing to KDES.  With this method# we achieve competing results over several public datasets comparing with state-of-the-art methods.,Peng Wang*# Peking University; Jingdong Wang# Microsoft Research Asia; Gang Zeng# Peking University; Weiwei Xu# ; Hongbin Zha# ; Shipeng Li# Microsoft Research Asia,jerryking234@gmail.com; jingdw@microsoft.com; g.zeng@ieee.org; weiwei.xu.g@gmail.com; zha@cis.pku.edu.cn; shipeng.li@microsoft.com,02.03 Feature descriptors*,02.03 Feature descriptors*,07.02 Category recognition,,, ,Features and Contours,P 3B
1925,Discriminative Color Descriptors,Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events# such as shadows# shading# specularities# illuminant color changes# and changes in viewing geometry. Traditionally# this challenge has been addressed by capturing the variations in physics-based models# and deriving invariants for the undesired variations. The drawback of this approach is that the cost# with which the photometric invariance has been obtained# remains hidden. This cost results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation# which is based on other data sets than the one at hand# can obtain competing performance. Experiments show that the proposed discriminative color descriptor outperforms existing photometric invariants. Furthermore# we show that combined with shape description these color descriptors obtain state-of-the-art results on two challenging datasets# namely# Stanford-dogs 120 and Birds-200.,Rahat Khan*# Saint Etienne Univ. CNRS; Joost van de Weijer# Computer Vision Center Barcelona; Fahad Khan# ; Damien Muselet# Saint Etienne Univ. CNRS; Christophe Ducottet# Saint Etienne Univ. CNRS; Cecile Barat# Saint Etienne Univ. CNRS,rahat.khan@univ-st-etienne.fr; joost@cvc.uab.es; fahad@cvc.uab.es; damien.muselet@univ-st-etienne.fr; ducottet@univ-st-etienne.fr; cecile.barat@univ-st-etienne.fr,02.03 Feature descriptors*,02.03 Feature descriptors*,07.07 Object Recognition,,, ,Features and Contours,P 3B
8,Boosting Binary Keypoint Descriptors,Binary  keypoint  descriptors  provide   an  efficient  alternative  to  their floating-point competitors  as they  enable faster processing  while requiring less memory. In this paper# we propose a novel framework to learn an extremely compact  binary descriptor  we call  BinBoost that is very robust  to illumination and  viewpoint changes.  Each  bit of our descriptor  is computed with a  boosted binary hash function# and  we show how to  efficiently optimize the different hash functions so that they  complement each other# which is key to compactness and  robustness.  The hash functions  rely on weak learners  that are applied directly  to the image patches#  which frees us  from any intermediate representation  and lets  us automatically  learn the  image  gradient pooling configuration of the final descriptor.  Our resulting descriptor significantly outperforms the state-of-the-art binary descriptors and performs similarly to the best floating-point descriptors at a fraction of the matching time and memory footprint.,Tomasz Trzcinski*# EPFL; Mario Christoudias# EPFL; Pascal Fua# EPFL; Vincent Lepetit# EPFL,tomasz.trzcinski@epfl.ch; mario.christoudias@epfl.ch; pascal.fua@epfl.ch; vincent.lepetit@epfl.ch,02.03 Feature descriptors*,02.03 Feature descriptors*,,,, ,Features and Contours,P 3B
882,Exploring weak stabilization for motion feature extraction,We describe novel but simple motion features for the problem of detecting objects in video sequences. Previous approaches either compute optical flow or temporal differences on video frame pairs with various assumptions about stabilization. We describe a combined approach that uses coarse-scale flow and fine-scale temporal difference features. Our approach performs weak motion stabilization by factoring out camera motion and coarse object motion while preserving nonrigid motions that serve as useful cues for recognition. We show results for pedestrian detection and human pose estimation in video sequences# achieving state-of-the-art results in both. In particular# given a fixed detection rate our method achieves a five-fold reduction in false positives over prior art on the Caltech Pedestrian benchmark. Finally# we perform extensive diagnostic experiments to reveal what aspects of our system are crucial for good performance; proper stabilization# long time-scale features# and proper normalization are all critical.,Dennis Park*# UC Irvine; Larry Zitnick# MSR; Deva Ramanan# UCI; Piotr Dollar# ,iypark@ics.uci.edu; larryz@microsoft.com; dramanan@ics.uci.edu; pdollar@microsoft.com,02.03 Feature descriptors*,02.03 Feature descriptors*,,,, ,Features and Contours,P 3B
1325,Dense Segmentation-aware Descriptors,In this work we pursue the construction of invariant appearance descriptors in a manner that accommodates occlusion and background changes by leveraging segmentation information. We build on recent advances in the construction of dense scale- and rotation-invariant descriptors through the Fourier Transform Modulus technique# and incorporate into them soft segmentation masks# which allow us to treat occlusions or background motion during descriptor construction. We apply our approach to datasets on multi-layered motion estimates and wide-baseline stereo. We show remarkably better performance than state-of-the-art invariant descriptors.,Eduard Trulls*# IRI (UPC/CSIC); Iasonas Kokkinos# ; Alberto Sanfeliu# ; Francesc Moreno-Noguer# Institut de Robotica i Informatica Industrial (UPC/CSIC),etru1927@gmail.com; iasonas.kokkinos@ecp.fr; sanfeliu@iri.upc.edu; fmoreno@iri.upc.edu,02.03 Feature descriptors*,02.03 Feature descriptors*,,,, ,Features and Contours,P 3B
1567,Keypoints from Symmetries by Wave Propagation,The paper conjectures and demonstrates that repeatable keypoints based on salient symmetries at different scales can be detected by a novel scale-space analysis grounded on the wave equation rather than the heat equation underlying traditional Gaussian scale--space theory. While the image structures found by most state-of-the-art detectors# such as blobs and corners# occur typically on planar highly textured surfaces# salient symmetries are widespread in diverse kinds of images# including those related to texture-less objects# which are hardly dealt with by current feature-based recognition pipelines.  We report experimental results on standard datasets and also contribute with a new dataset specifically focused on texture-less objects. Based on the positive experimental findings# we hope to foster further research activity on the novel promising topic of scale-space analysis through the wave equation.,Samuele Salti*# DISI# University of Bologna; Alessandro Lanza# CIRAM# University of Bologna; Luigi Di Stefano# DISI# University of Bologna,samuele.salti@unibo.it; alanza@arces.unibo.it; luigi.distefano@unibo.it,02.04 Feature detection*,02.04 Feature detection*,01.10 Multi-scale processing,01.11 PDEs and level-set methods,, ,Features and Contours,P 3B
680,Graph Matching with Anchor Nodes: A Learning Approach,In this paper# we consider the weighted graph matching problem with partially disclosed correspondences between some anchor nodes. Our construction employs the recently introduced node signatures based on graph Laplacians# namely the Laplacian Family Signature (LFS) on nodes# and the pairwise heat kernel map on edges. In this paper# without assuming an explicit form of parametric dependence nor a distance metric between node signatures# we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving the problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term# we then set up an integer quadratic problem (IQP) to solve for a near optimal matching. In our experiments# we show superior performance of our approach on randomly generated graphs and two widely-used image sequences in comparison with other existing signature based and adjacency matrix based matching methods. ,Nan Hu*# Stanford University; Raif Rustamov# ; Leonidas Guibas# ,nanhu@stanford.edu; rustamov@stanford.edu; guibas@cs.stanford.edu,02.05 Feature matching and indexing*,02.05 Feature matching and indexing*,02.01 Feature extraction and matching,03.05 Shape Representation and Matching,, ,Features and Contours,P 3B
1039,Dense Non-Rigid Point-Matching Using Random Projections,We present a robust and efficient technique for matching dense sets of points undergoing non-rigid spatial transformations. Our main intuition is that the subset of points that can be matched with high confidence should be used to guide the matching procedure for the rest. We propose a novel algorithm that incorporates these high-confidence matches as a spatial prior to learn a discriminative subspace that simultaneously encodes both the feature similarity as well as their spatial arrangement. Conventional subspace learning usually requires spectral decomposition of the pair-wise distance matrix across the point-sets# which can become inefficient even for moderately sized problems. To this end# we propose the use of random projections for approximate subspace learning# which can provide significant time improvements at the cost of minimal precision loss. This efficiency gain allows us to iteratively find and remove high-confidence matches from the point sets# resulting in high recall. To show the effectiveness of our approach# we present a systematic set of experiments and results for the problem of dense non-rigid image-feature matching.,Raffay Hamid*# eBay; Dennis Decoste# eBay Research; Chih-Jen Lin# National Taiwan University,raffay@gmail.com; ddecoste@ebay.com; cjlin@csie.ntu.edu.tw,02.05 Feature matching and indexing*,02.05 Feature matching and indexing*,02.01 Feature extraction and matching,03.05 Shape Representation and Matching,, ,Features and Contours,P 3B
73,Deformable Graph Matching,  Graph matching (GM) is a fundamental problem in computer science# and it has been successfully applied to many problems in computer vision. Although widely used# existing GM algorithms cannot incorporate global consistence among nodes# which is a natural constraint in computer vision problems. This paper proposes deformable graph matching (DGM)# an extension of GM for matching graphs subject to global rigid and non-rigid geometric constraints.  The key idea of this work is a new factorization of the pair-wise affinity matrix. This factorization decouples the affinity matrix into the local structure of each graph and the pair-wise affinity edges. Besides the ability to incorporate global geometric transformations# this factorization offers three more benefits. First# it provides a unified view of many GM methods and extends the standard iterative closest point algorithm. Second# it allows the use of path-following optimization algorithms that lead to improved optimization strategies and matching performance. Third# there is no need to compute the costly (in space and time) pair-wise affinity matrix. Experimental results on synthetic and real databases illustrate how DGM outperforms state-of-the-art algorithms for GM.,Feng Zhou*# Carnegie Mellon University; Fernando DelaTorre# ,zhfe99@gmail.com; ftorre@cs.cmu.edu,02.05 Feature matching and indexing*,02.05 Feature matching and indexing*,03.05 Shape Representation and Matching,04.02 Image alignment,, ,Features and Contours,P 3B
707,Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images,We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene# given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features# and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel# so no interest point detectors are required. The camera pose is inferred using a RANSAC optimization scheme. This starts with an initial set of hypothesized camera poses# constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest# counting inliers# and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.,Jamie Shotton*# MSR; Ben Glocker# Microsoft Research; Christopher Zach# Microsoft Research; Shahram Izadi# ; Antonio Criminisi# ; Andrew Fitzgibbon# Microsoft,Jamie.Shotton@microsoft.com; glocker@microsoft.com; chzach@microsoft.com; Shahrami@microsoft.com; antcrim@microsoft.com; awf@microsoft.com,02.05 Feature matching and indexing*,02.05 Feature matching and indexing*,04.04 Model-based reconstruction and tracking,05.02 Active rangefinding and depth sensors,, ,Features and Contours,P 3B
257,K-means Hashing: an Affinity-Preserving Quantization Method for Learning Binary Compact Codes,In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating distance-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper# we propose a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We show that this is a nontrivial problem# and a naive two-step solution is sub-optimal and not feasible. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method# named as K-means Hashing# outperforms various state-of-the-art hashing encoding methods for approximate nearest neighbors (ANN) search.,Kaiming He*# Mirosoft Research Asia; Fang Wen# Microsoft Research Asia; Jian Sun# Microsoft Research Asia,kahe@microsoft.com; fangwen@microsoft.com; jiansun@microsoft.com,02.05 Feature matching and indexing*,02.05 Feature matching and indexing*,,,, ,Features and Contours,P 3B
694,Optimized Product Quantization for Approximate Nearest Neighbor Search,Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search# but still remains unaddressed. In this paper# we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a non-parametric method that alternatively solves two smaller sub-problems# and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.,Tiezheng Ge# USTC; Kaiming He*# Mirosoft Research Asia; Qifa Ke# Microsoft Research Silicon Valley; Jian Sun# Microsoft Research Asia,getzh@mail.ustc.edu.cn; kahe@microsoft.com; qke@microsoft.com; jiansun@microsoft.com,02.05 Feature matching and indexing*,02.05 Feature matching and indexing*,,,, ,Features and Contours,P 3B
2084,A Non-Parametric Framework for Document Bleed-Through Removal,This paper presents recent work on a new framework for non-blind document bleed-through removal including image preprocessing to remove local intensity variations# pixel region classification based on a segmentation of the two dimensional recto-verso intensity histogram and connected component analysis on the subsequent image labelling. Finally restoration of the degraded regions is performed using exemplar-based image inpainting. The proposed method is evaluated visually and numerically on a freely available database of 25 scanned manuscript image pairs with ground truth# and is shown to outperform three recent non-blind bleed-through removal techniques.,RÃ³isÃ­n Rowley-Brooke*# Trinity College Dublin; FranÃ§ois PitiÃ©# ; Anil Kokaram# ,rowleybr@tcd.ie; fpitie@tcd.ie; anil.kokaram@tcd.ie,10.02 Document Analysis*,10.02 Document Analysis*,01.01 Image processing,01.08 Image enhancement# restoration# and denoising,01.13 Texture analysis and synthesis, ,Features and Contours,P 3B
784,Scene Text Recognition using Part-based Tree-structured Character Detections,Scene text recognition has inspired great interests from the computer vision community in recent years. In this paper# we propose a novel scene text recognition method using part-based tree-structured character detections. Different from conventional multi-scale sliding window character detection strategy# which would bring a lot of unwanted false positives# we use part-based tree-structure to model each type of character so as to detect and recognize the characters at the same time. While for word recognition# we build a Conditional Random Field model on the potential character locations to incorporate the detection scores# spatial constraints and linguistic knowledge into one framework. The final word recognition result is obtained by minimizing the cost function defined on the random field. Experimental results on a range of challenging public datasets (ICDAR 2003# ICDAR 2011# SVT) demonstrate that the proposed method outperforms state-of-the-art methods significantly both for character detection and word recognition.,Cunzhao Shi*# Institute of Automation# CAS,cunzhao.shi@ia.ac.cn,10.02 Document Analysis*,10.02 Document Analysis*,07.04 Image and Video Retrieval,,, ,Features and Contours,P 3B
328,Active Contours with Group Similarity,Active contours are widely used in image segmentation. To cope with missing or misleading features in the image# researchers have introduced various ways to model the prior of shapes and use the prior to constrain active contours. However# the shape prior is usually learnt from a large set of annotated data# which is not always accessible in practice. Moreover# it is often doubted that the existing shapes in the training set will be sufficient to model the new instance in the testing image. In this paper# we propose to use the group similarity of object shapes in multiple images as a prior to aid segmentation# which can be interpreted as an unsupervised approach of shape prior modeling. We show that the rank of the matrix consisting of multiple shapes is a good measure of the group similarity of the shapes# and the nuclear norm minimization is a simple and effective way to impose the proposed constraint on existing active contour models. Moreover# we develop a fast algorithm to solve the proposed model by using the accelerated proximal method. Experiments using echocardiographic image sequences acquired from acute canine experiments demonstrate that the proposed method can consistently improve the performance of active contour models and increase the robustness against image defects such as missing boundaries.,Xiaowei Zhou*# HKUST; Xiaojie Huang# ; James Duncan# Yale University; Weichuan Yu# HKUST,eexwzhou@ust.hk; xiaojie.huang@yale.edu; james.duncan@yale.edu; eeyu@ust.hk,03.02 Active contours and surfaces*,03.02 Active contours and surfaces*,03.01 Segmentation and 2D shape,10.03 Medical Image Analysis,, ,Features and Contours,P 3B
1696,Accurate and Robust Registration of Nonrigid Surface using Hierarchical Statistical Shape Model,In this paper# we propose a new non-rigid robust registration method that registers a point distribution model (PDM) of a surface to given 3D images. The contributions of the paper are (1) a new hierarchical statistical shape model (SSM) of the surface that has better generalization ability is introduced# (2) the registration algorithm of the hierarchical SSM that can estimate the marginal posterior distribution of the surface location is proposed# and (3) the registration performance is improved by (3-1) robustly registering each local shape of the surface with the sparsity regularization and by (3-2) referring to the appearance between the neighboring model points in the likelihood computation. The SSM of a liver was constructed from a set of clinical CT images# and the performance of the proposed method was evaluated.  Experimental results demonstrated that the proposed method outperformed some existing methods that use non-hierarchical SSMs.,Hidekata HONTANI*# Nagoya Institute of Technology; Yuto Tsunekawa# Nagoya Institute of Technology; Yoshihide Sawada# Nagoya Institute of Technology,hontani@nitech.ac.jp; tunekawa@iu.nitech.ac.jp; sawada@iu.nitech.ac.jp,03.02 Active contours and surfaces*,03.02 Active contours and surfaces*,08.04 Graphical models,10.03 Medical Image Analysis,, ,Features and Contours,P 3B
1648,Manhattan Junction Catalogue for Spatial Understanding of Indoor Scenes,Junctions are strong cues for understanding the geometry of a scene. In this paper# we consider the problem of detecting junctions and using them for recovering the spatial layout of an indoor scene. Junction detection has always been challenging due to missing and spurious lines. We work in a constrained Manhattan world setting where the junctions are formed by only line segments along the three principal orthogonal directions. Junctions can be classified into several categories based on the number and orientations of the incident line segments. We provide a simple and efficient voting scheme to detect and classify these junctions in real images. Indoor scenes are typically modeled as cuboids and we formulate the problem of the cuboid layout estimation as an inference problem in a conditional random field. Our formulation allows the incorporation of features based on the detected junctions and the training is done using structured prediction techniques. We outperform other single view geometry estimation methods on standard dataset using the proposed junction features. ,Srikumar  Ramalingam*# MERL; Jai Shanker Pillai# University of Maryland# College Park; Arpit Jain# University of Maryland; Yuichi Taguchi# Mitsubishi Electric Research Labs,ramalingam@merl.com; jsp@umiacs.umd.edu; ajain@umd.edu; taguchi@merl.com,07.03 Context and scene understanding*,07.03 Context and scene understanding*,02.01 Feature extraction and matching,,, ,Objects and Scenes,P 3C
1407,Tensor-based High-order Semantic Relation Transfer for Semantic Scene Segmentation,We propose a novel nonparametric approach for semantic scene segmentation by incorporating high-order semantic relations. Conventional context models mainly focus on learning pairwise relationships between objects. Pairwise relations# however# are not enough to represent high-level contextual knowledge within images. In this paper# we propose semantic relation transfer# a method to transfer high-order semantic relations of objects from annotated images to unlabeled images analogous to label transfer techniques where label information are transferred. We first define semantic tensor representing high-order relations of objects. Semantic relation transfer problem is then formulated as semi-supervised learning with quadratic optimization function of semantic tensor. By exploiting low-rank property of the semantic tensors# an efficient approximation algorithm is developed. We demonstrate that the proposed tensor-based semantic relation transfer outperforms the state-of-the-art label transfer approaches on several challenging datasets.,Heesoo Myeong# Seoul National University; Kyoung Mu Lee*# Seoul National University,rapper00@snu.ac.kr; kyoungmu@snu.ac.kr,07.03 Context and scene understanding*,07.03 Context and scene understanding*,03.03 Image segmentation,07.02 Category recognition,07.07 Object Recognition, ,Objects and Scenes,P 3C
600,Geometric Context From Videos,We present a novel algorithm for estimating the broad 3D geometric structure of outdoor video scenes. Leveraging spatio-temporal video segmentation# we decompose a dynamic scene captured by a video into geometric classes# based on predictions made by region-classifiers that are trained on appearance and motion features. By examining the homogeneity of the prediction# we combine predictions across multiple segmentation hierarchy levels alleviating the need to determine the granularity a-priori. We evaluate our method on a novel# challenging dataset for geometric context of video# consisting of over 100 ground-truth annotated outdoor videos with over 20#000 frames. To further scale beyond this dataset# we propose a semi-supervised learning framework to expand the pool of labeled by data with high confidence predictions obtained from unlabeled data. Our system produces an accurate prediction of geometric context of video achieving 96% accuracy across main geometric classes.,Syed Raza*# Gatech; Matthias Grundmann# ; Irfan Essa# GaTech,hussain.raza@gatech.edu; grundman@cc.gatech.edu; irfan@cc.gatech.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,03.04 Segmentation and Grouping,04.05 Object Tracking and Motion Analysis,07.07 Object Recognition, ,Objects and Scenes,P 3C
1272,It's Not Polite To Point: Describing People With Uncertain Attributes, Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication with humans. Since many attributes are nameable# the computer is able to communicate these concepts through language. However# doing so is not an easy task. Given a set of attributes# selecting a subset to be communicated is not trivial# and is task dependent. Moreover# because attribute classifiers are noisy# it is important to find ways to deal with this uncertainty. In this paper we address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes  him from the others. We develop an efficient method for choosing which attributes should be mentioned in a short description in order to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers# and show the strength of our method in creating effective descriptions. ,Amir Sadovnik*# Cornell University; Andrew Gallagher# ; Tsuhan Chen# ,as2373@cornell.edu; andrew.c.gallagher@gmail.com; tsuhan@ece.cornell.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,07.01 Recognition,,, ,Objects and Scenes,P 3C
285,Heterogeneous Visual Features Fusion via Sparse Multimodal Machine,To better understand# search# and classify image and video information# many visual feature descriptors have been proposed to describe elementary visual characteristics# such as the shape# the color# the texture# etc. How to integrate these heterogeneous visual features and identify the important ones from them for specific vision tasks has become an increasingly critical problem. In this paper# We propose a novel Sparse Multimodal Learning (SMML) approach to integrate such heterogeneous features by using the joint structured sparsity regularizations to learn the feature importance of for the vision tasks from both group-wise and individual point of views. A new optimization algorithm is also introduced to solve the non-smooth objective with rigorously proved global convergence. We applied our SMML method to five broadly used object categorization and scene understanding image data sets for both single-label and multi-label image classification tasks. For each data set we integrate six different types of popularly used image features. Compared to existing scene and object categorization methods using either single modality or multi-modalities of features# our approach always achieves a better performance measured.   ,Hua Wang*# Colorado School of Mines; Feiping Nie# University of Texas at Arlington; Heng Huang# UTA,huawangcs@gmail.com; feipingnie@gmail.com; heng@uta.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,07.04 Image and Video Retrieval,08.09 Statistical Methods and Learning,, ,Objects and Scenes,P 3C
251,A Max-Margin Riffled Independence Model for Image Tag Ranking,We propose Max-Margin Riffled Independence Model (MMRIM)# a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image# where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [9]# which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval# and demonstrate that MMRIM significantly improves the accuracy of these applications.,Tian Lan*# Simon Fraser University; Greg Mori# ,taran.lan1986@gmail.com; mori@cs.sfu.ca,07.03 Context and scene understanding*,07.03 Context and scene understanding*,07.07 Object Recognition,,, ,Objects and Scenes,P 3C
1638,Weakly Supervised Learning for Attribute Localization in Outdoor Scenes,In this paper# we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection of images associated with attributes in text# where the precise localization of the each attributes left unknown. Our method includes three aspects. {\em (i) Compositional scene configuration}. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation which can generate an excessive number of scene configurations through hierarchical composition of a relatively small number of parts. {\em (ii) Attribute association}. The scene attributes contains nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using non-maximum suppression of their correlation# then train an appearance model for each noun-adjective attribute pair.  {\em (iii) Joint inference and learning}. For an image# we compute the most probable parse tree with the attributes assigned to nodes in the parse tree as an instance of the HST by dynamic programming. Then update the HST and attribute association based on the inferred parse trees. We evaluate the proposed method by (a) showing the improvement of attribute recognition accuracy; and (b) comparing the average precision of localizing attributes to the scene parts.,Shuo Wang*# Peking University; Yizhou Wang# Peking University ; Song Chun Zhu# UCLA,shuoshuo.w@gmail.com; yizhou.wang@pku.edu.cn; sczhu@stat.ucla.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,10.01 Aerial and outdoor image analysis and modeling,,, ,Objects and Scenes,P 3C
134,Scene Parsing by Integrating Function# Geometry and Appearance Models,"Indoor functional objects exhibit large view and appearance variations# and thus are difficult to be recognized by the traditional appearance-based classification paradigm. In this paper# we present an algorithm to parse indoor images based on two observations: i) The Functionality is the most essential property to define an indoor object# e.g. ""a chair to sit on"";  ii) The Geometry (3D shape) of an object is designed to serve its own functions. We formulate the nature of the object functionality into a stochastic generative model. This model characterizes a joint distribution over the function-geometry-appearance (FGA) hierarchy. The hierarchical structure includes a scene category# functional groups# functional objects# functional parts and 3D geometric shapes. We use a simulated annealing MCMC method to maximize the joint posterior distribution. We design four data-driven steps to travel across the FGA space. The iterative steps first group the line segments into 3D primitive shapes# and then assign functional labels to theses 3D shapes# fill missing objects/parts by their assigned functional labels# and last verify the scene parse tree by the MH acceptance probability. The experimental results on several challenging indoor datasets demonstrate the advantage of inferring functionality and indicate that our model achieves the best object detection and scene segmentation results to date.",Yibiao Zhao*# UCLA; Song Chun Zhu# UCLA,ybzhao@ucla.edu; sczhu@stat.ucla.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,,,, ,Objects and Scenes,P 3C
376,Beyond Point Cloud: Scene Understanding by Reasoning Geometry and Physics,In this paper# we present an approach for scene understanding by reasoning physical stability of objects from  point cloud. We utilize a simple observation  that# by human design# objects in static scenes should be stable and safe with respect to gravity and various disturbances. This assumption is applicable to all scene categories and  poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning -- recovering solid 3D volumetric primitives from defective point cloud;  and 2) physical reasoning -- grouping the unstable primitives  to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization.  In experiments# we demonstrate that the algorithm achieves  substantially better  performance for i) object segmentation# ii) 3D volumetric recovery of the scene# and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset (to be released).,Bo Zheng*# Univ. of Tokyo; Yibiao Zhao# UCLA; ChengCheng Yu# UCLA; Song Chun Zhu# UCLA; Katsushi Ikeuchi# Univerity of Tokyo,zheng@cvl.iis.u-tokyo.ac.jp; ybzhao@ucla.edu; ycc.lhi@gmail.com; sczhu@stat.ucla.edu; ki@cvl.iis.u-tokyo.ac.jp,07.03 Context and scene understanding*,07.03 Context and scene understanding*,,,, ,Objects and Scenes,P 3C
500,Label Propagation from ImageNet to 3D Point Clouds,"Recent years have witnessed a growing interest in understanding the semantics of point clouds in a wide variety of applications. However# point cloud labeling remains an open problem# due to the difficulty in acquiring sufficient 3D point labels towards training effective classifiers. In this paper# we overcome this challenge by utilizing the existing massive 2D semantic labeled datasets from decade-long community efforts# such as ImageNet and LabelMe# and a novel ""cross-domain"" label propagation approach. Our proposed method consists of two major novel components# Exemplar SVM based label propagation# which effectively addresses the cross-domain issue# and a graphical model based contextual refinement incorporating 3D constraints. Most importantly# the entire process does not require any training data from the target scenes# also with good scalability towards large scale. We evaluate our approach on the well-known Cornell Point Cloud Dataset# achieving much greater efficiency and comparable accuracy even with no 3D training data used. Our approach shows major gains in accuracy when the training data from the target scenes is used# outperfroming state-of-the-art approaches with far better efficiency.",Yan Wang*# Columbia University; Rongrong Ji# Columbia University; Shih-Fu Chang# Columbia University,yanwang@ee.columbia.edu; rrji@ee.columbia.edu; sfchang@ee.columbia.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,,,, ,Objects and Scenes,P 3C
678,Analyzing Semantic Segmentation Using Human-Machine Hybrid CRFs,"Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks like object detection# scene recognition# shape analysis# contextual reasoning# local appearance based classifiers# etc. In this work# we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal# we ""plug-in"" human subjects for each of the various components in a state-of-the-art Conditional Random Field model on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much ""head room"" there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels# while being worse than current machine classifiers# provides a significant boost in performance when plugged into the CRF! Fascinated by this finding# we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance.",Roozbeh Mottaghi*# UCLA; Sanja Fidler# TTI chicago; Jian Yao# ; Raquel Urtasun# TTI Chicago; Devi Parikh# TTIC,roozbehm@cs.ucla.edu; sanja.fidler@gmail.com; yzyaojian@gmail.com; rurtasun@ttic.edu; dparikh@ttic.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,,,, ,Objects and Scenes,P 3C
2169,Nonparametric scene parsing with adaptive feature relevance and semantic context,This paper presents a nonparametric approach to semantic parsing using small patches and simple gradient# color and location features. We propose to learn the relevance of individual feature channels at test time using a locally adaptive distance metric. To further improve the accuracy of the non-parametric approach# we examine the importance of the retrieval set used to compute the nearest neighbours and propose a novel semantic descriptor to retrieve better candidates. The approach is validated by experiments on several datasets used for semantic parsing demonstrating the superiority of the method compared to the state of art approaches.,Gautam Singh*# George Mason University; Jana  Kosecka# George Mason University,gsinghc@cs.gmu.edu; kosecka@cs.gmu.edu,07.03 Context and scene understanding*,07.03 Context and scene understanding*,,,, ,Objects and Scenes,P 3C
621,Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection,We propose a novel approach to both learning and detecting local contour-based representations for mid-level features. Our features# called sketch tokens# are learned using supervised mid-level information in the form of hand drawn contours in images. Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. We demonstrate our approach on both top-down and bottom-up tasks. We show state-of-the-art results on the top-down task of contour detection while being over 100x faster than competing methods. We also achieve large improvements in detection accuracy for the bottom-up tasks of pedestrian and object detection as measured on INRIA and PASCAL# respectively# due to the complementary information provided by sketch tokens to low-level features such as gradient histograms.,Joseph Lim*# MIT; Larry Zitnick# MSR; Piotr Dollar# ,lim@csail.mit.edu; larryz@microsoft.com; pdollar@microsoft.com,07.06 Object Detection*,07.06 Object Detection*,02.02 Edge and contour detection,07.07 Object Recognition,, ,Objects and Scenes,P 3C
1639,Saliency Detection via Graph-Based Manifold Ranking,Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image# whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions# we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries# based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed.,Chuan Yang*# DLUT; Lihe Zhang# ; Huchuan Lu# Dalian University of Technolog; Ming-Hsuan Yang# UC Merced; Xiang Ruan# omron corporation,ycscience86@gmail.com; zhanglihe@dlut.edu.cn; lhchuan@dlut.edu.cn; mhyang@ucmerced.edu; gen@omm.ncl.omron.co.jp,07.06 Object Detection*,07.06 Object Detection*,03.03 Image segmentation,,, ,Objects and Scenes,P 3C
951,Maximum Cohesive Grid of Superpixels for Fast Object Localization,This paper addresses a challenging problem to regularize arbitrary superpixels into an optimal grid structure# which may significantly extend current low-level vision algorithms by allowing them to use superpixels (SPs) conveniently as using pixels. For this purpose# we aim at constructing maximum cohesive SP-grid# which is composed of real nodes# \ie SPs# and dummy nodes that are meaningless in the image with only position-taking function in the grid. For a given formation of image SPs and proper number of dummy nodes# we first dynamically align them into a grid based on the centroid localities of SPs. We then define the SP-grid coherence as the sum of edge weights (with SP locality and appearance encoded) along all direct paths connecting any two nearest neighboring real nodes in the grid. We finally maximize the SP-grid coherence by cascade dynamic programming. Our approach can take the regional objectness as an optional constraint to produce more semantically reliable SP-grids. \if 0 to the SP coherence measurement# thus leading to more semantically feasible SP-grids.\fi We have tested the proposed approach in the task of object localization. Extensive experiments show that our approach outperforms state-of-the-art methods in terms of both detection accuracy and speed. We also find that with the same searching strategy and features# object localization at SP-level is about $100$-$500$ times faster than pixel-level# with usually better detection accuracy.,Liang Li# Tianjin University; Wei Feng*# Tianjin University; Liang Wan# Tianjin University; Jiawan Zhang# Tianjin University,allen.liliang@gmail.com; wfeng@ieee.org; lwan@ieee.org; jwzhang@tju.edu.cn,07.06 Object Detection*,07.06 Object Detection*,03.04 Segmentation and Grouping,07.04 Image and Video Retrieval,, ,Objects and Scenes,P 3C
735,Accurate Localization of 3D Objects from RGB-D Data using Segmentation Hypotheses,In this paper we focus on the problem of finding objects in 3D from RGB-D images. We propose a novel framework that explores the compatibility between segmentation hypotheses of the object in the image and the corresponding 3D map. Our framework allows to discover the optimal location of the object in 3D using a generalization of the structural latent SVM formulation in 3D as well as the definition of a new loss function defined over 3D space in training. We evaluate our method using two existing RGB-D datasets. Extensive quantitative and qualitative experimental results show that our proposed approach outperforms state-of-the-art as well as a number of baseline methods  for both 3D and 2D object recognition tasks.,Byung-soo Kim*# University of Michigan; Shili Xu# ; Silvio Savarese# University of Michigan,bsookim@umich.edu; stevenxu@umich.edu; silvio@eecs.umich.edu,07.06 Object Detection*,07.06 Object Detection*,07.07 Object Recognition,10.05 Robot vision,, ,Objects and Scenes,P 3C
754,Efficient Maximum Appearance Search for Large-Scale Object Detection,In recent years# efficiency of large-scale object detection has arisen as an important topic due to the exponential growth in the size of benchmark object detection datasets. Most current object detection methods focus on improving accuracy of large-scale object detection with efficiency being an afterthought. In this paper# we present the Efficient Maximum Appearance Search (EMAS) model which is an order of magnitude faster than the existing large-scale object detection approaches# while maintaining state-of-the-art accuracy.   Our EMAS approach consists of representing an image as an ensemble of densely sampled feature points with the proposed Pointwise Fisher Vector encoding method# so that the learnt discriminative scoring function can be applied locally.  Consequently# the object detection problem is transformed into searching an image sub-area for maximum local appearance probability# thereby making EMAS an order of magnitude faster than the traditional detection methods. In addition# the proposed model is also suitable for incorporating global context at a negligible extra computational cost. EMAS can also incorporate fusion of multiple features# which greatly improves its performance in detecting multiple object categories. Our experiments show that the proposed algorithm can perform detection of 1000 object classes in less than one minute on the Image Net ILSVRC2012 datasets and 107 object classes in less than 5 seconds per image on the SUN09 dataset using a single CPU# while maintaining state-of-the-art accuracy.  ,Qiang Chen*# NUS; Zheng  Song# NUS; Rogerio Feris# ; Ankur Datta# IBM; Liangliang Cao# IBM Research Center; Zhongyang Huang# PSL; Shuicheng YAN# NUS,chenqiang@nus.edu.sg; zheng.s@nus.edu.sg; rsferis@us.ibm.com; ankurd@us.ibm.com; liangliang.cao@gmail.com; ZhongYang.Huang@sg.panasonic.com; eleyans@nus.edu.sg,07.06 Object Detection*,07.06 Object Detection*,07.07 Object Recognition,,, ,Objects and Scenes,P 3C
1175,Single-Pedestrian Detection aided by Multi-pedestrian Detection,In this paper# we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors# and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load.  15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech# TUD-Brussels and ETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset# 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate.  The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset# from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.,Wanli Ouyang*# The Chinese University of HK; Xiaogang Wang# The Chinese University of Hong Kong,wlouyang@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk,07.06 Object Detection*,07.06 Object Detection*,07.07 Object Recognition,,, ,Objects and Scenes,P 3C
1290,Robust Object Co-Detection,Object co-detection aims at simultaneous detection of objects of the same category from a pool of related images by exploiting the consistent visual pattern present in the candidate objects in the images. The related image set may contain a mixture of annotated objects and candidate objects generated by automatic detectors. Co-detection differs from the conventional object detection paradigm in which detection over each test image is only decided one by one independently without taking advantage of the common pattern in the data pool. In this paper# we propose a novel# robust approach to dramatically enhancing co-detection by extracting a shared low-rank representation of the object instances in multiple feature spaces. The ideas are analogous to that of the well-known robust PCA# but have not been explored in object co-detection so far. The representation is based on a linear reconstruction over the entire data set and the low-rank approach enables effective removal of noisy and outlier samples. The extracted low-rank representation can be used to detect the target objects by spectral clustering. Extensive experiments over diverse benchmark datasets demonstrate consistent and significant performance gains of the proposed method over the state-of-the-art.,Xin Guo*# BUPT; Dong Liu# Columbia University; Brendan Jou# Columbia University; Mojun Zhu# Columbia University; Anni Cai# BUPT; Shih-Fu Chang# Columbia University,guoxin523@gmail.com; dongliu@ee.columbia.edu; bwj2105@columbia.edu; mz2330@columbia.edu; annicai@bupt.edu.cn; sfchang@ee.columbia.edu,07.06 Object Detection*,07.06 Object Detection*,07.07 Object Recognition,08.09 Statistical Methods and Learning,, ,Objects and Scenes,P 3C
405,Integrating Grammar and Segmentation for Human Pose Estimation,In this paper we present a compositional grammar model for human pose estimation. Our model has three distinguishing features: (i) parts are hierarchically defined and can be substituted with alternative variants (ii) each variant can define its own articulated geometry and compatibility among its constituent parts# and (iii) the part appearance models utilize both foreground and image-specific background information. The resulting integrated framework allows us to learn a rich space of articulated poses# and more importantly the interdependencies between how variants of neighboring parts are selected# and how these selections influence the pose and appearance. Our model is trained discriminatively in a max-margin framework# and inference can be computed exactly and efficiently. We also present experimental evaluation of our method on two popular datasets# and show considerable performance improvements over the state-of-art on both benchmarks.,Brandon Rothrock*# UCLA; Seyoung Park# UCLA; Song Chun Zhu# UCLA,rothrock@cs.ucla.edu; seypark@cs.ucla.edu; sczhu@stat.ucla.edu,07.06 Object Detection*,07.06 Object Detection*,07.08 Part-based recognition,,, ,Objects and Scenes,P 3C
1176,Modeling Mutual Visibility Relationship in Pedestrian Detection,Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. The difficulty is added when several pedestrians overlap in the image region and occlude each other. We observe# however# that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. In this paper# we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. Compared with existing image-based pedestrian detection approaches# our approach has the lowest average miss rate on the Caltech-Train dataset# the Caltech-Test dataset and the ETH dataset. Including mutual visibility leads to 4% - 8% improvements on multiple benchmark datasets.,Wanli Ouyang*# The Chinese University of HK; Xingyu Zeng# ; Xiaogang Wang# The Chinese University of Hong Kong,wlouyang@ee.cuhk.edu.hk; xyzeng@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk,07.06 Object Detection*,07.06 Object Detection*,07.08 Part-based recognition,09.07 Person detection and tracking,, ,Objects and Scenes,P 3C
1143,Learning to Detect Partially Overlapping Instances,The objective of this work is to detect all instances of a class (such as cells or people) in an image. The instances may be partially overlapping and clustered# and hence quite challenging for a template style detector.  Our approach is to propose a set of candidate regions# and then select regions based on optimizing a global classification score# subject to the constraint that the selected regions are non-overlapping.  Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. For example# that a region may contain one instance or two object instances. We show that this formulation can be learnt within the structured output SVM framework using an efficient algorithm on a tree structured region graph. Furthermore# the learning only requires weak annotation -- a dot on each instance.  The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on two quite disparate data sets: fluorescence microscopy images and UCSD pedestrians.,Carlos Arteta*# University of Oxford; Victor Lempitsky# ; Alison Noble# University of Oxford; Andrew Zisserman# Oxford,carlos.arteta@eng.ox.ac.uk; victorlempitsky@gmail.com; alison.noble@eng.ox.ac.uk; az@robots.ox.ac.uk,07.06 Object Detection*,07.06 Object Detection*,08.01 Learning# statistics# and inference,08.09 Statistical Methods and Learning,09.08 Video Surveillance, ,Objects and Scenes,P 3C
790,Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection,We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn# from a large corpus of unlabelled images# which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image# we are able to correctly select an object in over 42% of the images in the Pascal VOC 2007 dataset# substantially outperforming existing approaches. Furthermore# we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images can achieve a higher annotation accuracy than many weakly supervised approaches.,Parthipan Siva*# Queen Mary University of Londo; Chris Russell# ; Tao Xiang# Queen Mary University of London; Lourdes Agapito# ,psiva@eecs.qmul.ac.uk; chrisr@eecs.qmul.ac.uk; txiang@eecs.qmul.ac.uk; lourdes@dcs.qmul.ac.uk,07.06 Object Detection*,07.06 Object Detection*,"08.01 Learning# statistics# and inference""",,, ,Objects and Scenes,P 3C
1025,Histograms of Sparse Codes for Object Detection ,Object detection has seen huge progress in recent years# much thanks to the heavily engineered Histograms of Oriented Gradients (HOG) features. Can we go beyond gradients and do better than HOG? We provide an affirmative answer by proposing and investigating a sparse representation for object detection# Histograms of Sparse Codes (HSC). We compute sparse codes with dictionaries learned from data using K-SVD# and aggregate per-pixel sparse codes to form local histograms.  We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. To keep training (and testing) efficient# we apply dimension reduction by computing SVD on learned models# and adopt supervised training where latent positions of roots and parts are given externally e.g. from a HOG-based detector. By learning and using local representations that are much more expressive than gradients# we demonstrate large improvements over the state of the art on the PASCAL benchmark for both root-only and part-based models.,Xiaofeng Ren*# ; Deva Ramanan# UCI,xren.cn@gmail.com; dramanan@ics.uci.edu,07.06 Object Detection*,07.06 Object Detection*,08.08 Sparse coding and dictionaries,,, ,Objects and Scenes,P 3C
933,Efficient Detector Adaptation for Object Detection in a Video,In this work# we present a novel and efficient detector adaptation method which improves the performance of    an offline trained classifier (baseline classifier) by adapting it to  new test datasets.    We address two critical aspects of adaptation methods: generalizability and computational efficiency.   We propose an adaptation method# which can be applied to various baseline classifiers with high computational    efficiency. For a given new test video# we collect online samples in unsupervised manner and train a random fern adaptive classifier .    The adaptive classifier improves the precision of the baseline classifier by validating the obtained detection responses from baseline classifier as correct detections or  false alarms.  Experiments demonstrate  generalizability# computational efficiency and effectiveness of our method# as we    compare our method with state of the art approaches for the problem of human detection and show good performance with high computational efficiency  on two different  baseline classifiers.,Pramod Sharma*# USC; Ram Nevatia# ,pksharma@usc.edu; nevatia@usc.edu,07.06 Object Detection*,07.06 Object Detection*,09.01 Applications: humans,09.07 Person detection and tracking,09.08 Video Surveillance, ,Objects and Scenes,P 3C
1487,A Lazy ManÂ’s Approach to Benchmarking: Semisupervised Classifier  Evaluation and Recalibration,How many labeled examples are needed to estimate a classifier's performance on a new dataset? We study the case where data is plentiful# but labels are expensive.  We show that by making a few reasonable assumptions on the structure of the data# it is possible to estimate performance curves# with confidence bounds# using a small number of ground truth labels. Our approach# which we call Semisupervised Performance Evaluation (SPE)# is based on a generative model for the classifier's confidence scores. In addition to estimating the performance of classifiers on new datasets# SPE can be used to recalibrate a classifier by re-estimating the class-conditional confidence distributions.,Peter Welinder*# Caltech; Max Welling# UC Irvine; Pietro Perona# Caltech,welinder@caltech.edu; welling@ics.uci.edu; perona@caltech.edu,07.06 Object Detection*,10.04 Performance Evaluation and Databases*,10.07 Additional applications,,, ,Objects and Scenes,P 3C
113,Fast Object Detection with Entropy-Driven Evaluation,Cascade-style approaches to implementing ensemble classifiers can deliver significant speed-ups at test time. While highly effective# they remain challenging to tune and their overall performance depends on the availability of large validation sets to estimate rejection thresholds. These characteristics are often prohibitive and thus limit their applicability.  We introduce an alternative approach  to speeding-up classifier evaluation which overcomes these limitations. It involves maintaining a probability estimate of the class label at each intermediary response and stopping when the corresponding uncertainty becomes small enough. As a result# the evaluation terminates early based on the sequence of responses observed. Furthermore# it does so independently of the type of ensemble classifier used or the way it was trained. We show through extensive experimentation that our method provides 2 to 10 fold speed-ups# over existing state-of-the-art methods# at almost no loss in accuracy on a number of object classification tasks.,Raphael  Sznitman *# EPFL; Carlos Becker# EPFL; Francois Fleuret# IDIAP; Pascal Fua# EPFL,raphael.sznitman@epfl.ch; carlos.becker@epfl.ch; francois.fleuret@idiap.ch; pascal.fua@epfl.ch,07.06 Object Detection*,07.06 Object Detection*,,,, ,Objects and Scenes,P 3C
909,Discriminatively Trained And-Or Tree Models for Object Detection,This paper presents a method of discriminatively learning reconfigurable And-Or Tree (AOT) models for object detection from weakly labeled data. Two main issues are addressed: (i) To organize the space of latent structures effectively# including both appearance and geometry# we first quantize the image lattice using an overcomplete set of shape primitives (e.g. rectangular shapes with different sizes and aspect ratios)# and then organize them into a directed acyclic And-Or Graph (AOG) structure by exploiting their compositional relations. We allow overlaps between children nodes when combining them into a parent node# which is equivalent to introduce an appearance Ornode implicitly for the overlapped portion. (ii) To search the globally optimal AOT in the AOG space efficiently# we propose a dynamic programming (DP) algorithm consisting of two phases: (1) The bottom-up phase factorizes the scoring function based on the depth-first search of AOG. Appearance templates are discriminatively trained for terminalnodes and their error rates on validation dataset are calculated. Then# each encountered Or-node selects the child node with minimal error rate# and encountered And-nodes are treated as local deformable part-based models to calculate its error rate. (2) In top-down phase# we retrieve the globally optimal AOT by using the error rates to guide the traversal. The retrieved AOT is re-trained jointly using latent structural SVM. In experiments# we tested our method on PASCAL VOC2007 and VOC2010 detection benchmark of 20 object classes and obtained better performance than comparable state-of-the-art methods.,Xi Song*# Beijing Institute of Technolog; Tianfu Wu# UCLA; Yunde JIA# Beijing Inst. of Tech.; Song Chun Zhu# UCLA,songxi@bit.edu.cn; tfwu@stat.ucla.edu; jiayunde@bit.edu.cn; sczhu@stat.ucla.edu,07.06 Object Detection*,07.06 Object Detection*,,,, ,Objects and Scenes,P 3C
1563,Occlusion patterns for object class detection,Despite the success of recent object class recognition systems# the long-standing problem of partial occlusion remains a major challenge# and a principled solution is yet to be found. In this paper# we leave the beaten path of methods that treat occlusion as just another source of noise -- instead# we include the occluder itself into the modeling# by mining distinctive# reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular# we evaluate and compare models that range from standard object class detectors to hierachical# part-based representations of occluder/occludee pairs. In an extensive evaluation# we derive insights that can aid further developments in tackling the occlusion challenge. ,Bojan Pepikj*# Max Planck Institute for Infor; Michael Stark# Stanford; Peter Gehler# MPI for Intelligent Systems; Bernt Schiele# MPI Informatics,bpepikj@mpi-inf.mpg.de; mst@stanford.edu; pgehler@tuebingen.mpg.de; schiele@mpi-inf.mpg.de,07.06 Object Detection*,07.06 Object Detection*,,,, ,Objects and Scenes,P 3C
1700,Bottom-up Segmentation for Top-down Detection,In this paper we are interested in how semantic segmentation can help object detection. Towards this goal# we propose a novel deformable part-based model which exploits  region-based segmentation algorithms that  compute candidate object regions by bottom-up clustering follow by  ranking of  those regions.  Our approach allows every detection hypothesis to select a segment (including void)#  and scores each box in the image using both the traditional HOG filters as well as a set of novel segmentation features. Thus our model ``blends'' between the detector and the segmentation models. Since our features  can be computed very efficiently given the segments# we   maintain the same complexity as the original DPM.  We demonstrate the effectiveness of our approach in PASCAL VOC 2010# and show that when employing only a root filter our approach outperforms Dalal \& Triggs detector on {\bf all} classes# and achieves 13\% higher average AP. When employing the parts# we outperform  the original DPM  in $18$ out of $20$ classes and achieve an improvement of 8\% AP.  ,Sanja Fidler# TTI chicago; Roozbeh Mottaghi# UCLA; Alan Yuille# UCLA; Raquel Urtasun*# TTI Chicago,fidler@ttic.edu; roozbehm@cs.ucla.edu; yuille@stat.ucla.edu; rurtasun@ttic.edu,07.06 Object Detection*,07.06 Object Detection*,,,, ,Objects and Scenes,P 3C
1073,Composite Statistical Inference for Semantic Segmentation,In this paper we present an inference procedure for the semantic segmentation of images. Different from many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials# our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals with large spatial support and the objects present in the image.  Continuous latent variables that model the overlap between each object segmentation proposal and each ground truth object region are defined at the level of superpixels resulting from segment intersections. Inference for the optimal layout# involving segment \emph{refinement} and \emph{recombination}# as well as \emph{handling multiple interacting objects# even from the same class# in one image}# is jointly performed by maximizing the composite likelihood of the underlying model using an EM algorithm. In the PASCAL VOC segmentation challenge# the proposed approach obtains top accuracy and successfully handles images showing complex object interactions.,Fuxin Li*# Georgia Inst. of Tech.; Joao Carreira# University of Coimbra; Guy Lebanon# Georgia Institute of Technology;  Cristian Sminchisescu# ,fli@cc.gatech.edu; joaoluiscarreira@gmail.com; lebanon@cc.gatech.edu; cristian.sminchisescu@ins.uni-bonn.de,07.07 Object Recognition*,07.07 Object Recognition*,03.04 Segmentation and Grouping,"08.01 Learning# statistics# and inference""",, ,Objects and Scenes,P 3C
2141,Multi-Attribute Queries: To Merge or Not to Merge?,Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently# and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query# which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions \emph{without} explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discriminative binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improve performance significantly over several strong baselines# while being an order of magnitude faster than exhaustively searching through all possible conjunctions. ,Mohammad Rastegari*# University of Maryland; Devi Parikh# TTIC; Ali  Diba# Sharif university of technology; Ali Farhadi# CMU,mrastega@cs.umd.edu; dparikh@ttic.edu; ali.diba67@gmail.com; afarhadi@cs.cmu.edu,07.07 Object Recognition*,07.07 Object Recognition*,07.01 Recognition,,, ,Objects and Scenes,P 3C
2006,Local Fisher Discriminant Analysis for Pedestrian Re-identification,Metric learning methods# for person re-identification# estimate a scaling for distances in a vector space  that is optimized for picking out observations of the same individual. This paper presents a novel approach to the pedestrian re-identification problem that uses metric learning to improve the state-of-the-art performance on standard public datasets. Very high dimensional features are extracted from the source color image. A first processing stage performs  unsupervised PCA dimensionality reduction# constrained to maintain the redundancy in color-space representation. A second stage further reduces the dimensionality# using a Local Fisher Discriminant Analysis defined by a training set. A regularization step is introduced to avoid singular matrices during this stage. The experiments conducted on three publicly available datasets confirm that the proposed method outperforms the state-of-the-art performance# including all other known metric learning methods. Furthermore# the method is an effective way to process observations comprising multiple shots# and is non-iterative: the computation times are relatively modest. Finally# a novel statistic is derived to characterize the Match Characteristic:  the system uncertainties prior and posterior to the measurement are used to define the 'Proportion of Uncertainty Removed'. This measure is invariant to test set size and provides an intuitive indication of performance.,Sateesh Pedagadi*# Kingston University; James Orwell# Kingston upon Thames University,sateesh.k112001@gmail.com; james@kingston.ac.uk,07.07 Object Recognition*,07.07 Object Recognition*,07.04 Image and Video Retrieval,09.08 Video Surveillance,, ,Objects and Scenes,P 3C
1004,Explicit Occlusion Modeling for 3D Object Class Representations,Despite the success of current state-of-the-art object class detectors# severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation# the focus has mostly been on rather clean datasets# where occlusion is not an issue. In this paper# we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained# part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning# we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion# and demonstrate superior performance in occlusion estimation and part localization# compared to baselines that are unaware of occlusions.,M. Zeeshan Zia*# ETH Zurich; Michael Stark# Stanford; Konrad Schindler# ETH Zurich,mzia@ethz.ch; mst@stanford.edu; konrad.schindler@geod.baug.ethz.ch,07.07 Object Recognition*,07.07 Object Recognition*,07.06 Object Detection,,, ,Objects and Scenes,P 3C
447,Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection,This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models# we study two advanced capabilities in hierarchy for object modeling: (i)``switch'' variables(i.e. or-nodes) for specifying alternative compositions# and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model# in the form of an And-Or Graph# comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the and-nodes to classify objects as a whole; one root-node on the top for switching multiclass classification# which is also an or-node.  For model training# we present an EM-type algorithm# namely dynamical structural optimization (DSO)# to iteratively determine the structural configuration# (e.g.# leaf-node generation associated with their parent or-nodes and shared across other classes)# along with optimizing multi-layer parameters. The proposed method is valid on challenging databases# e.g.# PASCAL VOC 2007 and UIUC-People# and it achieves state-of-the-arts performance.,Xiaolong Wang# Sun Yat-Sen University; Liang Lin*# Sun Yat-Sen University,dragonwxl123@gmail.com; linliang@ieee.org,07.07 Object Recognition*,07.07 Object Recognition*,07.08 Part-based recognition,,, ,Objects and Scenes,P 3C
880,Articulated Pose Estimation using Discriminative Armlet Classifiers,We propose a novel approach for human pose estimation in real-world cluttered scenes# and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose# we build on the notion of poselets and train highly discriminative classifiers to differentiate among arm configurations# which we call armlets. We propose a rich representation which# in addition to standard HOG features# integrates the information of strong contours# skin color and contextual cues in a principled manner. Unlike existing methods# we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset# where critical visual phenomena# such as occlusion# truncation# multiple instances and clutter are the norm. Our approach significantly outperforms Yang and Ramanan#  the state-of-the-art technique# with an improvement from 29.0%  to 37.5% PCP accuracy on the arm keypoint prediction task# on this new pose estimation dataset. ,Georgia Gkioxari*# UC Berkeley; Jitendra Malik# Berkeley; Pablo Arbelaez# ; Lubomir Bourdev# Adobe Systems Inc.,gkioxari@eecs.berkeley.edu; malik@eecs.berkeley.edu; arbelaez@eecs.berkeley.edu; lubomir.bourdev@gmail.com,07.07 Object Recognition*,07.07 Object Recognition*,07.08 Part-based recognition,09.01 Applications: humans,, ,Objects and Scenes,P 3C
757,Sparse Output Coding for Large-scale Visual Recognition,Many vision tasks require a multi-class classifier to discriminate multiple categories# on the order of hundreds or thousands. In this paper# we propose sparse output coding# a principled way for large-scale multi-class classification# by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem. Specifically# sparse output coding is composed of two steps: efficient coding matrix learning with scalability to thousands of classes# and probabilistic decoding. Empirical results on object recognition and scene classification demonstrate the effectiveness of our proposed approach.,Bin Zhao*# CMU; Eric Xing# Carnegie Mellon University,binzhao@andrew.cmu.edu; epxing@cs.cmu.edu,07.07 Object Recognition*,07.07 Object Recognition*,08.04 Graphical models,08.06 Optimization Methods,08.07 Regularization, ,Objects and Scenes,P 3C
727,From N to N+1: Multiclass Transfer Incremental Learning,Since the seminal work of Thrun []# the learning to learn paradigm has been defined as the ability of an agent to improve its performance at each task with experience#  with the number of tasks. Within the object categorization domain# the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems# addressing how to learn a new target class from few samples by leveraging over the known sources. But if one thinks of an agent learning to learn from experience# over multiple tasks# there is a need for multiclass transfer learning algorithms able to exploit their previous source knowledge when learning a new class# while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue# based on a max margin formulation. Our approach is designed  to balance between transferring to the new class and preserving what it has already been learned on the source models. Extensive experiments on subsets of publicly available datasets prove the effectiveness of our approach.,Ilja Kuzborskij*# Idiap Research Institute; Francesco Orabona# Toyota Technological Institute at Chicago; Barbara Caputo# ,ilja.kuzborskij@idiap.ch; francesco@orabona.com; bcaputo@idiap.ch,07.07 Object Recognition*,07.07 Object Recognition*,08.09 Statistical Methods and Learning,,, ,Objects and Scenes,P 3C
1049,WhatÂ’s in a Name? First Names as Facial Attributes,This paper introduces a new idea in describing people using their first names# i.e.# the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers.  We build models for 100 common first names used in the United States and# for each pair# construct a pairwise first-name classifier. These classifiers are built using training images downloaded from the internet# with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes.  We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes can also be applied to gender recognition and to age classification# outperforming state-of-the-art methods with all training images automatically gathered from the internet.,Huizhong Chen*# Stanford University; Andrew Gallagher# ; Bernd Girod# Stanford University,hchen2@stanford.edu; andrew.c.gallagher@gmail.com; bgirod@stanford.edu,07.07 Object Recognition*,07.07 Object Recognition*,09.06 Human Identification,,, ,Objects and Scenes,P 3C
1795,Kernel Null Space Methods for Novelty Detection,Detecting samples from previously unknown classes is a crucial task in object recognition# especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection# which maps all training samples of one class to a single point. Beside the possibility of modeling a single class# we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to model the support of each known class individually# our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample# our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore# we derive a simple yet powerful method for multi-class novelty detection# an important problem not studied sufficiently so far. Our novelty detection approach is assessed in comprehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other approaches.,Paul Bodesheim*# University of Jena# Germany; Alexander Freytag# Friedrich Schiller University Jena; Erik Rodner# ; Michael Kemmler# Friedrich Schiller University Jena; Joachim Denzler# ,Paul.Bodesheim@uni-jena.de; alexander.freytag@uni-jena.de; Erik.Rodner@uni-jena.de; michael.kemmler@uni-jena.de; joachim.denzler@uni-jena.de,07.07 Object Recognition*,07.07 Object Recognition*,,,, ,Objects and Scenes,P 3C
1706,Expressive Visual Text-To-Speech Using Active Appearance Models,We propose several modifications to the standard active appearance model (AAM) which make it more applicable to the task of expressive visual text-to-speech (VTTS). We incorporate the resulting model into a complete system which is capable of producing expressive output# in the form of a `talking head'# given an input text and a set of continuous expression weights.   The statistical model used to synthesize novel text is based upon a current state-of-the-art text-to-speech system. We extend the standard AAM framework to allow for normalization with respect to both pose and blink state which significantly reduces artifacts in the resulting synthesized sequences. We demonstrate quantitative improvements in terms of reconstruction error# as well as on large-scale user studies# comparing the output of different systems.,Robert Anderson*# Cambridge University; Bjorn Stenger# Toshiba Research Europe; Roberto Cipolla# Cambridge University; Vincent Wan# Toshiba Research Europe,ra312@cam.ac.uk; bjorn.stenger@crl.toshiba.co.uk; cipolla@eng.cam.ac.uk; vincent.wan@crl.toshiba.co.uk,09.01 Applications: humans,10.06 Vision for Graphics*,09.01 Applications: humans,,, ,People and Faces,P 3D
30,Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation,We present a hierarchical method for human pose estimation from a single still image. In our approach# a dependency graph representing relationships between reference points such as body joints is specified and the positions of these reference points are sequentially estimated by a successive application of multidimensional output regressions along the dependency paths# starting from the root node. Each regressor takes image features computed from an image patch centered on the current node's position estimated by the previous regressor and is specialized for estimating its child nodes' positions. The use of the dependency graph allows us to decompose a complex pose estimation problem into a set of local pose estimation problems that are less complex. We design a dependency graph for two commonly used human pose estimation datasets# the Buffy Stickmen dataset and the ETHZ PASCAL Stickmen datset# and demonstrate that our method achieves comparable accuracy to state-of-the-art results on both datasets with significantly lower computation time than existing methods. Furthermore# we propose an importance weighted gradient boosting regression method for transductive learning settings and demonstrate the resulting improved performance for pose estimation tasks.,Kota Hara*# University of Maryland; Rama Chellappa# UMD,kotahara.kh@gmail.com; rama@umiacs.umd.edu,09.01 Applications: humans*,09.01 Applications: humans*,07.01 Recognition,07.08 Part-based recognition,08.01 Learning# statistics# and inference, ,People and Faces,P 3D
1892,Hollywood 3D: Recognizing Actions in 3D Natural Scenes,Action recognition in unconstrained situations is a difficult task# suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane# losing a great deal of information. The recent emergence of 3D data# both in broadcast content# and commercial depth sensors# provides the possibility to overcome this issue. This paper presents a new dataset# for benchmarking action recognition algorithms in natural environments# while making use of 3D information. The dataset contains around 650 video clips# across 14 classes.  In addition# two state of the art action recognition algorithms are extended to make use of the 3D data# and five new interest point detection strategies are also proposed# that extend to the 3D data. Our evaluation compares all 4 feature descriptors# using 7 different types of interest point# over a variety of threshold levels# for the Hollywood3D dataset. We make the dataset including stereo video# estimated depth maps and all code required to reproduce the benchmark results# available to the wider community.,Simon Hadfield*# University of Surrey; Richard Bowden# ,S.Hadfield@surrey.ac.uk; r.bowden@surrey.ac.uk,09.01 Applications: humans*,09.01 Applications: humans*,07.01 Recognition,09.09 Video Analysis and Event Recognition,, ,People and Faces,P 3D
1942,3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image,We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. Unlike previous approaches that directly map people/face locations in 2D image space into features for classification# we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First# it can accurately estimate relative distances and orientations between people in world coordinates.  Second# it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve an accurate 3D people layout estimation# we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size# heights (e.g. age) and poses.  An accurate 3D layout also allows us to construct features informed by Proxemics that improve our semantic classification. To characterize the human interaction space# we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large# challenging dataset.,ISHANI CHAKRABORTY*# SRI International Sarnoff; Hui Cheng# SRI International Sarnoff; Omar Javed# Sarnoff,ishani.chakraborty@sri.com; hui.cheng@sri.com; omar.javed@sri.com,09.01 Applications: humans*,09.01 Applications: humans*,07.03 Context and scene understanding,,, ,People and Faces,P 3D
1621,Decoding Children's Social Behavior,We introduce a new problem domain for activity recognition: The analysis of children's social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 2-4 years and an adult clinician. Such interactions arise naturally in the diagnosis and treatment of developmental disorders# such as Down Syndrome# ADHD# and autism. We introduce a new publicly-available dataset containing 44 sessions of child-adult interaction. In each session# the adult clinician followed a Social Response Protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors# and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions. We also show promising results for multi-modal fusing.,James Rehg*# Georgia Tech; Gregory Abowd# Georgia Institute of Technology; Agata Rozga# Georgia Institute of Technology; Mario Romero# Georgia Institute of Technology; Mark Clements# Georgia Institute of Technology; Liliana Presti# Boston University; Stan Sclaroff# Boston University; Irfan Essa# GaTech,rehg@gatech.edu; abowd@gatech.edu; agata@gatech.edu; mario@gatech.edu; clements@ece.gatech.edu; loprest@bu.edu; sclaroff@bu.edu; irfan@cc.gatech.edu,09.01 Applications: humans*,09.01 Applications: humans*,09.05 Gesture Analysis,09.07 Person detection and tracking,09.09 Video Analysis and Event Recognition, ,People and Faces,P 3D
1924,Capturing Complex Spatio-Temporal Relations among Facial Muscles for Facial Expression Recognition,Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events# or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information# we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles# as well as its advantage over the existing methods. ,Ziheng Wang*# RPI; SHANGFEI WANG# USTC; Qiang Ji# Rensselaer Polytechnic Institute,wangz10@rpi.edu; sfwang@ustc.edu.cn; jiq@rpi.edu,09.01 Applications: humans*,09.01 Applications: humans*,09.09 Video Analysis and Event Recognition,,, ,People and Faces,P 3D
1882,Detecting Pulse From Head Motion,We extract heart rate and beat lengths from videos by measuring subtle head motion caused by the Newtonian reaction to the influx of blood at each beat. Our method tracks features on the head and performs independent component analysis (ICA) to decompose their trajectories into a set of component motions. It then chooses the component that best corresponds to heartbeats based on its temporal frequency spectrum. Finally# we analyze the motion projected to this component and identify peaks of the trajectories# which correspond to heartbeats. When evaluated on seven subjects# our system reported heart rates nearly identical to an electrocardiogram device. Additionally# when combined with a simple peak detector# our system was able to capture some of the clinically relevant information about heart rate variability.,Guha Balakrishnan*# MIT; John  Guttag# ; John Guttag# ; Eugene Shih# QRC,balakg@mit.edu; guttag@mit.edu; fredo@mit.edu; eugene.shih@qrclab.com,09.01 Applications: humans*,09.01 Applications: humans*,10.03 Medical Image Analysis,,, ,People and Faces,P 3D
781,Towards Contactless# Low-Cost and Accurate 3D Fingerprint Identification ,In order to avail the benefits of higher user convenience# hygiene# and improved accuracy# contactless 3D fingerprint recognition techniques have recently been introduced.  One of the key limitations of these emerging 3D fingerprint technologies to replace the conventional 2D fingerprint system is their bulk and high cost# which mainly results from the use of multiple imaging cameras or structured lighting employed in these systems. This paper details the development of a contactless 3D fingerprint identification system that uses only single camera. We develop new representation of 3D finger surface features using Finger Surface Codes and illustrate its effectiveness in matching 3D fingerprints. Conventional minutiae representation is extended in 3D space and a nonlinear matching matric is developed to robustly match recovered 3D minutiae. Multiple 2D fingerprint images (with varying illumination profile) acquired to build 3D fingerprints can themselves be used to extract 2D features for improving 3D fingerprint recognition and has been illustrated in this paper. We use first order approximation while attempting to answer one of the most fundamental questions on the availability of inherent discriminable information from 3D fingerprint. The experimental results are shown a database of 240 client fingerprints# which is made publicly available to further research efforts in this area#   and confirm the advantages of the single camera based 3D fingerprint identification.,Ajay Kumar*# Hong Kong Polytechnic Univ; Cyril Kwong# The Hong Kong Polytechnic University,csajaykr@comp.polyu.edu.hk; csmckwong@comp.polyu.edu.hk,09.02 Biometrics*,09.02 Biometrics*,09.06 Human Identification,,, ,People and Faces,P 3D
2082,Robust Discriminative Response Map Fitting with Constrained Local Models,We present a novel discriminative regression based approach for the Constrained Local Models (CLMs) framework# referred to as the Discriminative Response Map Fitting (DRMF) method# which shows impressive performance in the generic face fitting scenario. The motivation behind this approach is that# unlike the holistic texture based features used in the discriminative AAM approaches# the response map can be represented by a small set of parameters and these parameters can be very efficiently used for reconstructing unseen response maps. Furthermore# we show that by adopting very simple off-the-shelf regression techniques# it is possible to learn robust functions from response maps to the shape parameter updates. The experiments# conducted on Multi-PIE# XM2VTS and LFPW database# show that the proposed DRMF method outperforms state-of-the-art algorithms for the task of generic face fitting. Moreover# the DRMF method is computationally very efficient and is real-time capable. The current MATLAB implementation takes 1 second per image. To facilitate future comparisons# we release the source code and the pre-trained models for research purposes.,Akshay Asthana*# Imperial College London; shiyang Cheng# Imperial College London; Stefanos Zafeiriou# Imperial; Maja Pantic# ,a.asthana@imperial.ac.uk; shiyang.cheng11@imperial.ac.uk; s.zafeiriou@imperial.ac.uk; m.pantic@imperial.ac.uk,09.03 Face detection and head tracking*,09.03 Face detection and head tracking*,04.02 Image alignment,,, ,People and Faces,P 3D
1354,Facial Feature Tracking under Varying Facial Expressions and Face Poses based on Restricted Boltzmann Machine,Facial feature tracking is an active area in computer vision due to its relevance for many applications. It is a nontrivial task# especially when subjects are under varying facial expressions# poses or occlusion. In this paper# we address this problem by incorporating a face shape prior model that is constructed based on the Restricted Boltzmann Machine (RBM) and its variants. Specifically# we first propose Deep Belief Networks based face prior model to capture the face shape variations under varying facial expressions for near-frontal view. To handle the pose variation# we then combine the frontal face prior model with a 3-way RBM model that could capture the relationship between frontal face shape and non-frontal face shapes. Finally# we introduce methods to systematically combine the face prior models with the image measurements of facial feature points. Experiments on benchmark databases show that the proposed methods can robustly track facial feature points under significant facial expression and pose variations.,Yue Wu*# RPI; Zuoguan Wang# RPI; Qiang Ji# ,wuy9@rpi.edu; wangz6@rpi.edu; qji@ecse.rpi.edu,09.03 Face detection and head tracking*,09.03 Face detection and head tracking*,04.04 Model-based reconstruction and tracking,,, ,People and Faces,P 3D
745,Detecting and Aligning Faces by Image Retrieval,Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods due to the large variation in facial appearances# as well as occlusion and clutter. In order to overcome these challenges# we present a novel and robust exemplar-based face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected# and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result# faces can be very efficiently detected by selecting the modes from the voting maps# without resorting to exhaustive sliding window-style scanning. Moreover# due to the exemplar-based framework# our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient# and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localizations. The same methodology can also be easily generalized to other face-related tasks# such as attribute recognition# as well as general object detection.,"Xiaohui Shen*# Northwestern University; Zhe Lin# """"""""""""""Adobe Systems# Inc.""""""""""""""; Jonathan Brandt# Adobe; Ying Wu# ",xsh835@eecs.northwestern.edu; zlin@adobe.com; jbrandt@adobe.com; yingwu@eecs.northwestern.edu,09.03 Face detection and head tracking*,09.03 Face detection and head tracking*,07.04 Image and Video Retrieval,,, ,People and Faces,P 3D
1740,Learning SURF Cascade for Fast and Accurate Object Detection,This paper presents a novel learning framework for training boosting cascade based object detector from large scale dataset.   The framework is derived from the well-known Viola-Jones (VJ) framework but distinguished by three key differences.   First# the proposed framework adopts multi-dimensional SURF features instead of single dimensional haar features to describe local patches.   In this way# the number of local patches used can be reduced from hundreds of thousands to several hundreds.   Second# it adopts the logistic regression as weak classifier for each local patch instead of decision tree classifiers in the VJ framework.   Third# we adopt AUC as a single criterion for the convergency test during cascade training   rather than the two trade-off criteria (false-positive-rate and hit-rate) in the VJ framework.    The benefit is that the false-positive-rate can be adaptive among different cascade stages# and thus yields super fast convergence speed of the cascade.   Combining these points together# the proposed framework yields three good properties.   First# the boosting cascade can be trained very efficiently. We made  experiments on training face detector and car detector from large scale database. Results shows   that it is able to train detectors with scanning billions of negative samples within one hour even on personal computers.   Second# the built detector is comparable to the state-of-the-art algorithm not only on the accuracy but also on the processing speed.   Third# the built detector has much shorter cascade stages and thus the model size is very smaller.,Jianguo Li*# Intel Labs,leeplus@gmail.com,09.03 Face detection and head tracking*,09.03 Face detection and head tracking*,07.06 Object Detection,,, ,People and Faces,P 3D
498,Deep Convolutional Network Cascade for Facial Point Detection,We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level# the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks# global high-level features are extracted over the whole face region at the initialization stage# which help to locate high accuracy keypoints. There are two folds of advantage for this. First# the texture context information over the entire face is utilized to locate each keypoint. Second# since the networks are trained to predict all the keypoints simultaneously# the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions# large pose variations# and extreme lightings. The networks at the following two levels are trained to locally refine the initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are also investigated. Extensive experiments show that our approach outperforms state-of-the-art methods in both detection accuracy and reliability.,Yi Sun*# CUHK; Xiaogang Wang# The Chinese University of Hong Kong,sy011@ie.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk,09.03 Face detection and head tracking*,09.03 Face detection and head tracking*,"08.01 Learning# statistics# and inference""",,, ,People and Faces,P 3D
1836,Exemplar-Based Face Parsing,In this work# we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes.  Our approach assumes a database of exemplar face images# each of which is associated with a hand-labeled segmentation map. Given a testing image# our algorithm first selects a subset of exemplar images from the database. Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image.  Finally# we propagate labels from the exemplar images to the test image in a pixel-wise manner# using trained weights to modulate and combine label maps from different exemplars.  We evaluate our method on several challenging datasets and compare with two face segmentation algorithms. We also compare our segmentation results with contour-based face alignment results that is# we first run the alignment algorithms to extract contour points and then derive segments from the contours.  Our algorithm compares favorably with all previous works on all datasets that we evaluated.,"Brandon Smith*# University of Wisconsin; Li Zhang# ; Jonathan Brandt# Adobe; Zhe Lin# """"""""""""""Adobe Systems# Inc.""""""""""""""; Jianchao Yang# Adobe",bmsmith@cs.wisc.edu; lizhang@cs.wisc.edu; jbrandt@adobe.com; zlin@adobe.com; jiayang@adobe.com,09.03 Face detection and head tracking*,09.03 Face detection and head tracking*,,,, ,People and Faces,P 3D
1908,Graph PCA: Closed-form Solution and Robustness,Principal Component Analysis (PCA) is a widely used method to learn a low dimensional representation. In many applications# both vector data X and graph data W are available. Laplacian embedding (LE) is widely used for embedding graph data. We propose a graph PCA to learn a low dimensional representation of X that incorporates data cluster structures encoded in W. This is achieved by combining the strengths of both PCA and LE. This graph PCA model has several advantages: (1) It is a data representation model. (2) It has a compact closed-form solution and can be efficiently computed using eigenvectors. (3) It has inherent capability to remove corruptions. Extensive experiments on eight datasets show promising results on data reconstruction and significantly improvement on clustering and classification tasks.,Bo Jiang*# Anhui University; Chris Ding# ; Bin Luo# Anhui University; Jin Tang# ; Ying Xie# ,zeyiabc@163.com; chqding@uta.edu; luobin@ahu.edu.cn; ahhftang@gmail.com; xieying.ahu@gmail.com,09.04 Face recognition*,01.01 Image processing*,07.01 Recognition,"08.01 Learning# statistics# and inference""",,PCA for face recognition,People and Faces,P 3D
246,Probabilistic Elastic Matching for Pose Variant Face Verification,Pose variation remains to be a major challenge for real-world face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g.# LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location# a Gaussian mixture model (GMM) is trained to capture the spatial-appearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification# we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks# which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-of-the-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin. ,"Haoxiang Li# Stevens Institute of Technology; Gang Hua*# Stevens Institute of Technology; Zhe Lin# """"""""""""""Adobe Systems# Inc.""""""""""""""; Jonathan Brandt# Adobe; Jianchao Yang# Adobe",hli18@stevens.edu; ganghua@gmail.com; zlin@adobe.com; jbrandt@adobe.com; jiayang@adobe.com,09.04 Face recognition*,09.04 Face recognition*,07.08 Part-based recognition,,, ,People and Faces,P 3D
1510,Constrained Clustering and Its Application to Face Clustering In Videos,In this paper# we focus on face clustering in videos: Given the detected faces from real-world videos# we want to partition all faces into K disjoint clusters. Different from clustering on a collection of facial images# the faces from videos are organized as face tracks and the frame index of each face is also provided. As a result# many pairwise constraints between faces can be easily obtained from the temporal and spatial knowledge of the face tracks. These constraints can be effectively incorporated into a generative clustering model based on the Hidden Markov Random Fields (HMRFs). Within the HMRF model# the pairwise constraints are augmented by label-level and constraintlevel local smoothness to guide the clustering process. The parameters for both the unary and the pairwise potential functions are learned by the simulated field algorithm# and the weights of constraints can be easily adjusted. We further introduce an efficient clustering framework specially for face clustering in videos# considering that faces in adjacent frames of the same face track are very similar. The framework is applicable to other clustering algorithms to significantly reduce the computational cost. Experiments on two face data sets from real-world videos demonstrate the superior performance of our algorithm to state-of-theart algorithms.,Baoyuan Wu*# CASIA & RPI; Yifan Zhang# ; Baogang Hu# ; Qiang Ji# ,wubaoyuan1987@gmail.com; yfzhang@nlpr.ia.ac.cn; hubg@nlpr.ia.ac.cn; qji@ecse.rpi.edu,09.04 Face recognition*,09.04 Face recognition*,08.05 Markov Random Fields,09.01 Applications: humans,, ,People and Faces,P 3D
172,Selective Transfer Machine for Personalized Facial Action Unit Detection,Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. The problem is challenging in large part because classifiers must generalize to previously unknown people that differ markedly in facial morphology (e.g.# heavy versus delicate brows# smooth versus deeply etched wrinkles) and behavior. While some progress has been achieved through improvements in choice of features and classifiers# the challenge occasioned by these individual differences remains. One solution would be use of person-specific classifiers that are trained for use with specific persons. Except for specialized applications# person-specific classifiers are neither feasible nor theoretically compelling.  The problem that we address in this paper is how to personalize a generic classifier in an unsupervised manner (no additional labels for the test subject are required). We propose a transductive learning method# which we refer to as a Selective Transfer Machine (STM)# to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously re-weighting the training samples that are most relevant to each test subject. STM has been evaluated in three major databases: CK+# GEMEP-FERA and RU-FACS.,Wen-Sheng Chu*# Carnegie Mellon University; Fernando DelaTorre# ; Jeffery Cohn# Carnegie Mellon University,wensheng.chu@gmail.com; ftorre@cs.cmu.edu; jeffcohn@cs.cmu.edu,09.04 Face recognition*,09.04 Face recognition*,08.09 Statistical Methods and Learning,,, ,People and Faces,P 3D
1539,The SVM-minus Similarity Score for Video Face Recognition,Face recognition in unconstrained videos requires specialized tools beyond those developed for still images: the fact that the confounding factors change state during the video sequence presents a unique challenge# but also an opportunity to eliminate spurious similarities. Luckily# a major source of confusion in visual similarity of faces is the 3D head orientation# for which image analysis tools provide an accurate estimation.  The method we propose belongs to a family of classifier-based similarity scores. We present an effective way to discount pose induced similarities within such a framework# which is based on a newly introduced classifier called SVM-minus. The presented method is shown to outperform existing techniques on the most challenging and realistic publicly available video face recognition benchmark# both by itself# and in concert with other methods.,Lior Wolf*# Tel-Aviv University; Noga Levy# Tel-Aviv University,wolf@cs.tau.ac.il; nogaor@gmail.com,09.04 Face recognition*,09.04 Face recognition*,09.02 Biometrics,09.08 Video Surveillance,, ,People and Faces,P 3D
653,Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-based Classification,In this paper# we employ a large database of still images from the Internet to perform complete video face recognition from face tracking to face track identification. A straightforward application of the popular \l1-minimization for face recognition on a frame-by-frame basis is prohibitively expensive computationally# so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and employing the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to \l1-minimization that forces individual frames in a face track to all reconstruct a single identity# we show the optimization reduces to a single minimization over the mean of the face track. Furthermore# we introduce# analyze# and release the new Movie Trailer Face Dataset collected from 113 movie trailers on YouTube. Finally# we show that our method equals or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities# YouTube Faces# and Buffy) and our real-world# unconstrained Movie Trailer Face Dataset. More importantly# our method excels at rejecting background# unknown identities by at least 8\% on average precision. ,Enrique Ortiz*# UCF; Mubarak Shah# ucf,ortizeg@gmail.com; shah@crcv.ucf.edu,09.04 Face recognition*,09.04 Face recognition*,09.09 Video Analysis and Event Recognition,,, ,People and Faces,P 3D
81,Towards Pose Robust Face Recognition,Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper# we propose a novel unsupervised method for pose robust face recognition towards practical applications# which is fast# pose robust and working well under unconstrained environments. Firstly# a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly# a group of Gabor filters are transformed according to the pose of face image for feature extraction. Finally# PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity of feature vectors. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space# which makes our method is less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering# the final feature is not only robust to pose but also can resist other negative factors in face recognition; (3) By defining symmetric 3D feature points# the facial symmetry is successfully used to reduce the impact of self-occlusion. Extensive experiments on FERET and PIE show the proposed method is good for pose problem and outperforms other state-of-the-arts significantly. Meanwhile# the method works well on LFW and its performance is higher than the best unsupervised method.,Dong Yi*# NLPR# CASIA; Zhen Lei# CASIA# NLPR; Stan Li# ,dyi@cbsr.ia.ac.cn; zlei@nlpr.ia.ac.cn; szli@nlpr.ia.ac.cn,09.04 Face recognition*,09.04 Face recognition*,,,, ,People and Faces,P 3D
1744,Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer,Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption# and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images# a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes# and form an illumination transfer dictionary. By enforcing a sparse representation of the query image# the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular# the face alignment accuracy is comparable to that of the well-known deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those of the SRC and extended SRC algorithms using hand labeled alignment initialization.,Liansheng ZHUANG# USTC; Allen Yang*# UC Berkeley; Zihan Zhou# University of Illinois; Shankar Sastry# UC Berkeley; Yi Ma# University of Illinois,lszhuang@ustc.edu.cn; yang@eecs.berkeley.edu; zzhou7@illinois.edu; sastry@eecs.berkeley.edu; yima@illinois.edu,09.04 Face recognition*,09.04 Face recognition*,,,, ,People and Faces,P 3D
1838,Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild,In many real-world face recognition scenarios# face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue# we propose a new approach to extract robust face region descriptors. Specifically# we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and represent each block (resp. volume) by sum-pooling the sparse codes of the position-free patches sampled within the block (resp. volume). Whiten principal component analysis (WPCA) is further utilized to reduce the feature dimension# which leads to our spatial face region descriptors (SFRDs) (resp. spatial-temporal face region descriptor# STFRDs) for images (resp. videos). Moreover# we develop a new distance metric learning method for face verification called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors from all blocks (resp. volumes) in an image (resp. a video). Our work achieves the state-of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.,Zhen Cui*# CAS; Wen LI# Nanyang Technological Universi; Dong Xu# ; Shiguang Shan# ; Xilin Chen# ICT#CAS,zhen.cui@vipl.ict.ac.cn; wli1@e.ntu.edu.sg; dongxu@ntu.edu.sg; sgshan@ict.ac.cn; xlchen@ict.ac.cn,09.04 Face recognition*,09.04 Face recognition*,,,, ,People and Faces,P 3D
158,Action Recognition by Hierarchical Sequence Summarization,Choosing a good feature representation for human activity data is difficult in part because it contains temporal information at various resolutions. We present a discriminative latent variable model that learns from multiple layers of temporal representations found by hierarchical sequence summarization. Each layer in the hierarchy is generated dynamically and recursively by grouping observations from the preceding layer that are judged to be similar in the latent space# which is learned to be discriminative and thus more effective. We assume that each layer is conditionally independent given the label# and formulate our model as the product of recursively defined Hidden CRFs learned from each layer# thereby learning ever more high level concepts at each coarser-grained representation. The key to the efficiency of our model is an incremental optimization that trains the model layer-wise# from the finest to the coarsest# making the complexity grow sublinearly with the number of layers considered. Experimental results show that our approach outperforms previous state-of-the-art results on the ArmGesture and Canal9 datasets# and that learning from hierarchical sequence summaries found in the latent space is indeed effective.,Yale Song*# MIT; Louis-Philippe Morency# USC; Randall Davis# MIT,yalesong@csail.mit.edu; morency@ict.usc.edu; davis@csail.mit.edu,09.05 Gesture Analysis,01.10 Multi-scale processing*,08.04 Graphical models,09.05 Gesture Analysis,,Action recognition,People and Faces,P 3D
2119,Pixel-level Hand Detection in Ego-Centric Videos,We address the task of pixel-level hand detection in the context of ego-centric cameras. Extracting hand regions in ego-centric videos is a critical step for understanding hand-object manipulation and analyzing hand-eye coordination. However# in contrast to traditional applications of hand detection# such as gesture interfaces or sign-language recognition# ego-centric videos present new challenges such as rapid changes in illuminations# significant camera motion and complex hand-object manipulations. To quantify the challenges and performance in this new domain# we present a fully labeled indoor/outdoor ego-centric hand detection benchmark dataset containing over 200 million labeled pixels# which contains hand images taken under various illumination conditions. Using both our dataset and a publicly available ego-centric indoors dataset# we give extensive analysis of detection performance using a wide range of local appearance features. Our analysis highlights the effectiveness of sparse features and the importance of modeling global illumination. We propose a modeling strategy based on our finds and show that our model outperforms several baseline approaches. ,Cheng Li# ; Kris Kitani*# Carnegie Mellon University,chengli@cs.cmu.edu; kkitani@cs.cmu.edu,09.05 Gesture Analysis*,09.05 Gesture Analysis*,07.06 Object Detection,,, ,People and Faces,P 3D
1197,Human Pose Estimation using a Joint Pixel-wise and Part-wise Formulation,Our goal is to detect humans and estimate their 2D pose in single images. In particular# handling cases of partial visibility where some limbs may be occluded or one person is partially occluding another.  Two standard# but disparate# approaches have developed in the field: the first is the part based approach for layout type problems# involving optimising an articulated pictorial structure; the second is the pixel based approach for image labelling involving optimising a random field graph defined on the image.  Our novel contribution is a formulation for pose estimation which combines these two models in a principled way in one optimisation problem and thereby inherits the advantages of both of them. Inference on this joint model finds the set of instances of persons in an image# the location of their joints# and a pixel-wise body part labelling.  We achieve near or state of the art results on standard human pose datasets# and demonstrate the correct estimation for cases of self-occlusion# person overlap and image truncation.,Lubor Ladicky*# ; Andrew Zisserman# Oxford; Philip Torr# Oxford Brookes University,lubor@robots.ox.ac.uk; az@robots.ox.ac.uk; philiptorr@brookes.ac.uk,09.05 Gesture Analysis*,09.05 Gesture Analysis*,,,, ,People and Faces,P 3D
353,Unsupervised Salience Learning for Person Re-identification,Many approaches learn discriminative feature matching to handle dramatic viewpoint change in person re-identification. However# discriminative power of conventional feature learning is non-adaptive and sometime may become unreliable. In this paper# we propose a novel perspective for handling person re-identification based on unsupervised salience learning. First# we propose to use adjacency constrained patch matching to build dense correspondence between image pairs# which shows usefulness in handling large viewpoint variation. Second# we learn human salience in unsupervised manner. To improve the performance of person re-identification# human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on widely used public datasets (the VIPeR dataset# and the ETHZ dataset). It outperforms all the state-of-the-art supervised and unsupervised person re-identification methods published on these two datasets.,Rui Zhao*# CUHK; Wanli Ouyang# The Chinese University of HK; Xiaogang Wang# The Chinese University of Hong Kong,rzhao@ee.cuhk.edu.hk; wlouyang@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk,09.06 Human Identification*,09.06 Human Identification*,02.03 Feature descriptors,02.05 Feature matching and indexing,09.08 Video Surveillance, ,People and Faces,P 3D
1628,Locally Aligned Feature Transforms across Views,In this paper# we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person re-identification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized.  The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsity-inducing norm and information theoretical regularization. This approach can be generalized to the settings where test images are from new camera views# not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.,Wei LI*# The Chinese University of HK; Xiaogang Wang# The Chinese University of Hong Kong,liwei@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk,09.06 Human Identification*,09.06 Human Identification*,07.07 Object Recognition,09.08 Video Surveillance,, ,People and Faces,P 3D
95,Semi-supervised Learning with Constraints for Person Identification in Multimedia Data,In this paper# we address the problem of person identification in TV series. Despite the large amount of data available even just from a single TV series# most of it remains unlabeled# and is therefore unusable. We propose a unified learning framework which combines both labeled and unlabeled data# as well as constraints between pairs of face tracks in the video. We apply the framework to train multinomial logistic regression classifiers for multi-class face recognition. The method is completely automatic# as the labeled data is obtained by tagging speaking faces using subtitles and fan transcripts of the videos. We demonstrate our approach on six episodes each of two diverse TV series and achieve state-of-the-art performance. ,Martin BÃ¤uml*# KIT; Makarand Tapaswi# KIT; Rainer Stiefelhagen# Karlsruhe Institute of Technology,baeuml@kit.edu; makarand.tapaswi@kit.edu; rainer.stiefelhagen@kit.edu,09.06 Human Identification*,09.06 Human Identification*,"08.01 Learning# statistics# and inference""",,, ,People and Faces,P 3D
1080,Learning Locally-Adaptive Decision Functions for Person Verification,"This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person# even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g.# by metric learning algorithms)# and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem# and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML# but also the state-of-the-art in the computer vision community. For example# our method achieves 89.3% accuracy on the ""Labeled Face in the Wild"" (LFW) dataset# which outperforms the best reported results under the same setting in recent years. Our method also achieves the best performance on a number of benchmark datasets for pedestrian verification# including VIPeR and CAVIAR4REID.",Zhen Li*# UIUC; Shiyu Chang# UIUC; Feng Liang# UIUC; Thomas Huang# UIUC; Liangliang Cao# IBM Research Center; John Smith# IBM Thomas J. Watson Research Center,zhenli3@illinois.edu; chang87@illinois.edu; liangf@illinois.edu; huang@ifp.illinois.edu; liangliang.cao@gmail.com; jsmith@us.ibm.com,09.06 Human Identification*,09.06 Human Identification*,08.09 Statistical Methods and Learning,,, ,People and Faces,P 3D
1827,3D Pictorial Structures for Multiple View Articulated Pose Estimation,We consider the problem of automatically estimating the 3D pose of humans from images taken from multiple views. We assume that the cameras are calibrated. We show that it is possible and tractable to extend the pictorial structures framework# popular for 2D pose estimation# to 3D. We first investigate the tractability of the algorithm by doing experiments on synthetic data. We then evaluate the algorithm qualitatively and quantitatively on a challenging image data set from a professional football game.,Magnus Burenius*# KTH,burenius@kth.se,09.07 Person detection and tracking,05.03 Calibration and pose estimation*,05.01 3D modeling and reconstruction,07.08 Part-based recognition,09.07 Person detection and tracking,Human pose detection,People and Faces,P 3D
1129,Pedestrian Detection with Unsupervised and Multi-Stage Feature Learning,Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision# we report state-of-the-art and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists# such as multi-stage features# connections that skip layers to integrate global shape information with local distinctive motif information# and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.,Pierre Sermanet*# New York University; Koray Kavukcuoglu# New York University; Soumith Chintala# New York University; Yann Lecun# New York University,pierre.sermanet@gmail.com; koray.kavukcuoglu@gmail.com; soumith@gmail.com; yann@cs.nyu.edu,09.07 Person detection and tracking*,09.07 Person detection and tracking*,02.01 Feature extraction and matching,07.06 Object Detection,"08.01 Learning# statistics# and inference""", ,People and Faces,P 3D
1683,A Joint Model for 2D and 3D Pose Estimation from a Single Image,In this paper we introduce a novel approach to automatically recover the 3D human pose from a single image. Most previous work on this problem follows a pipelined approach: initially# a set of 2D features such as edges# joints or silhouettes are detected in the image# and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper# we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose# we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs# and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results# and the ability of our methodology to provide accurate 2D and 3D pose estimations even if the 2D detectors are inaccurate.,Edgar Simo-Serra*# IRI (CSIC-UPC); Ariadna Quattoni# Universitat PolitÃ¨cnica de Catalunya; Carme Torras# IRI (CSIC-UPC); Francesc Moreno-Noguer# Institut de Robotica i Informatica Industrial (UPC/CSIC),esimo@iri.upc.edu; aquattoni@lsi.upc.edu; torras@iri.upc.edu; fmoreno@iri.upc.edu,09.07 Person detection and tracking*,09.07 Person detection and tracking*,03.05 Shape Representation and Matching,07.08 Part-based recognition,, ,People and Faces,P 3D
991,Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-modality Regression Forest,This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE) from a novel perspective. Existing approaches struggle to operate in realistic applications# mainly due to their scene-dependent priors# such as background segmentation and multicamera network# which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features# e.g. silhouettes# we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method# which demonstrated promising results# significantly outperforming the relevant state-of-the-arts.,Tsz-Ho Yu*# University of Cambridge; Tae-Kyun Kim# ; Roberto Cipolla# University of Cambridge,thy23@cam.ac.uk; tk.kim@imperial.ac.uk; cipolla@cam.ac.uk,09.07 Person detection and tracking*,09.07 Person detection and tracking*,04.04 Model-based reconstruction and tracking,09.09 Video Analysis and Event Recognition,, ,People and Faces,P 3D
1273,Joint Multi-Camera Reconstruction and Multi-Object Tracking in a Global Unified Optimization Framework,We generalize the network flow formulation for multi-object tracking to multi-camera setups. In the past# reconstruction of multi-camera data was done as a separate extension. In this work# we present a combined maximum a posteriori (MAP) formulation# which jointly models multi-camera reconstruction as well as global temporal data association. A flow graph is constructed# which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming and runs in more than real time on the PETS 2009 dataset. On this dataset we achieve results that significantly exceed the current state of the art. ,Martin Hofmann*# Technische UniversitÃ¤t MÃ¼nchen; Daniel Wolf# Technische UniversitÃ¤t MÃ¼nchen,martin.hofmann@tum.de; dinsn87@gmail.com,09.07 Person detection and tracking*,09.07 Person detection and tracking*,04.05 Object Tracking and Motion Analysis,05.05 Multi-view stereo,09.08 Video Surveillance, ,People and Faces,P 3D
1588,Tracking People and Their Objects ,Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently# but they closely interact with their environment# which includes not only other persons# but also different scene objects. Typical everyday scenarios include people moving in groups# pushing child strollers# or pulling luggage items. In this paper# we propose a probabilistic approach for classifying such person-object interactions# associating objects to persons# and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D# while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15#000 frames of person-object interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.,Tobias Baumgartner# RWTH Aachen University; Dennis Mitzel*# RWTH Aachen University; Bastian Leibe# ,tobias.baumgartner@rwth-aachen.de; mitzel@umic.rwth-aachen.de; leibe@umic.rwth-aachen.de,09.07 Person detection and tracking*,09.07 Person detection and tracking*,04.05 Object Tracking and Motion Analysis,,, ,People and Faces,P 3D
1384,Seeking the strongest rigid detector,The current state of the art solutions for object detection describe each class by a set of models trained on discovered sub-classes (so called Â“componentsÂ”)# with each model it- self composed of collections of interrelated parts (deform- able models). These detectors build upon the now classic Histogram of Oriented Gradients+linear SVM combo. In this paper we revisit some of the core assumptions in HOG+SVM and show that by properly designing the fea- ture pooling# feature selection# preprocessing# and training methods# it is possible to reach top quality# at least for ped- estrian detections# using a single rigid component. We provide experiments for a large design space# that give insights on the design of classifiers# as well as relev- ant information for practitioners. Our best detector is fully feed-forward# has a single unified architecture# uses only histogram of oriented gradients and colour information in monocular static images# and improves over 23 other meth- ods on INRIA# ETH and Caltech-USA datasets# reducing the average miss-rate over HOG+SVM by more than 30%.,Rodrigo Benenson*# KU Leuven; Markus Mathias# KU Leuven; Tinne  Tuytelaars# KU Leuven; Luc Van Gool# KU Leuven,rodrigo.benenson@gmail.com; markus.mathias@esat.kuleuven.be; Tinne.Tuytelaars@esat.kuleuven.be; Luc.VanGool@esat.kuleuven.be,09.07 Person detection and tracking*,09.07 Person detection and tracking*,07.06 Object Detection,,, ,People and Faces,P 3D
1310,MODEC: Multimodal Decomposable Models for Human Pose Estimation,We propose a multimodal# decomposable model for articulated human pose estimation in monocular images. Most approaches for this problem use a single linear model# which make it difficult to capture  the wide range of appearance present in realistic# unconstrained images.  In this paper# we propose a model of human pose that explicitly captures a variety of pose modes.  Unlike other multimodal models# our approach includes both holistic and local cues and uses a convex objective and joint training for mode selection and pose estimation. We also employ a cascaded mode selection step which controls the trade-off between speed and accuracy# yielding a 5x speedup in inference and learning.  Our model significantly dominates state-of-the-art approaches across the accuracy-speed trade-off curve for several pose datasets.   ,Benjamin Sapp*# University of Pennsylvania; Ben Taskar# ,bensapp@cis.upenn.edu; taskar@cis.upenn.edu,09.07 Person detection and tracking*,09.07 Person detection and tracking*,07.08 Part-based recognition,08.01 Learning# statistics# and inference,08.05 Markov Random Fields, ,People and Faces,P 3D
265,Detection- and Trajectory-Level Exclusion in Multiple Object Tracking ,When tracking multiple targets in crowded scenarios# modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association# each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation# two trajectories should remain spatially separated at all times to avoid collisions. Yet# existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional random field (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms# and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore# we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling datafidelity# target dynamics and inter-target occlusion. ,Anton Andriyenko*# TU Darmstadt; Stefan Roth# Darmstadt; Konrad Schindler# ,anton.andriyenko@gris.tu-darmstadt.de; sroth@cs.tu-darmstadt.de; schindler@geod.baug.ethz.ch,09.07 Person detection and tracking*,09.07 Person detection and tracking*,08.04 Graphical models,,, ,People and Faces,P 3D
964,Optimized Pedestrian Detection for Multiple and Occluded People,We present a quadratic unconstrained binary optimization (QUBO) framework for reasoning about multiple object detections with spatial overlaps. The method maximizes an objective function composed of unary detection confidence scores and pairwise overlap constraints to determine which overlapping detections should be suppressed# and which should be kept. The framework is flexible enough to handle the problem of detecting objects as a shape covering of a foreground mask# and to handle the problem of filtering confidence weighted detections produced by a traditional sliding window object detector. The performance also improves when combining the unary scores from foreground mask and detector confidence in a hybrid approach. In our experiments# we show that our method outperforms the other state-of-the-art pedestrian detectors. ,Sitapa Rujikietgumjorn*# Pennsylvania State University; Robert Collins# ,sur167@psu.edu; rcollins@cse.psu.edu,09.07 Person detection and tracking*,09.07 Person detection and tracking*,08.06 Optimization Methods,,, ,People and Faces,P 3D
1811,Long-term Occupancy Analysis using Graph-Based Optimisation in Thermal Imagery,This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes# due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy# when people enter or leave the monitored area. In stable periods# with no activity close to the borders# people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected# local tracking is applied in order to identify a crossing. After a full sequence# the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51#000 frames# captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %# which is a half of the error percentage optained by detection only# and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene# which proves the generality of the method.,Rikke Gade*# Aalborg University; Anders JÃ¸rgensen# Aalborg University; Thomas Moeslund# Aalborg University,rg@create.aau.dk; andjor@create.aau.dk; tbm@create.aau.dk,09.07 Person detection and tracking*,09.07 Person detection and tracking*,09.01 Applications: humans,09.08 Video Surveillance,09.09 Video Analysis and Event Recognition, ,People and Faces,P 3D
1972,Detecting and Naming Actors in Movies using Generative Appearance Models,We introduce a generative model for learning person and costume specific  detectors from labeled examples. We demonstrate the model on the task of localizing and naming actors in long video sequences. More specifically# the actor's head and shoulders are each represented as a constellation  of optional  color regions. Detection can proceed despite changes in view-point and partial occlusions. We explain how to learn the models from a small number of labeled keyframes or video tracks#  and how to detect novel appearances of the actors in a maximum likelihood framework. We present results on a challenging  movie example# with 81\% recall in actor detection (coverage) and  89\% precision in actor identification (naming).,VIneet Gandhi*# INRIA; Remi Ronfard# ,vineet.gandhi@inria.fr; remi.ronfard@inria.fr,09.07 Person detection and tracking*,09.07 Person detection and tracking*,09.06 Human Identification,,, ,People and Faces,P 3D
216,Harry PotterÂ’s MarauderÂ’s Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization,Possessing a device just like Harry PotterÂ’s MarauderÂ’s Map# which pinpoints the location of each person-of-interest at all times# provides invaluable information for analysis of surveillance videos. To make this device real# a system would be required to perform robust localization and tracking in the real world surveillance scenarios# such as complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. However# existing tracking algorithms mainly focus on tracking in open uncluttered scenes and use only a subset of all available information to perform tracking. Therefore# we propose a semi-supervised approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs# our framework takes advantage of all important cues such as color# person detection# face recognition and non-background information to perform semi-supervised tracking. Local learning approaches are used to uncover the manifold in the spatio-temporal space with the appearance cue. Nonnegative discretization is used to enforce the mutual exclusion constraint# which limits a person detection output to only belong to exactly one individual. Experiments show that our algorithm can perform robust localization and tracking of persons-of-interest not only in outdoor scenes# but also in a complex indoor real-world nursing home environment.,Shoou-I Yu*# Carnegie Mellon University; Yi Yang# Carnegie Mellon University; Alexander Hauptmann# Carnegie Mellon University,iyu@cs.cmu.edu; yiyang@cs.cmu.edu; alex@cs.cmu.edu,09.07 Person detection and tracking*,09.07 Person detection and tracking*,09.08 Video Surveillance,,, ,People and Faces,P 3D
872,Improving an Object Detector and Extracting Regions using Superpixels,We propose an approach to improve the detection performance of a generic detector when it is applied in a particular video. The performance of offline-trained objects detectors are usually degraded in unconstrained video environments due to variant illuminations# backgrounds and camera views. Moreover# most object detectors are trained using Haar-like features or gradient features but ignore the consistent color patterns in a particular video. In our approach# we apply a Superpixel-based Bag-of-Word model to iteratively refine the output of a generic detector. Compared to other related work using class-level detector# our method builds a more specific individual-level detector using superpixels# hence it can handle the problem of appearance variation. Most importantly# we develop a algorithm to segment the object out of the background using superpixels and a CRF model. Therefore our method can generate an output of the exact object regions instead of the coarse bounding boxes by most detectors. In general# our method takes rough detection bounding boxes of a generic detector as input and generate the detection output with higher average precision and precise object regions. Our method is fully automatic and requires no annotations. The experiments on four recent datasets demonstrate the effectiveness of our approach and significantly improves the state-of-art detector  by $5$-$12\%$ in average precision.,Guang Shu*# University of Central Florida; Afshin Dehghan# UCF; Mubarak Shah# ,thesg2008@gmail.com; afshin.dn@gmail.com; shah@eecs.ucf.edu,09.07 Person detection and tracking*,09.07 Person detection and tracking*,09.08 Video Surveillance,,, ,People and Faces,P 3D
2061,Tracking Human Pose by Tracking Symmetric Parts,The human body is structurally symmetric. Tracking by detection approaches for human pose suffer from \emph{double counting}# where the same image evidence is used to explain two separate parts# such as the left and right feet. Double counting# if left unaddressed# occurs often in video sequences and can critically affect subsequent processes# such as action recognition# affordance estimation# and pose reconstruction.  In this work# we present an occlusion aware algorithm for tracking human pose in an image sequence# that addresses the problem of double counting. Our key insight is  that the human body can be modeled as a combination of singleton parts (such as the head and neck) and  symmetric pairs of parts (such as the shoulders# knees and feet). We formulate the joint tracking of symmetric body parts in a multi-target tracking setting that enforces mutual exclusion constraints to prevent double counting by reasoning about occlusion. We evaluate our algorithm on an outdoor dataset with natural background clutter# a standard indoor dataset (\emph{HumanEva-I)}# and compare against a state of the art pose estimation algorithm. ,Varun Ramakrishna*# Carnegie Mellon University; Yaser Sheikh# ; Takeo Kanade# Carnegie Mellon University,varunnr@cmu.edu; yaser@cs.cmu.edu; tk@cs.cmu.edu,09.07 Person detection and tracking*,09.07 Person detection and tracking*,,,, ,People and Faces,P 3D
